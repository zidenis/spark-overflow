<?xml version="1.0" encoding="utf-8"?>
<posts>
  <row Id="49067143" PostTypeId="1" CreationDate="2018-03-02T10:21:04.567" Score="-2" ViewCount="49" Body="&lt;p&gt;I have two variables, as below in each variable will have the same number of elements. in this case my variable 1 and 2 contains 3 elements.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val myVar  = List(first,second,third)&#xA;val mySecondVar  = List(one,two,three)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now i have to call a method based on the number of elements here since my variable1(myvar) contains 3 values hence i need to call my method 3 times it may increase also. However the condition is when i call method first time input parameter to the method should be first element of first variable and first element of second variable for the first call 3rd parameter will be passed as df(data from csv file). after reading the data from csv file data will be filtered based on first and second parameter the result should be passed to second iteration in 3rd parameter.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Second iteration: First parameter to the method will be second element from variable 1 and second parameter will be second element from variable 2 and now the third parameter should contain the result of first iteration. will do some logic again will store the data in one variable the result should be passed to third iteration.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val input = spark.read.option(&quot;header&quot;, &quot;true&quot;).option(&quot;inferSchema&quot;, &quot;false&quot;).csv(&quot;matching.csv&quot;)&#xA;var result = method(first,one,input)&#xA;var result2 = method(second,two,result)&#xA;var result3 = method(third,three,result2)&#xA;&#xA;def(myvar : Any,mySecondVar : Any,input : org.apache.spark.sql.DataFrame) : &#xA;org.apache.spark.sql.DataFrame={&#xA;//some logic&#xA;return &quot;result&quot;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="5405954" LastEditorUserId="4420967" LastEditDate="2018-03-02T15:28:30.820" LastActivityDate="2018-03-02T15:43:01.187" Title="scala how to send unknown variables dynamically to one single method" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="49067442" PostTypeId="1" AcceptedAnswerId="49069911" CreationDate="2018-03-02T10:39:50.470" Score="1" ViewCount="38" Body="&lt;p&gt;I have the next &lt;em&gt;build.sbt&lt;/em&gt; file:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;name := &quot;olbico-spark-solution&quot;&#xA;&#xA;version := &quot;0.2&quot;&#xA;scalaVersion := &quot;2.11.8&quot;&#xA;&#xA;resolvers += &quot;Typesafe ivy repository&quot; at &quot;https://repo.typesafe.com/typesafe/ivy-releases/&quot;&#xA;resolvers += &quot;Typesafe maven repository&quot; at &quot;https://repo.typesafe.com/typesafe/maven-releases/&quot;&#xA;&#xA;mainClass in (Compile, run) := Some(&quot;com.olbico.spark.MergeManager&quot;)&#xA;mainClass in (Compile, packageBin) := Some(&quot;com.olbico.spark.MergeManager&quot;)&#xA;&#xA;mappings in (Compile, packageBin) += {&#xA;  (baseDirectory.value / &quot;src/main/config/current.conf&quot;) -&amp;gt; &quot;config/current.conf&quot;&#xA;}&#xA;&#xA;libraryDependencies ++= {&#xA;  val sparkVer = &quot;2.2.0&quot;&#xA;  Seq(&#xA;    &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % sparkVer % &quot;provided&quot; withSources(),&#xA;    &quot;org.apache.spark&quot; %% &quot;spark-sql&quot; % sparkVer % &quot;provided&quot; withSources(),&#xA;    &quot;com.typesafe&quot; % &quot;config&quot; % &quot;1.3.1&quot; withSources()&#xA;  )&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What I am trying to achieve is to add an dependency to com.typesafe.config. With the current sbt configuration I would expect that the final bytecode would have the following structure:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;~/test-spark-solution/target/scala-2.11/classes&#xA;&#xA; -com&#xA;     -olbico&#xA;          -spark&#xA;                JobManager$class.class&#xA;                JobManager.class&#xA;                ... more classes&#xA;     -typesafe&#xA;              -config&#xA;                    Optional.class&#xA;                    DefaultConfigLoadingStrategy.class&#xA;                    ... more classes&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or at least to add a config-1.3.1.jar to the final package jar. Both solution are accepted by me although right now none of those is done. Under ~/olbico-spark-solution/target/scala-2.11/classes I have the following structure:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; -com&#xA;     -olbico&#xA;          -spark&#xA;                JobManager$class.class&#xA;                JobManager.class&#xA;                ... more classes&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Also after packaging there is no config-1.3.1.jar included into the package jar. What is the best way to add a single dependency to my project with SBT?&lt;/p&gt;&#xA;" OwnerUserId="750376" LastEditorUserId="750376" LastEditDate="2018-03-02T10:45:16.687" LastActivityDate="2018-03-02T13:17:23.093" Title="SBT is not generating bytecode class files for dependency" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;intellij-idea&gt;&lt;sbt&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49067479" PostTypeId="1" AcceptedAnswerId="49067539" CreationDate="2018-03-02T10:41:48.283" Score="0" ViewCount="31" Body="&lt;p&gt;I am using Databricks and have uploaded an external file from data lake. I have used the following code to import the data, using the library SparkR:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df = read.df(&quot;adl://test.azuredatalakestore.net/test.csv&quot;, source = 'csv', header = TRUE)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It is a Spark DataFrame, and it has restricted me from doing some manipulations on it. I can't import the file by using &lt;code&gt;read.csv&lt;/code&gt; so is there a way that I could change it to a normal DataFrame so I could perform some changes. &lt;/p&gt;&#xA;" OwnerUserId="9334206" LastActivityDate="2018-03-02T10:45:37.323" Title="Change Spark DataFrame to Standard R dataframe" Tags="&lt;r&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49067619" PostTypeId="1" CreationDate="2018-03-02T10:49:59.790" Score="0" ViewCount="19" Body="&lt;ul&gt;&#xA;&lt;li&gt;I am using datastax cluster with 5.0.5. [cqlsh 5.0.1 | Cassandra&#xA;3.0.11.1485 | DSE 5.0.5 | CQL spec 3.4.0 | Native proto&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;using spark-cassandra-connector 1.6.8&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried to implement below code.. import is not working. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;val rdd: RDD[SomeType] = ... // create some RDD to save import&#xA;   com.datastax.bdp.spark.writer.BulkTableWriter._&lt;/p&gt;&#xA;&#xA;&lt;p&gt;rdd.bulkSaveToCassandra(keyspace, table)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can someone suggest me how to implement this code. Are they any dependenceis required for this.&lt;/p&gt;&#xA;" OwnerUserId="5433741" LastActivityDate="2018-03-02T12:42:24.997" Title="How to implement rdd.bulkSaveToCassandra in datastax" Tags="&lt;apache-spark&gt;&lt;cassandra&gt;&lt;datastax&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49067739" PostTypeId="1" AcceptedAnswerId="49068926" CreationDate="2018-03-02T10:57:31.027" Score="1" ViewCount="23" Body="&lt;p&gt;I run Spark (2.2) PCA with threee variables: x, y and z.&#xA;I get:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+-----------------------------------------------------------+&#xA;|pcaFeatures                                                |&#xA;+-----------------------------------------------------------+&#xA;|[4192.998527751072,7.815744760976605,2.064076348440629]    |&#xA;|[934.9987857492071,6.178849121007534,2.0229856767680876]   |&#xA;|[81.99880210954893,6.012098465539804,2.0127405793319535] ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So these are eigenvectors. Do they correspond to x, y and z in that order? If PCA is about feature reduction then can I say x explains most of the data so just use x? Can I express this mathematically as a percentage, since I have a vector of values?&lt;/p&gt;&#xA;" OwnerUserId="1773592" LastActivityDate="2018-03-02T12:12:42.900" Title="How do I interpret Spark PCA output?" Tags="&lt;apache-spark&gt;&lt;pca&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49067774" PostTypeId="1" AcceptedAnswerId="49068110" CreationDate="2018-03-02T10:59:13.530" Score="0" ViewCount="19" Body="&lt;p&gt;I would like to take the last 6 characters of string in a dataframe &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; val loc =&quot;/data/published/omega/omega_logs/20171205_4801&quot;&#xA; val df =sqlContext.read.avro(loc)&#xA; val df1 = df.withColumn(&quot;sub_str&quot;, substring(df(&quot;broadcast_end_date_time&quot;),9,14)).select(&quot;broadcast_end_date_time&quot;,&quot;sub_str&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The above code works . but I dont want to hardcode the 14 in my substring fuction . How do i find the length of a column and convert that as Int &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; +-----------------------+-------+&#xA; |broadcast_end_date_time|sub_str|&#xA;  +-----------------------+-------+&#xA; |20171205124000         |124000 |&#xA; |20171205254000         |254000 |&#xA; |20171205143000         |143000 |&#xA; |20171205111000         |111000 |&#xA; |20171205124000         |124000 |&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am trying the below code  and I get below error &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; val df1 = df.withColumn(&quot;sub_str&quot;, substring(df(&quot;broadcast_end_date_time&quot;),9,length(df(&quot;broadcast_end_date_time&quot;)))).select(&quot;broadcast_end_date_time&quot;,&quot;sub_str&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It seems that length function returns a Column . How do i convert that to Int &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; scala&amp;gt; val df1 = df.withColumn(&quot;sub_str&quot;, substring(df(&quot;broadcast_end_date_time&quot;),9,length(df(&quot;broadcast_end_date_time&quot;)))).select(&quot;broadcast_end_date_time&quot;,&quot;sub_str&quot;)&#xA; &amp;lt;console&amp;gt;:52: error: type mismatch;&#xA; found   : org.apache.spark.sql.Column&#xA; required: Int&#xA;   val df1 = df.withColumn(&quot;sub_str&quot;, substring(df(&quot;broadcast_end_date_time&quot;),9,length(df(&quot;broadcast_end_date_time&quot;)))).select(&quot;broadcast_end_date_time&quot;,&quot;sub_str&quot;)&#xA;                                                                                      ^&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Could someone help me to fix this issue ?&lt;/p&gt;&#xA;" OwnerUserId="3240790" LastActivityDate="2018-03-02T11:21:52.113" Title="How do i convert a Column to Int in Spark sql" Tags="&lt;apache-spark-sql&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49067858" PostTypeId="1" CreationDate="2018-03-02T11:05:15.120" Score="0" ViewCount="6" Body="&lt;p&gt;I am using spark 2.1.0 version with kafka 0.9 in MapR environment.I am trying to read from Kafka topic into spark streaming. However i am facing error as below when i am running Kafkautils createDirectStream command.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;py4j.protocol.Py4JError: An error occurred while calling z:org.apache.spark.streaming.kafka09.KafkaUtilsPythonHelper.createDirectStream. Trace:&#xA;py4j.Py4JException: Method createDirectStream([class org.apache.spark.streaming.api.java.JavaStreamingContext, class java.util.ArrayList, class java.util.HashMap]) does not exist&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Code that i am running&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from __future__ import print_function&#xA;import sys&#xA;from pyspark import SparkContext,SparkConf&#xA;from pyspark.streaming import StreamingContext&#xA;from pyspark.sql import SQLContext&#xA;from pyspark.streaming.kafka09 import KafkaUtils;&#xA;&#xA;sqlContext = SQLContext(sc)&#xA;ssc = StreamingContext(sc, 3)&#xA;strLoc   = '/home/mapr/stream:info'&#xA;kafkaparams = {&quot;zookeeper.connect&quot; : &quot;x.x.x.x:5181&quot;,&quot;metadata.broker.list&quot; : &quot;x.x.x.x:9092&quot;}&#xA;&#xA;strarg = KafkaUtils.createDirectStream(ssc,[strLoc],kafkaparams) &amp;lt;- Error when i run this command on pyspark shell&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="3501573" LastEditorUserId="3501573" LastEditDate="2018-03-02T11:12:14.557" LastActivityDate="2018-03-02T11:12:14.557" Title="pyspark streaming with kafka error" Tags="&lt;spark-streaming&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49068141" PostTypeId="1" CreationDate="2018-03-02T11:24:10.220" Score="-1" ViewCount="31" Body="&lt;p&gt;I am learning data analytics( complete beginner). As a part of it, I need to write a simple ETL program using PYSPARK. the operations I need to carry out are as follows.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Load a mysql(source) database from localhost&lt;/li&gt;&#xA;&lt;li&gt;write it to mongodb in localhost.&lt;/li&gt;&#xA;&lt;li&gt;if update/delete/insertion occurs perform it on the mongodb(target) database.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Suppose, my source mysql db has table: &quot;testdata&quot; having columns: id &amp;amp; name, and a middleware table called ETL_operation, which has the columns id, table_name and operation (to store the updated data using mysql triggers.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;I want to know how i can write an ETL satisfying above conditions in pyspark&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can do it using python, but couldnt do the same on pyspark, spent a whole day searching and trying before asking this question here. Any help will be appreciated  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;edit: here is the pyspark code i have managed so far, and i will post the python code for the same.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;the python code: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pymongo import MongoClient&#xA;from pymysql import connect&#xA;&#xA;&#xA;client = MongoClient('mongodb://localhost:27017/')&#xA;db = client['etl_learn']&#xA;# collection = db['tbl1']&#xA;&#xA;conn = connect(&quot;localhost&quot;, &quot;root&quot;, &quot;root&quot;, &quot;test_trigger&quot;)&#xA;cursor = conn.cursor()&#xA;&#xA;stmt = &quot;SHOW TABLES&quot;&#xA;cursor.execute(stmt)&#xA;tables = cursor.fetchall()&#xA;for table in tables:&#xA;    tblname = table[0]&#xA;    print(tblname)&#xA;    if tblname != 'event_table':&#xA;        stmt = &quot;SELECT * FROM %s WHERE created_date BETWEEN '2018-03-02 00:00:00' AND '2018-03-02 23:59:59'&quot; %tblname&#xA;        cursor.execute(stmt)&#xA;        resultSet = cursor.fetchall()&#xA;        for result in resultSet:&#xA;            #all col name of mysql&#xA;            id = result[0]&#xA;            name = result[1]&#xA;            created_date = result[2]&#xA;            updated_date = result[3]&#xA;            collection = db[tblname]&#xA;            documents = collection.find({&quot;_id&quot;: id})&#xA;            if documents.count() == 0:&#xA;                print(&quot;inside insert&quot;)&#xA;                document_post = {&#xA;                    '_id': id,&#xA;                    'name': name,&#xA;                    'created_date': created_date,&#xA;                    'updated_date': updated_date&#xA;                }&#xA;                collection.insert_one(document_post)&#xA;            if documents.count() &amp;gt; 0:&#xA;                print(&quot;inside update&quot;)&#xA;                collection.remove({&quot;_id&quot;: id})&#xA;                document_post = {&#xA;                    '_id': id,&#xA;                    'name': name,&#xA;                    'created_date': created_date,&#xA;                    'updated_date': updated_date&#xA;                }&#xA;                collection.insert_one(document_post)&#xA;    else:&#xA;&#xA;        stmt = &quot;SELECT * FROM eventtbl&quot;&#xA;        cursor.execute(stmt)&#xA;        resultSet = cursor.fetchall()&#xA;        for result in resultSet:&#xA;            id = result[0]&#xA;            name = result[1]&#xA;            collection = db[name]&#xA;            documents = collection.find({&quot;_id&quot;: id})&#xA;            if documents.count() != 0:&#xA;                print(&quot;inside Delete&quot;)&#xA;                collection.remove({&quot;_id&quot;: id})&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;here is the pyspark code which i have managed so far, i am trying to retrieve the table names from the database right now.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df= sqlContext.read.format(&quot;jdbc&quot;).options(&#xA;url=&quot;jdbc:mysql://localhost:3306/test_trigger&quot;,&#xA;driver = &quot;com.mysql.jdbc.Driver&quot;,&#xA;continueBatchOnError= True,&#xA;useSSL= False,&#xA;user=&quot;root&quot;,&#xA;password=&quot;root&quot;,&#xA;dbtable=&quot;information_schema.tables&quot; ).load().filter(&quot;TABLE_SCHEMA= 'test_trigger' &quot;).filter(&quot;column= 'TABLE_NAME'&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="9372742" LastEditorUserId="9372742" LastEditDate="2018-03-05T09:39:37.113" LastActivityDate="2018-03-05T09:39:37.113" Title="Any way to write a simple ETL program with pyspark" Tags="&lt;python-3.x&gt;&lt;pyspark&gt;&lt;etl&gt;" AnswerCount="0" CommentCount="4" />
  <row Id="49068313" PostTypeId="1" CreationDate="2018-03-02T11:34:39.063" Score="1" ViewCount="43" Body="&lt;p&gt;I am going to do stream processing with pyspark and use Kafka as a data source.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I see that Kafka 0.10 connector is not supported under Spark Python API.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can I use Kafka 0.8 connector in Spark 2.3.0 regardless it is deprecated?&lt;/p&gt;&#xA;" OwnerUserId="1271919" LastActivityDate="2018-03-02T11:57:49.573" Title="Can I use spark 2.3.0 and pyspark to do stream processing from Kafka?" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;&lt;apache-kafka&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49068527" PostTypeId="1" CreationDate="2018-03-02T11:49:58.700" Score="0" ViewCount="21" Body="&lt;p&gt;I have a SparkR Dataframe of 160 numeric Columns (can variate) and i want to calculate the mean of each one of them without grouping in any way. Just plain calculation on 160 cols. What is the &quot;god&quot; way to do that in SparkR?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Approach 1:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ListOfColMean &amp;lt;- List()&#xA;&#xA;for (colIdx in 1:160){&#xA;&#xA;  *ListOfColMean[[colIdx]]* &amp;lt;- sparkDF %&amp;gt;% select(mean(sparkDF[[colIdx]]))&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Approach 2:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;*Mean_SparkDF* &amp;lt;- SparkR::agg(sparkDF, meanCol1 = SparkR::mean(sparkDF$col1), meanCol2 = SparkR::mean(sparkDF$col2), meanCol3 = SparkR::mean(sparkDF$col3)....and so on until... &#xA;meanCol160 = SparkR::mean(sparkDF$col160))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there any other, more elegant, ways of doing this? a way without having to loop through all the 160 columns or having to write the names of 160 columns?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm looking for something more kind of:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;rDataFrame &amp;lt;- &amp;lt;ApplyFunOnEachColumn&amp;gt;(sparkDF, func)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Where&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;rDataFrame&lt;/code&gt; is: a R dataframe, or a vector or a list of 160 means&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;func is: SparkR::mean()&lt;/code&gt; applied to each one of the 160 columns&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;&amp;lt;ApplyFunOnEachColumn&amp;gt;&lt;/code&gt; is: a SparkR API function or a bunch of functions &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thankfull for any help.&lt;/p&gt;&#xA;" OwnerUserId="9433700" LastEditorUserId="5094664" LastEditDate="2018-03-02T13:14:33.020" LastActivityDate="2018-03-02T13:14:33.020" Title="Applying an Agg function to all columns of a dataframe in SparkR" Tags="&lt;function&gt;&lt;sparkr&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49068696" PostTypeId="1" CreationDate="2018-03-02T11:59:16.907" Score="0" ViewCount="19" Body="&lt;p&gt;I have a data source (hive external tables) which refresh the data in adhoc manner. To avoid any discrepancies in the execution i'm trying to save the data as a table in my location.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Initially, i have loaded the data from data source to a dataframe&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;source = hqlContext.table(&quot;datasourcedb.table1&quot;) // this is working fine&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then, trying to save it the my application location -&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  source.write.mode('overwrite').saveAsTable(&quot;appdb.table1&quot;)  //No read/write operations on appdb.table1 while doing this action&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Above actions throwing exceptions: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: BLOCK&#xA;        at org.apache.parquet.hadoop.ParquetFileWriter$STATE.error(ParquetFileWriter.java:146)&#xA;        at org.apache.parquet.hadoop.ParquetFileWriter$STATE.startBlock(ParquetFileWriter.java:138)&#xA;        at org.apache.parquet.hadoop.ParquetFileWriter.startBlock(ParquetFileWriter.java:195)&#xA;        at org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:153)&#xA;        at org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:113)&#xA;        at org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:112)&#xA;        at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetRelation.scala:101)&#xA;        at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.abortTask$1(WriterContainer.scala:294)&#xA;        at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:271)&#xA;        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)&#xA;        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)&#xA;        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)&#xA;        at org.apache.spark.scheduler.Task.run(Task.scala:89)&#xA;        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)&#xA;        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;        at java.lang.Thread.run(Thread.java:748)&#xA;&#xA;18/03/02 04:31:32 ERROR TaskSetManager: Task 9 in stage 1.0 failed 4 times; aborting job&#xA;18/03/02 04:31:32 ERROR InsertIntoHadoopFsRelation: Aborting job.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;**Note: The size of the source is abot 6GB. Hence, no persist action is planned ** &lt;/p&gt;&#xA;" OwnerUserId="605343" LastActivityDate="2018-03-05T10:11:47.233" Title="Issue in saving the content of a dataframe to table" Tags="&lt;spark-dataframe&gt;&lt;pyspark-sql&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49069533" PostTypeId="1" CreationDate="2018-03-02T12:53:41.370" Score="0" ViewCount="22" Body="&lt;h1&gt;I cannot properly connect to Kudu from Spark, error says &quot;Kudu master has no leader&quot;&lt;/h1&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;CDH 5.14&lt;/li&gt;&#xA;&lt;li&gt;Kudu 1.6&lt;/li&gt;&#xA;&lt;li&gt;Spark 1.6.0 standalone and 2.2.0&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;When I use Impala in HUE to create and query kudu tables, it works flawlessly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, connecting from Spark throws some errors I cannot decipher.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have tried using both pyspark and spark-shell. With spark shell I had to use spark 1.6 instead of 2.2 because some maven dependencies problems, that I have localized but not been able to fix. More info here.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h2&gt;Case 1: using pyspark2 (Spark 2.2.0)&lt;/h2&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ pyspark2 --master yarn --jars /opt/cloudera/parcels/CDH-5.14.0-1.cdh5.14.0.p0.24/lib/kudu/kudu-spark2_2.11.jar&#xA;&#xA;&amp;gt; df = sqlContext.read.format('org.apache.kudu.spark.kudu').options(**{&quot;kudu.master&quot;:&quot;172.17.0.43:7077&quot;, &quot;kudu.table&quot;:&quot;impala::default.test&quot;}).load()&#xA;&#xA;18/03/02 10:23:27 WARN client.ConnectToCluster: Error receiving response from 172.17.0.43:7077&#xA;org.apache.kudu.client.RecoverableException: [peer master-172.17.0.43:7077] encountered a read timeout; closing the channel&#xA;        at org.apache.kudu.client.Connection.exceptionCaught(Connection.java:412)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)&#xA;        at org.apache.kudu.client.Connection.handleUpstream(Connection.java:239)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.SimpleChannelUpstreamHandler.exceptionCaught(SimpleChannelUpstreamHandler.java:153)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:536)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.handler.timeout.ReadTimeoutHandler.readTimedOut(ReadTimeoutHandler.java:236)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.handler.timeout.ReadTimeoutHandler$ReadTimeoutTask$1.run(ReadTimeoutHandler.java:276)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.socket.ChannelRunnableWrapper.run(ChannelRunnableWrapper.java:40)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:391)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:315)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)&#xA;        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;        at java.lang.Thread.run(Thread.java:748)&#xA;Caused by: org.apache.kudu.shaded.org.jboss.netty.handler.timeout.ReadTimeoutException&#xA;        at org.apache.kudu.shaded.org.jboss.netty.handler.timeout.ReadTimeoutHandler.&amp;lt;clinit&amp;gt;(ReadTimeoutHandler.java:84)&#xA;        at org.apache.kudu.client.Connection$ConnectionPipeline.init(Connection.java:782)&#xA;        at org.apache.kudu.client.Connection.&amp;lt;init&amp;gt;(Connection.java:199)&#xA;        at org.apache.kudu.client.ConnectionCache.getConnection(ConnectionCache.java:133)&#xA;        at org.apache.kudu.client.AsyncKuduClient.newRpcProxy(AsyncKuduClient.java:248)&#xA;        at org.apache.kudu.client.AsyncKuduClient.newMasterRpcProxy(AsyncKuduClient.java:272)&#xA;        at org.apache.kudu.client.ConnectToCluster.run(ConnectToCluster.java:157)&#xA;        at org.apache.kudu.client.AsyncKuduClient.getMasterTableLocationsPB(AsyncKuduClient.java:1350)&#xA;        at org.apache.kudu.client.AsyncKuduClient.exportAuthenticationCredentials(AsyncKuduClient.java:651)&#xA;        at org.apache.kudu.client.KuduClient.exportAuthenticationCredentials(KuduClient.java:293)&#xA;        at org.apache.kudu.spark.kudu.KuduContext$$anon$1.run(KuduContext.scala:97)&#xA;        at org.apache.kudu.spark.kudu.KuduContext$$anon$1.run(KuduContext.scala:96)&#xA;        at java.security.AccessController.doPrivileged(Native Method)&#xA;        at javax.security.auth.Subject.doAs(Subject.java:360)&#xA;        at org.apache.kudu.spark.kudu.KuduContext.&amp;lt;init&amp;gt;(KuduContext.scala:96)&#xA;        at org.apache.kudu.spark.kudu.KuduRelation.&amp;lt;init&amp;gt;(DefaultSource.scala:162)&#xA;        at org.apache.kudu.spark.kudu.DefaultSource.createRelation(DefaultSource.scala:75)&#xA;        at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:306)&#xA;        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)&#xA;        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146)&#xA;        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#xA;        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#xA;        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#xA;        at java.lang.reflect.Method.invoke(Method.java:498)&#xA;        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#xA;        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#xA;        at py4j.Gateway.invoke(Gateway.java:280)&#xA;        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#xA;        at py4j.commands.CallCommand.execute(CallCommand.java:79)&#xA;        at py4j.GatewayConnection.run(GatewayConnection.java:214)&#xA;        ... 1 more&#xA;18/03/02 10:23:27 WARN client.ConnectToCluster: Unable to find the leader master 172.17.0.43:7077; will retry&#xA;&#xA;Py4JJavaError                             Traceback (most recent call last)&#xA;&amp;lt;ipython-input-1-e1dfaec7a544&amp;gt; in &amp;lt;module&amp;gt;()&#xA;----&amp;gt; 1 df = sqlContext.read.format('org.apache.kudu.spark.kudu').options(**{&quot;kudu.master&quot;:&quot;172.17.0.43:7077&quot;, &quot;kudu.table&quot;:&quot;impala::default.logika_dataset_kudu&quot;}).load()&#xA;&#xA;/opt/cloudera/parcels/SPARK2-2.2.0.cloudera2-1.cdh5.12.0.p0.232957/lib/spark2/python/pyspark/sql/readwriter.py in load(self, path, format, schema, **options)&#xA;    163             return self._df(self._jreader.load(self._spark._sc._jvm.PythonUtils.toSeq(path)))&#xA;    164         else:&#xA;--&amp;gt; 165             return self._df(self._jreader.load())&#xA;    166&#xA;    167     @since(1.4)&#xA;&#xA;/opt/cloudera/parcels/SPARK2-2.2.0.cloudera2-1.cdh5.12.0.p0.232957/lib/spark2/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args)&#xA;   1131         answer = self.gateway_client.send_command(command)&#xA;   1132         return_value = get_return_value(&#xA;-&amp;gt; 1133             answer, self.gateway_client, self.target_id, self.name)&#xA;   1134&#xA;   1135         for temp_arg in temp_args:&#xA;&#xA;/opt/cloudera/parcels/SPARK2-2.2.0.cloudera2-1.cdh5.12.0.p0.232957/lib/spark2/python/pyspark/sql/utils.py in deco(*a, **kw)&#xA;     61     def deco(*a, **kw):&#xA;     62         try:&#xA;---&amp;gt; 63             return f(*a, **kw)&#xA;     64         except py4j.protocol.Py4JJavaError as e:&#xA;     65             s = e.java_exception.toString()&#xA;&#xA;/opt/cloudera/parcels/SPARK2-2.2.0.cloudera2-1.cdh5.12.0.p0.232957/lib/spark2/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)&#xA;    317                 raise Py4JJavaError(&#xA;    318                     &quot;An error occurred while calling {0}{1}{2}.\n&quot;.&#xA;--&amp;gt; 319                     format(target_id, &quot;.&quot;, name), value)&#xA;    320             else:&#xA;    321                 raise Py4JError(&#xA;&#xA;Py4JJavaError: An error occurred while calling o59.load.&#xA;: java.security.PrivilegedActionException: org.apache.kudu.client.NoLeaderFoundException: Master config (172.17.0.43:7077) has no leader. Exceptions received: org.apache.kudu.client.RecoverableException: [peer master-172.17.0.43:7077] encountered a read timeout; closing the channel&#xA;        at java.security.AccessController.doPrivileged(Native Method)&#xA;        at javax.security.auth.Subject.doAs(Subject.java:360)&#xA;        at org.apache.kudu.spark.kudu.KuduContext.&amp;lt;init&amp;gt;(KuduContext.scala:96)&#xA;        at org.apache.kudu.spark.kudu.KuduRelation.&amp;lt;init&amp;gt;(DefaultSource.scala:162)&#xA;        at org.apache.kudu.spark.kudu.DefaultSource.createRelation(DefaultSource.scala:75)&#xA;        at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:306)&#xA;        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)&#xA;        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146)&#xA;        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#xA;        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#xA;        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#xA;        at java.lang.reflect.Method.invoke(Method.java:498)&#xA;        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#xA;        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#xA;        at py4j.Gateway.invoke(Gateway.java:280)&#xA;        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#xA;        at py4j.commands.CallCommand.execute(CallCommand.java:79)&#xA;        at py4j.GatewayConnection.run(GatewayConnection.java:214)&#xA;        at java.lang.Thread.run(Thread.java:748)&#xA;Caused by: org.apache.kudu.client.NoLeaderFoundException: Master config (172.17.0.43:7077) has no leader. Exceptions received: org.apache.kudu.client.RecoverableException: [peer master-172.17.0.43:7077] encountered a read timeout; closing the channel&#xA;        at org.apache.kudu.client.ConnectToCluster.incrementCountAndCheckExhausted(ConnectToCluster.java:272)&#xA;        at org.apache.kudu.client.ConnectToCluster.access$100(ConnectToCluster.java:49)&#xA;        at org.apache.kudu.client.ConnectToCluster$ConnectToMasterErrCB.call(ConnectToCluster.java:349)&#xA;        at org.apache.kudu.client.ConnectToCluster$ConnectToMasterErrCB.call(ConnectToCluster.java:338)&#xA;        at com.stumbleupon.async.Deferred.doCall(Deferred.java:1280)&#xA;        at com.stumbleupon.async.Deferred.runCallbacks(Deferred.java:1259)&#xA;        at com.stumbleupon.async.Deferred.handleContinuation(Deferred.java:1315)&#xA;        at com.stumbleupon.async.Deferred.doCall(Deferred.java:1286)&#xA;        at com.stumbleupon.async.Deferred.runCallbacks(Deferred.java:1259)&#xA;        at com.stumbleupon.async.Deferred.callback(Deferred.java:1002)&#xA;        at org.apache.kudu.client.KuduRpc.handleCallback(KuduRpc.java:238)&#xA;        at org.apache.kudu.client.KuduRpc.errback(KuduRpc.java:292)&#xA;        at org.apache.kudu.client.RpcProxy.failOrRetryRpc(RpcProxy.java:388)&#xA;        at org.apache.kudu.client.RpcProxy.responseReceived(RpcProxy.java:217)&#xA;        at org.apache.kudu.client.RpcProxy.access$000(RpcProxy.java:60)&#xA;        at org.apache.kudu.client.RpcProxy$1.call(RpcProxy.java:132)&#xA;        at org.apache.kudu.client.RpcProxy$1.call(RpcProxy.java:128)&#xA;        at org.apache.kudu.client.Connection.cleanup(Connection.java:694)&#xA;        at org.apache.kudu.client.Connection.exceptionCaught(Connection.java:439)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)&#xA;        at org.apache.kudu.client.Connection.handleUpstream(Connection.java:239)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.SimpleChannelUpstreamHandler.exceptionCaught(SimpleChannelUpstreamHandler.java:153)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.Channels.fireExceptionCaught(Channels.java:536)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.handler.timeout.ReadTimeoutHandler.readTimedOut(ReadTimeoutHandler.java:236)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.handler.timeout.ReadTimeoutHandler$ReadTimeoutTask$1.run(ReadTimeoutHandler.java:276)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.socket.ChannelRunnableWrapper.run(ChannelRunnableWrapper.java:40)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:391)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:315)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)&#xA;        at org.apache.kudu.shaded.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)&#xA;        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;        ... 1 more&#xA;Caused by: org.apache.kudu.client.RecoverableException: [peer master-172.17.0.43:7077] encountered a read timeout; closing the channel&#xA;        at org.apache.kudu.client.Connection.exceptionCaught(Connection.java:412)&#xA;        ... 21 more&#xA;Caused by: org.apache.kudu.shaded.org.jboss.netty.handler.timeout.ReadTimeoutException&#xA;        at org.apache.kudu.shaded.org.jboss.netty.handler.timeout.ReadTimeoutHandler.&amp;lt;clinit&amp;gt;(ReadTimeoutHandler.java:84)&#xA;        at org.apache.kudu.client.Connection$ConnectionPipeline.init(Connection.java:782)&#xA;        at org.apache.kudu.client.Connection.&amp;lt;init&amp;gt;(Connection.java:199)&#xA;        at org.apache.kudu.client.ConnectionCache.getConnection(ConnectionCache.java:133)&#xA;        at org.apache.kudu.client.AsyncKuduClient.newRpcProxy(AsyncKuduClient.java:248)&#xA;        at org.apache.kudu.client.AsyncKuduClient.newMasterRpcProxy(AsyncKuduClient.java:272)&#xA;        at org.apache.kudu.client.ConnectToCluster.run(ConnectToCluster.java:157)&#xA;        at org.apache.kudu.client.AsyncKuduClient.getMasterTableLocationsPB(AsyncKuduClient.java:1350)&#xA;        at org.apache.kudu.client.AsyncKuduClient.exportAuthenticationCredentials(AsyncKuduClient.java:651)&#xA;        at org.apache.kudu.client.KuduClient.exportAuthenticationCredentials(KuduClient.java:293)&#xA;        at org.apache.kudu.spark.kudu.KuduContext$$anon$1.run(KuduContext.scala:97)&#xA;        at org.apache.kudu.spark.kudu.KuduContext$$anon$1.run(KuduContext.scala:96)&#xA;        at java.security.AccessController.doPrivileged(Native Method)&#xA;        at javax.security.auth.Subject.doAs(Subject.java:360)&#xA;        at org.apache.kudu.spark.kudu.KuduContext.&amp;lt;init&amp;gt;(KuduContext.scala:96)&#xA;        at org.apache.kudu.spark.kudu.KuduRelation.&amp;lt;init&amp;gt;(DefaultSource.scala:162)&#xA;        at org.apache.kudu.spark.kudu.DefaultSource.createRelation(DefaultSource.scala:75)&#xA;        at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:306)&#xA;        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)&#xA;        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:146)&#xA;        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#xA;        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#xA;        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#xA;        at java.lang.reflect.Method.invoke(Method.java:498)&#xA;        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#xA;        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#xA;        at py4j.Gateway.invoke(Gateway.java:280)&#xA;        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#xA;        at py4j.commands.CallCommand.execute(CallCommand.java:79)&#xA;        at py4j.GatewayConnection.run(GatewayConnection.java:214)&#xA;        ... 1 more&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Case 2: using spark-shell (Spark 1.6.0 standalone):&lt;/h2&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;$ spark-shell --master spark://localhost:7077 --packages org.apache.kudu:kudu-spark_2.10:1.1.0&#xA;&amp;gt; import org.apache.kudu.spark.kudu._&#xA;&amp;gt; import org.apache.kudu.client._&#xA;&amp;gt; import collection.JavaConverters._&#xA;&amp;gt; val df = sqlContext.read.options(Map(&quot;kudu.master&quot; -&amp;gt; &quot;localhost:7051&quot;,&quot;kudu.table&quot; -&amp;gt; &quot;impala::default.test&quot;)).kudu&#xA;df: org.apache.spark.sql.DataFrame = [dataset: string, id: string, itemnumber: string, srcid: string, timestamp: string, year: string, month: string, day: string, week: string, quarter: string, season: string, city: string, region1: string, region2: string, region3: string, region4: string, locality: string, itemname: string, itembqu: string, product_category: string, amount: string, mapped_zipcode: string, latitude: string, longitude: string, depositor_code: string, depositor_name: string, customer_code: string, is_island: string]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It seems to be connecting, as it is able to show the column names, but if I&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// register a temporary table and use SQL&#xA;df.registerTempTable(&quot;test&quot;)&#xA;val filteredDF = sqlContext.sql(&quot;select count(*) from test&quot;).show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;bang!&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[Stage 0:&amp;gt;                                                          (0 + 6) / 6]&#xA;Lost task 1.0 in stage 0.0 (TID 1, tt-slave-2.novalocal, executor 1): org.apache.kudu.client.NonRecoverableException: RPC can not complete before timeout: KuduRpc(method=GetTableSchema, tablet=null, attempt=30, DeadlineTracker(timeout=30000, elapsed=27307), Traces: &#xA;&#xA;[0ms] querying master, &#xA;[48ms] Sub rpc: GetMasterRegistration sending RPC to server Kudu Master - localhost:7051, &#xA;[71ms] Sub rpc: GetMasterRegistration received from server Kudu Master - localhost:7051 response &#xA;Network error: &#xA;[Peer Kudu Master - localhost:7051] Connection reset, &#xA;[75ms] delaying RPC due to Service unavailable: Master config (localhost:7051) has no leader. &#xA;Exceptions received: org.apache.kudu.client.RecoverableException: &#xA;[Peer Kudu Master - localhost:7051] Connection reset, &#xA;&#xA;...&#xA;(SAME MESSAGE REPEATS 25 TIMES)&#xA;...&#xA;&#xA;[24262ms] querying master, &#xA;[24262ms] Sub rpc: GetMasterRegistration sending RPC to server Kudu Master - localhost:7051, &#xA;[24263ms] Sub rpc: GetMasterRegistration received from server Kudu Master - localhost:7051 response &#xA;Network error: &#xA;[Peer Kudu Master - localhost:7051] Connection reset, &#xA;[24263ms] delaying RPC due to Service unavailable: Master config (localhost:7051) has no leader. &#xA;Exceptions received: org.apache.kudu.client.RecoverableException: &#xA;[Peer Kudu Master - localhost:7051] Connection reset, &#xA;[24661ms] trace too long, truncated)&#xA;&#xA;        at org.apache.kudu.client.AsyncKuduClient.tooManyAttemptsOrTimeout(AsyncKuduClient.java:961)&#xA;        at org.apache.kudu.client.AsyncKuduClient.delayedSendRpcToTablet(AsyncKuduClient.java:1203)&#xA;        at org.apache.kudu.client.AsyncKuduClient.access$800(AsyncKuduClient.java:110)&#xA;        at org.apache.kudu.client.AsyncKuduClient$RetryRpcErrback.call(AsyncKuduClient.java:764)&#xA;        at org.apache.kudu.client.AsyncKuduClient$RetryRpcErrback.call(AsyncKuduClient.java:754)&#xA;        at com.stumbleupon.async.Deferred.doCall(Deferred.java:1278)&#xA;        at com.stumbleupon.async.Deferred.runCallbacks(Deferred.java:1257)&#xA;        at com.stumbleupon.async.Deferred.callback(Deferred.java:1005)&#xA;        at org.apache.kudu.client.GetMasterRegistrationReceived.incrementCountAndCheckExhausted(GetMasterRegistrationReceived.java:156)&#xA;        at org.apache.kudu.client.GetMasterRegistrationReceived.access$300(GetMasterRegistrationReceived.java:45)&#xA;        at org.apache.kudu.client.GetMasterRegistrationReceived$GetMasterRegistrationErrCB.call(GetMasterRegistrationReceived.java:236)&#xA;        at org.apache.kudu.client.GetMasterRegistrationReceived$GetMasterRegistrationErrCB.call(GetMasterRegistrationReceived.java:225)&#xA;        at com.stumbleupon.async.Deferred.doCall(Deferred.java:1278)&#xA;        at com.stumbleupon.async.Deferred.runCallbacks(Deferred.java:1257)&#xA;        at com.stumbleupon.async.Deferred.callback(Deferred.java:1005)&#xA;        at org.apache.kudu.client.KuduRpc.handleCallback(KuduRpc.java:220)&#xA;        at org.apache.kudu.client.KuduRpc.errback(KuduRpc.java:274)&#xA;        at org.apache.kudu.client.TabletClient.failOrRetryRpc(TabletClient.java:770)&#xA;        at org.apache.kudu.client.TabletClient.failOrRetryRpcs(TabletClient.java:747)&#xA;        at org.apache.kudu.client.TabletClient.cleanup(TabletClient.java:736)&#xA;        at org.apache.kudu.client.TabletClient.channelClosed(TabletClient.java:698)&#xA;        at org.apache.kudu.client.shaded.org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:88)&#xA;        at org.apache.kudu.client.TabletClient.handleUpstream(TabletClient.java:679)&#xA;        at org.apache.kudu.client.shaded.org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)&#xA;        at org.apache.kudu.client.shaded.org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)&#xA;        at org.apache.kudu.client.shaded.org.jboss.netty.handler.timeout.ReadTimeoutHandler.channelClosed(ReadTimeoutHandler.java:176)&#xA;        at org.apache.kudu.client.shaded.org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:88)&#xA;        at org.apache.kudu.client.shaded.org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)&#xA;        at org.apache.kudu.client.shaded.org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)&#xA;        at org.apache.kudu.client.shaded.org.jboss.netty.channel.Channels.fireChannelClosed(Channels.java:468)&#xA;        at org.apache.kudu.client.shaded.org.jboss.netty.channel.Channels$6.run(Channels.java:457)&#xA;        at org.apache.kudu.client.shaded.org.jboss.netty.channel.socket.ChannelRunnableWrapper.run(ChannelRunnableWrapper.java:40)&#xA;        at org.apache.kudu.client.shaded.org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:391)&#xA;        at org.apache.kudu.client.shaded.org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:315)&#xA;        at org.apache.kudu.client.shaded.org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)&#xA;        at org.apache.kudu.client.shaded.org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)&#xA;        at org.apache.kudu.client.shaded.org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)&#xA;        at org.apache.kudu.client.shaded.org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)&#xA;        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;        at java.lang.Thread.run(Thread.java:748)&#xA;Caused by: org.apache.kudu.client.NoLeaderFoundException: Master config (localhost:7051) has no leader. Exceptions received: org.apache.kudu.client.RecoverableException: [Peer Kudu Master - localhost:7051] Connection reset&#xA;        at org.apache.kudu.client.GetMasterRegistrationReceived.incrementCountAndCheckExhausted(GetMasterRegistrationReceived.java:154)&#xA;        ... 32 more&#xA;Caused by: org.apache.kudu.client.RecoverableException: [Peer Kudu Master - localhost:7051] Connection reset&#xA;        at org.apache.kudu.client.TabletClient.cleanup(TabletClient.java:734)&#xA;        ... 21 more&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As I said, Kudu service is up an running, and I am able to query kudu tables from Hue using Impala.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What am I missing here? Is this the right approach to interfacing Spark with Kudu?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;" OwnerUserId="4623227" LastEditorUserId="7852833" LastEditDate="2018-03-09T20:07:51.873" LastActivityDate="2018-03-09T20:07:51.873" Title="Cannot connect to Kudu from Spark" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;&lt;apache-spark-sql&gt;&lt;apache-kudu&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49069720" PostTypeId="1" AcceptedAnswerId="49069974" CreationDate="2018-03-02T13:04:56.080" Score="1" ViewCount="31" Body="&lt;pre&gt;&lt;code&gt;import org.apache.log4j.{Level, Logger}&#xA;import org.apache.spark.sql.SparkSession&#xA;import org.apache.spark.sql.functions._&#xA;import org.apache.spark._&#xA;import org.apache.spark.sql.types._&#xA;import org.apache.spark.sql._&#xA;&#xA;object fixedLength {&#xA;&#xA;  def main(args:Array[String]) {&#xA;&#xA;    def getRow(x : String) : Row={    &#xA;    val columnArray = new Array[String](4)&#xA;    columnArray(0)=x.substring(0,3)&#xA;    columnArray(1)=x.substring(3,13)&#xA;    columnArray(2)=x.substring(13,18)&#xA;    columnArray(3)=x.substring(18,22)&#xA;    Row.fromSeq(columnArray)  &#xA;  }&#xA;&#xA;    Logger.getLogger(&quot;org&quot;).setLevel(Level.ERROR)&#xA;&#xA;    val spark = SparkSession.builder().master(&quot;local&quot;).appName(&quot;ReadingCSV&quot;).getOrCreate()&#xA;&#xA;&#xA;    val conf = new SparkConf().setAppName(&quot;FixedLength&quot;).setMaster(&quot;local[*]&quot;).set(&quot;spark.driver.allowMultipleContexts&quot;, &quot;true&quot;);&#xA;    val sc = new SparkContext(conf)    &#xA;    val fruits = sc.textFile(&quot;in/fruits.txt&quot;)&#xA;&#xA;    val schemaString = &quot;id,fruitName,isAvailable,unitPrice&quot;;&#xA;    val fields = schemaString.split(&quot;,&quot;).map( field =&amp;gt; StructField(field,StringType,nullable=true))&#xA;    val schema = StructType(fields)&#xA;&#xA;    val df = spark.createDataFrame(fruits.map { x =&amp;gt; getRow(x)} , schema)&#xA;    df.show() // Error&#xA;    println(&quot;End of the program&quot;)&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm getting error in the df.show() command. &#xA;My file content is &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&#xA;56 apple     TRUE 0.56&#xA;45 pear      FALSE1.34&#xA;34 raspberry TRUE 2.43&#xA;34 plum      TRUE 1.31&#xA;53 cherry    TRUE 1.4 &#xA;23 orange    FALSE2.34&#xA;56 persimmon FALSE23.2&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)&#xA;  java.lang.ClassCastException: org.apache.spark.util.SerializableConfiguration cannot be cast to [B&#xA;      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Can you please help?&lt;/p&gt;&#xA;" OwnerUserId="6867048" LastEditorUserId="1671066" LastEditDate="2018-03-02T13:07:02.613" LastActivityDate="2018-03-02T15:32:24.177" Title="Error with spark Row.fromSeq for a text file" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="49070158" PostTypeId="1" CreationDate="2018-03-02T13:32:50.733" Score="1" ViewCount="29" Body="&lt;p&gt;I am new to AWS glue,&#xA;While implementing &lt;a href=&quot;https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-python-samples-legislators.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-python-samples-legislators.html&lt;/a&gt; tutorial, I am encountering the following error at the to.DF() function call--&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Caused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@13bb5560, see the next exception for details.&#xA;Caused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------&#xA;java.sql.SQLException: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@13bb5560, see the next exception for details.&#xA;Caused by: ERROR XJ040: Failed to start database 'metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@13bb5560, see the next exception for details.&#xA;Traceback (most recent call last):&#xA;  File &quot;&quot;, line 1, in &#xA;  File &quot;/mnt/tmp/spark-cf5111b0-bd13-411c-848c-eebb73e16ae2/userFiles-30fbbe43-826e-49c3-b432-914734180daa/PyGlue.zip/awsglue/dynamicframe.py&quot;, line 128, in toDF&#xA;  File &quot;/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py&quot;, line 1133, in __call__&#xA;  File &quot;/usr/lib/spark/python/pyspark/sql/utils.py&quot;, line 79, in deco&#xA;    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)&#xA;pyspark.sql.utils.IllegalArgumentException: u&quot;Error while instantiating 'org.apache.spark.sql.hive.HiveSessionState':&quot;&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS: I ran the code snippets on the development shell environment provided by AWS &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can someone please help me resolve this?&lt;/p&gt;&#xA;" OwnerUserId="9433953" LastEditorUserId="9433953" LastEditDate="2018-03-02T17:29:29.913" LastActivityDate="2018-03-05T08:30:12.320" Title="aws glue to.DF() function error" Tags="&lt;java&gt;&lt;python&gt;&lt;amazon-web-services&gt;&lt;apache-spark&gt;&lt;hive&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49070303" PostTypeId="1" CreationDate="2018-03-02T13:41:05.633" Score="0" ViewCount="58" Body="&lt;p&gt;Goal is to ready this I have a schema as shown below. How can I parse the nested objects and load it in HIVE table, so far I have this code.&#xA;Running Spark version 2.2.0.2.6.4.0-91&#xA;I need help with this coding, I have attached initial code if anyone can please help.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&#xA;     root&#xA;     |-- CustData: array (nullable = true)&#xA;     |    |-- element: struct (containsNull = true)&#xA;     |    |    |-- TimeStamp: double (nullable = true)&#xA;     |    |    |-- Value_x: double (nullable = true)&#xA;     |    |    |-- Value_y: double (nullable = true)&#xA;     |    |    |-- Value_z: double (nullable = true)&#xA;     |-- Cust_ID: string (nullable = true)&#xA;     |-- Deprt_ID: string (nullable = true)&#xA;     |-- EndTime: double (nullable = true)&#xA;     |-- EndTimeZone: string (nullable = true)&#xA;     |-- Salesd: array (nullable = true)&#xA;     |    |-- element: struct (containsNull = true)&#xA;     |    |    |-- Salesd_Value1: long (nullable = true)&#xA;     |    |    |-- Salesd_Value2: long (nullable = true)&#xA;     |    |    |-- Salesd_Value3: double (nullable = true)&#xA;     |    |    |-- Salesd_Value4: double (nullable = true)&#xA;     |-- Cust_RespData: array (nullable = true)&#xA;     |    |-- element: struct (containsNull = true)&#xA;     |    |    |-- TimeStamp: double (nullable = true)&#xA;     |    |    |-- Cust_RespData_val1: double (nullable = true)&#xA;     |    |    |-- Cust_RespData_val1: double (nullable = true)&#xA;     |    |    |-- Cust_RespData_val1: double (nullable = true)&#xA;     |    |    |-- Cust_RespData_val1: double (nullable = true)&#xA;     |-- Cust_RespData_ID: string (nullable = true)&#xA;&lt;/pre&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/bin/python2&#xA;from pyspark import SparkContext&#xA;from pyspark.sql import SparkSession&#xA;##from pyspark.sql.functions import get_json_object&#xA;from pyspark.sql.functions import *&#xA;from pyspark.sql.types import StringType&#xA;import json&#xA;&#xA;# define context&#xA;sc = SparkContext()&#xA;spark = SparkSession(sc)&#xA;&#xA;# load sources&#xA;jsonFile = &quot;hdfs://loaclhost/data/cust_salesd.json&quot;&#xA;sd = spark.read.json(jsonFile)&#xA;sd.printSchema()&#xA;sdf = sd.select &quot;CustData&quot;,&quot;Cust_ID&quot;,&quot;Deprt_ID&quot;,&quot;DriverID&quot;,&quot;EndTime&quot;,&quot;EndTimeZone&quot;,explode(col(&quot;Salesd&quot;).alias(&quot;Salesd_ROW&quot;)))  &#xA;sdf.show()&#xA;sdf1 = sdf.select(&quot;CustData&quot;,&quot;Cust_ID&quot;,&quot;Deprt_ID&quot;,&quot;DriverID&quot;,&quot;EndTime&quot;,&quot;EndTimeZone&quot;, &quot;Salesd_ROW.Salesd_Value1&quot;, &quot;Salesd_ROW.Salesd_Value2&quot;, &quot;Salesd_ROW.Salesd_Value3&quot;, &quot;Salesd_ROW.Salesd_Value4&quot;)&#xA;&#xA;sdf1.show()   &#xA;## how can I load it to HIVE table ??? &#xA;    spark.stop()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="5983694" LastEditorUserId="5983694" LastEditDate="2018-03-02T17:47:51.277" LastActivityDate="2018-03-02T17:47:51.277" Title="Parse the nested json objects" Tags="&lt;python-2.7&gt;&lt;pyspark&gt;" AnswerCount="0" CommentCount="2" FavoriteCount="1" />
  <row Id="49070334" PostTypeId="1" AcceptedAnswerId="49072907" CreationDate="2018-03-02T13:42:57.437" Score="0" ViewCount="36" Body="&lt;p&gt;I am newbie to Apache Hive and Spark.  I have some existing Hive tables sitting on my Hadoop server that I can run some HQL commands and get what I want out of the table using hive or beeline, e.g, selecting first 5 rows of my table.  Instead of that I want to use Spark to achieve the same goal.  My Spark version on server is 1.6.3.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using below code (I replace my database name and table with &lt;strong&gt;database&lt;/strong&gt; and &lt;strong&gt;table&lt;/strong&gt;):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sc = SparkContext(conf = config)&#xA;sqlContext = HiveContext(sc)&#xA;&#xA;query = sqlContext.createDataFrame(sqlContext.sql(&quot;SELECT * from database.table LIMIT 5&quot;).collect())&#xA;df = query.toPandas()&#xA;df.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I get this error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ValueError: Some of types cannot be determined after inferring.  &#xA;Error:root: An unexpected error occurred while tokenizing input&#xA;The following traceback may be corrupted or invalid&#xA;The error message is: ('EOF in multi-line string', (1, 0))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, I can use beeline with same query and see the results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After a day of googling and searching I modified the code as:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;table_ccx = sqlContext.table(&quot;database.table&quot;)&#xA;table_ccx.registerTemplate(&quot;temp&quot;)&#xA;sqlContext.sql(&quot;SELECT * FROM temp LIMIT 5&quot;).show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now the error is gone but all the row values are null except one or two dates and column names.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also tried&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;table_ccx.refreshTable(&quot;database.table&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and it did not help.  Is there a setting or configuration that I need to ask my IT team to do? I appreciate any help.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;: Having said that, my python code is working for some of the table on Hadoop.  Do not know the problem is because of some entries on table or not? If yes, then how come the corresponding beeline/Hive command is working? &lt;/p&gt;&#xA;" OwnerUserId="3261772" LastEditorUserId="3261772" LastEditDate="2018-03-02T15:22:53.357" LastActivityDate="2018-03-02T16:04:43.507" Title="Query hive table with Spark" Tags="&lt;python-2.7&gt;&lt;apache-spark&gt;&lt;hive&gt;" AnswerCount="1" CommentCount="5" />
  <row Id="49070378" PostTypeId="1" AcceptedAnswerId="49071340" CreationDate="2018-03-02T13:46:07.530" Score="0" ViewCount="32" Body="&lt;p&gt;I'm new in spark and scala and I would like to select several columns from a dataset.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I transformed my data in RDD a file using :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val dataset = sc.textFile(args(0))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then I split my line &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val resu = dataset.map(line =&amp;gt; line.split(&quot;\001&quot;))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But I in my dataset I have a lot of features and I just want to keep some of then (colums 2 and 3)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried this (which works with Pyspark) but It does'nt work.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val resu = dataset.map(line =&amp;gt; line.split(&quot;\001&quot;)[2,3])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I know this is a newbie question but is there someone who can help me ? thanks.&lt;/p&gt;&#xA;" OwnerUserId="9366049" LastEditorUserId="3180489" LastEditDate="2018-03-02T14:45:09.060" LastActivityDate="2018-03-02T14:45:09.060" Title="How to select several element from an RDD file line using Spark in Scala" Tags="&lt;scala&gt;&lt;variables&gt;&lt;apache-spark&gt;&lt;selection&gt;&lt;rdd&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49070848" PostTypeId="1" CreationDate="2018-03-02T14:10:35.313" Score="-2" ViewCount="39" Body="&lt;p&gt;I have 2 arrays in scala, the one have id of a person and ects, the other have id and grade, i want to find the average for this arrays for each person.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;eg arrays below&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;array 1&#xA;&#xA;ID ects&#xA;1  13&#xA;2  19&#xA;2  12&#xA;3  09&#xA;&#xA;&#xA;array 2&#xA;&#xA;ID GRADE&#xA;1  15&#xA;2  19&#xA;1  14&#xA;3  16&#xA;3  20&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;*Avg=(ects * grades)/SumEcts&lt;/p&gt;&#xA;" OwnerUserId="9434366" LastEditorUserId="9434366" LastEditDate="2018-03-02T14:38:52.217" LastActivityDate="2018-03-03T06:49:00.240" Title="Scala RDD for average values" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;average&gt;&lt;rdd&gt;" AnswerCount="1" CommentCount="6" />
  <row Id="49071049" PostTypeId="1" CreationDate="2018-03-02T14:21:51.433" Score="0" ViewCount="43" Body="&lt;p&gt;I am apparently facing a read shuffle problems.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My Pyspark Script is running on a Hadoop cluster 1 EdgeNode and 12 Datanodes, using YARN as resources manager and Spark 1.6.2.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;###[ini_file containing conf spark]&#xA;spark.app.name = MY_PYSPARK_APP&#xA;spark.master = yarn-client&#xA;spark.yarn.queue = agr_queue&#xA;spark.executor.instances = 24&#xA;spark.executor.memory = 14&#xA;spark.executor.cores = 3&#xA;#spark.storage.memoryFraction = 0.5&#xA;#spark.sql.shuffle.partitions = 2001&#xA;#spark.sql.shuffle.partitions = 1000&#xA;spark.sql.shuffle.partitions = 100&#xA;spark.shuffle.memoryFraction=0.5&#xA;spark.memory.offHeap.enabled = True&#xA;spark.serializer = org.apache.spark.serializer.KryoSerializer&#xA;#spark.driver.memory = 14g&#xA;spark.driver.maxResultSize = 20g&#xA;spark.python.worker.memory = 14g&#xA;spark.akka.heartbeat.interval = 100&#xA;spark.yarn.executor.memoryOverhead=2000&#xA;spark.yarn.driver.memoryOverhead=2000&#xA;spark.scheduler.mode = FIFO&#xA;spark.sql.tungsten.enabled = True&#xA;spark.default.parallelism = 200&#xA;spark.speculation = True&#xA;spark.speculation.interval = 1000ms&#xA;spark.speculation.multiplier = 2.0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h3&gt;Python script&lt;/h3&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sconf = SparkConf()&#xA;sc = SparkContext(sconf)&#xA;hctx = HiveContext(sc)&#xA;&#xA;dataframe1 = hctx.sql(&quot;SELECT * FROM DB1.TABLE1&quot;)&#xA;dataframe2 = hctx.sql(&quot;SELECT * FROM DB2.TABLE2&quot;)&#xA;&#xA;df = dataframe1.join(dataframe2, conditions)&#xA;&#xA;# No major problem at this count()&#xA;# it returns 550 000 000 rows&#xA;df.count()&#xA;&#xA;# 288 elements in List_dtm_t&#xA;List_dtm_t=['00:00:00', '00:05:00', ... '23:45:00', '23:50:00', '23:55:00']&#xA;dat_tm_bdcst = sc.broadcast(List_dtm)&#xA;global dat_tm_bdcst&#xA;&#xA;def mapper(row):&#xA;        import datetime&#xA;        def ts_minus_5(tmstmp):&#xA;            import datetime&#xA;            return tmstmp-datetime.timedelta(minutes=5)&#xA;&#xA;        lst_tuple = ()&#xA;        poids = row[9]&#xA;&#xA;        for dtm in dat_tm_bdcst.value:&#xA;            t_minus = ts_minus_5(dtm)&#xA;&#xA;            if (row[0]&amp;lt;=dtm) &amp;amp; (row[1]&amp;gt;t_minus):&#xA;&#xA;                v1 = str(dtm)&#xA;                v2 = str(t_minus)&#xA;                v3 = row[2]&#xA;                v4 = row[3]&#xA;                v5 = row[4]&#xA;                v6 = row[5]&#xA;                v7 = row[6]&#xA;                v8 = row[7]&#xA;                v9 = row[8]&#xA;                v10 = row[10]&#xA;                v11 = poids * (min(dtm,row[1])-max(t_minus,row[0])).total_seconds()&#xA;                v12 = poids&#xA;&#xA;                if row[0] &amp;lt;= dtm &amp;lt;= row[1] : v13 = poids&#xA;                else : v13 = 0&#xA;&#xA;                lst_tuple += (((v1, v2, v3, v4, v5, v6, v7, v8, v9, v10),(v11, v12, v13)),)&#xA;&#xA;        return lst_tuple&#xA;&#xA;global list_to_row&#xA;def list_to_row(keys, values):&#xA;    from pyspark.sql import Row&#xA;    row_dict = dict(zip(keys, values[0]+values[1]))&#xA;    return Row(**row_dict)&#xA;&#xA;f_reduce = lambda x,y: (x[0]+y[0], x[1]+y[1], x[2]+y[2])&#xA;&#xA;# This flatMap takes a really infinite long time&#xA;# It generally returns a KO because it retries more than 3 times&#xA;# Or lose some shuffle path&#xA;mapped_df = df.limit(10000000)\&#xA;              .flatMap(mapper)&#xA;&#xA;reduced_rdd = mapped_df.reduceByKey(f_reduce)&#xA;&#xA;reduced_rdd.count()&#xA;&#xA;list_of_rows = reduced_rdd.map(lambda x: list_to_row(header, x))&#xA;&#xA;df_to_exp = hctx.createDataFrame(list_of_rows)&#xA;&#xA;## register as tempTable df_to_exp then write it into Hive&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I tried different ways like :&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Resolve skew problem using repartition([keys]) to distribute data by keys used by the reducer then&lt;/li&gt;&#xA;&lt;li&gt;Different values for spark.sql.shuffle.partitions, spark.default.parallelism and memoryOverhead conf&lt;/li&gt;&#xA;&lt;li&gt;A partial dataframe version using grouypBy&lt;/li&gt;&#xA;&lt;li&gt;Use persistence even if I pass over the data only one time&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;I am looking for solution to reach the end and also speed up the process.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Two screenshot of spark UI:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/BV2ls.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;List of Stages&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/i4hbK.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;ReduceByKey Task&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We can see the ReduceByKey stage (don't know if it represents only the reduce task, with only 1 task ?!!)&#xA;And the shuffle read /records which inscrease too slowly (300 000/100Millions after 13 minutes)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hope someone could help,&#xA;Thanks !&lt;/p&gt;&#xA;" OwnerUserId="5848202" LastActivityDate="2018-03-02T14:21:51.433" Title="Process a 1/2 billion rows with PySpark creates shuffle read problems" Tags="&lt;apache-spark&gt;&lt;mapreduce&gt;&lt;pyspark&gt;&lt;spark-dataframe&gt;&lt;yarn&gt;" AnswerCount="0" CommentCount="0" FavoriteCount="1" />
  <row Id="49071424" PostTypeId="1" CreationDate="2018-03-02T14:43:31.447" Score="1" ViewCount="48" Body="&lt;p&gt;I've configured an AWS Glue dev endpoint and can connect to it successfully in a pyspark REPL shell - like this &lt;a href=&quot;https://docs.aws.amazon.com/glue/latest/dg/dev-endpoint-tutorial-repl.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://docs.aws.amazon.com/glue/latest/dg/dev-endpoint-tutorial-repl.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Unlike the example given in the AWS documentation I receive WARNings when I begin the session, and later on various operations on AWS Glue DynamicFrame structures fail. Here's the full log on starting the session - note the errors about spark.yarn.jars and PyGlue.zip:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Python 2.7.12 (default, Sep  1 2016, 22:14:00)&#xA;[GCC 4.8.3 20140911 (Red Hat 4.8.3-9)] on linux2&#xA;Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&#xA;Setting default log level to &quot;WARN&quot;.&#xA;To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).&#xA;SLF4J: Class path contains multiple SLF4J bindings.&#xA;SLF4J: Found binding in [jar:file:/usr/share/aws/glue/etl/jars/glue-assembly.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#xA;SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]&#xA;SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.&#xA;SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]&#xA;18/03/02 14:18:58 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.&#xA;18/03/02 14:19:03 WARN Client: Same path resource file:/usr/share/aws/glue/etl/python/PyGlue.zip added multiple times to distributed cache.&#xA;18/03/02 14:19:13 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException&#xA;Welcome to&#xA;      ____              __&#xA;     / __/__  ___ _____/ /__&#xA;    _\ \/ _ \/ _ `/ __/  '_/&#xA;   /__ / .__/\_,_/_/ /_/\_\   version 2.1.0&#xA;      /_/&#xA;&#xA;Using Python version 2.7.12 (default, Sep  1 2016 22:14:00)&#xA;SparkSession available as 'spark'.&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Many operations work as I expect but I also receive some unwelcome exceptions, for example I can load data from my Glue catalog inspect its structure and the data within, but I can't apply a Map to it, or convert it to a DF. Here's my full execution run log (apart from the longest error message). The first few commands and setup all work well, but the final two operations fail:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import sys&#xA;&amp;gt;&amp;gt;&amp;gt; from awsglue.transforms import *&#xA;&amp;gt;&amp;gt;&amp;gt; from awsglue.utils import getResolvedOptions&#xA;&amp;gt;&amp;gt;&amp;gt; from pyspark.context import SparkContext&#xA;&amp;gt;&amp;gt;&amp;gt; from awsglue.context import GlueContext&#xA;&amp;gt;&amp;gt;&amp;gt; from awsglue.job import Job&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt; glueContext = GlueContext(spark)&#xA;&amp;gt;&amp;gt;&amp;gt; # Receives a string of the format yyyy-mm-dd hh:mi:ss.nnn and returns the first 10 characters: yyyy-mm-dd&#xA;... def TruncateTimestampString(ts):&#xA;...   ts = ts[:10]&#xA;...   return ts&#xA;...&#xA;&amp;gt;&amp;gt;&amp;gt; TruncateTimestampString('2017-03-05 06:12:08.376')&#xA;'2017-03-05'&#xA;&amp;gt;&amp;gt;&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt; # Given a record with a timestamp property returns a record with a new property, day, containing just the date portion of the timestamp string, expected to be yyyy-mm-dd.&#xA;... def TruncateTimestamp(rec):&#xA;...   rec[day] = TruncateTimestampString(rec[timestamp])&#xA;...   return rec&#xA;...&#xA;&amp;gt;&amp;gt;&amp;gt; # Get the history datasource - WORKS WELL BUT LOGS log4j2 ERROR&#xA;&amp;gt;&amp;gt;&amp;gt; datasource_history_1 = glueContext.create_dynamic_frame.from_catalog(database = &quot;dev&quot;, table_name = &quot;history&quot;, transformation_ctx = &quot;datasource_history_1&quot;)&#xA;ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.&#xA;&amp;gt;&amp;gt;&amp;gt; # Tidy the history datasource - WORKS WELL&#xA;&amp;gt;&amp;gt;&amp;gt; history_tidied = datasource_history_1.drop_fields(['etag', 'jobmaxid', 'jobminid', 'filename']).rename_field('id', 'history_id')&#xA;&amp;gt;&amp;gt;&amp;gt; history_tidied.printSchema()&#xA;root&#xA;|-- jobid: string&#xA;|-- spiderid: long&#xA;|-- timestamp: string&#xA;|-- history_id: long&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; # Trivial observation of the SparkSession objects&#xA;&amp;gt;&amp;gt;&amp;gt; SparkSession&#xA;&amp;lt;class 'pyspark.sql.session.SparkSession'&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt; spark&#xA;&amp;lt;pyspark.sql.session.SparkSession object at 0x7f8668f3b650&amp;gt;&#xA;&amp;gt;&amp;gt;&amp;gt; &#xA;&amp;gt;&amp;gt;&amp;gt; &#xA;&amp;gt;&amp;gt;&amp;gt; # Apply a mapping to the tidied history datasource. FAILS&#xA;&amp;gt;&amp;gt;&amp;gt; history_mapped = history_tidied.map(TruncateTimestamp)&#xA;Traceback (most recent call last):&#xA;File &quot;&amp;lt;stdin&amp;gt;&quot;, line 1, in &amp;lt;module&amp;gt;&#xA;File &quot;/mnt/tmp/spark-1f0341db-5de6-4008-974f-a1d194524a86/userFiles-6a67bdee-7c44-46d6-a0dc-9daa7177e7e2/PyGlue.zip/awsglue/dynamicframe.py&quot;, line 101, in map&#xA;File &quot;/mnt/tmp/spark-1f0341db-5de6-4008-974f-a1d194524a86/userFiles-6a67bdee-7c44-46d6-a0dc-9daa7177e7e2/PyGlue.zip/awsglue/dynamicframe.py&quot;, line 105, in mapPartitionsWithIndex&#xA;File &quot;/usr/lib/spark/python/pyspark/rdd.py&quot;, line 2419, in __init__&#xA;    self._jrdd_deserializer = self.ctx.serializer&#xA;AttributeError: 'SparkSession' object has no attribute 'serializer'&#xA;&amp;gt;&amp;gt;&amp;gt; history_tidied.toDF()&#xA;ERROR&#xA;Huge error log and stack trace follows, longer than my console can remember. Here's how it finishes:&#xA;Traceback (most recent call last):&#xA;  File &quot;&amp;lt;stdin&amp;gt;&quot;, line 1, in &amp;lt;module&amp;gt;&#xA;  File &quot;/mnt/tmp/spark-1f0341db-5de6-4008-974f-a1d194524a86/userFiles-6a67bdee-7c44-46d6-a0dc-9daa7177e7e2/PyGlue.zip/awsglue/dynamicframe.py&quot;, line 128, in toDF&#xA;  File &quot;/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py&quot;, line 1133, in __call__&#xA;  File &quot;/usr/lib/spark/python/pyspark/sql/utils.py&quot;, line 79, in deco&#xA;    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)&#xA;pyspark.sql.utils.IllegalArgumentException: u&quot;Error while instantiating 'org.apache.spark.sql.hive.HiveSessionState':&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I think I'm following the instructions given by Amazon in their Dev Endpoint REPL instructions, but with these fairly simple operations (DynamicFrame.join and DynamicFrame.toDF) failing I'm working in the dark when I want to run the job for real (which seems to succeed, but my DynamicFrame.printSchema() and DynamicFrame.show() commands don't show up in the CloudWatch logs for the execution).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know what do I need to do to fix my REPL environment so that I can properly test pyspark AWS Glue scripts?&lt;/p&gt;&#xA;" OwnerUserId="4046384" LastActivityDate="2018-03-03T17:23:07.883" Title="Unable to run scripts peoperly in AWS Glue PySpark Dev Endpoint - receive warnings when starting the shell" Tags="&lt;amazon-web-services&gt;&lt;pyspark&gt;&lt;aws-glue&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49071533" PostTypeId="1" CreationDate="2018-03-02T14:48:29.070" Score="1" ViewCount="46" Body="&lt;p&gt;Running the following code repeatedly generates inconsistent results.  So far, I have only seen two outputs.  The results get repeated any random number of times before switching to the other results, which then also repeat any random number of times before switching back again.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why is this happening?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this example I could use the indexing window function and include an &lt;code&gt;orderBy()&lt;/code&gt; before I use &lt;code&gt;%&lt;/code&gt; to modify the single column, but my real example, I do not have this option, so this is not a solution for me.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pyspark&#xA;spark = pyspark.sql.SparkSession.builder.getOrCreate()&#xA;import pyspark.sql.functions as F &#xA;from pyspark.sql.window import Window as W&#xA;&#xA;window = W.rowsBetween(W.unboundedPreceding, W.currentRow)&#xA;testCol = [tuple([x]) for x in range(1,5000)]&#xA;&#xA;# repeatedly re-run from here:&#xA;testDF = (spark.createDataFrame(testCol,['testCol'])&#xA;              .withColumn('testCol',&#xA;                          F.when(F.col('testCol') % 2 == 0, &#xA;                             F.col('testCol'))&#xA;                          .otherwise(0.0))              &#xA;              .withColumn('int', F.lit(1))&#xA;              .withColumn('index', F.sum('int').over(window))&#xA;              .drop('int') &#xA;)&#xA;&#xA;testDF.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Result 1 (expected):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+-------+-----+&#xA;|testCol|index|&#xA;+-------+-----+&#xA;|    0.0|    1|&#xA;|    2.0|    2|&#xA;|    0.0|    3|&#xA;|    4.0|    4|&#xA;|    0.0|    5|&#xA;|    6.0|    6|&#xA;|    0.0|    7|&#xA;|    8.0|    8|&#xA;|    0.0|    9|&#xA;|   10.0|   10|&#xA;|    0.0|   11|&#xA;|   12.0|   12|&#xA;|    0.0|   13|&#xA;|   14.0|   14|&#xA;|    0.0|   15|&#xA;|   16.0|   16|&#xA;|    0.0|   17|&#xA;|   18.0|   18|&#xA;|    0.0|   19|&#xA;|   20.0|   20|&#xA;+-------+-----+&#xA;only showing top 20 rows&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Result 2 (not expected):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+-------+-----+&#xA;|testCol|index|&#xA;+-------+-----+&#xA;|    0.0|    1|&#xA;| 2050.0|    2|&#xA;|    0.0|    3|&#xA;| 2052.0|    4|&#xA;|    0.0|    5|&#xA;| 2054.0|    6|&#xA;|    0.0|    7|&#xA;| 2056.0|    8|&#xA;|    0.0|    9|&#xA;| 2058.0|   10|&#xA;|    0.0|   11|&#xA;| 2060.0|   12|&#xA;|    0.0|   13|&#xA;| 2062.0|   14|&#xA;|    0.0|   15|&#xA;| 2064.0|   16|&#xA;|    0.0|   17|&#xA;| 2066.0|   18|&#xA;|    0.0|   19|&#xA;| 2068.0|   20|&#xA;+-------+-----+&#xA;only showing top 20 rows&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This code also produces the exact same inconsistent output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;testDF = (spark.createDataFrame(testCol,['testCol'])&#xA;              .repartition(1) # to address how monotonically_increasing_id works&#xA;              .withColumn('id', F.monotonically_increasing_id())            &#xA;              .withColumn('testCol',&#xA;                          F.when(F.col('testCol') % 2 == 0, &#xA;                             F.col('testCol'))&#xA;                          .otherwise(0.0))              &#xA;)&#xA;&#xA;testDF.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="5060792" LastEditorUserId="5060792" LastEditDate="2018-03-02T15:11:56.073" LastActivityDate="2018-03-02T15:11:56.073" Title="Inconsistent results in pyspark" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;&lt;apache-spark-sql&gt;&lt;spark-dataframe&gt;&lt;pyspark-sql&gt;" AnswerCount="0" CommentCount="11" />
  <row Id="49071799" PostTypeId="1" CreationDate="2018-03-02T15:03:35.820" Score="0" ViewCount="28" Body="&lt;p&gt;We have been working with Thingsboard for a few weeks, trying to configure a data-analytics solution to gather and process data from a crop. So far, we managed to:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Configure new alarms based on device readings.&lt;/li&gt;&#xA;&lt;li&gt;Configure the architecture proposed in &lt;a href=&quot;https://thingsboard.io/docs/samples/analytics/spark-integration-with-thingsboard&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://thingsboard.io/docs/samples/analytics/spark-integration-with-thingsboard&lt;/a&gt; (spark+kafka), to read/display (from an asset) data generated by a spark Worker node.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;However, we have a particular requirement we haven't been able to implement (we are stuck!). We need Thingsboard to trigger an alarm depending on the outcome of a Spark Worker node (a node that could use a machine-learning classifier or an expert system). As far we understand, the alarm triggering system is configured to work for data sent through the MQTT protocol (we already identified the Akka actor which handle such events). However, Spark+Kafka pushes its outgoing data to Thingsboard through HTTP, and is received by a different Akka Actor, so we haven't found a way to handle such data with the alarm triggering mechanism.&#xA;Could anyone please tell us if it is possible to define alarms based on Spark process outcomes? if not, could you suggest the best approach to achieve this? (for example, modifying the source code to redirect Spark's outcomes to a MQTT topic?).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any advice is really appreciated!&lt;/p&gt;&#xA;" OwnerUserId="9410845" LastActivityDate="2018-03-02T15:03:35.820" Title="Thingsboard alarms based on spark analytics" Tags="&lt;apache-spark&gt;&lt;thingsboard&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49071917" PostTypeId="1" AcceptedAnswerId="49117395" CreationDate="2018-03-02T15:10:32.310" Score="1" ViewCount="44" Body="&lt;p&gt;I am trying to start an Ignite Context using the Ignite-Spark plugin, for Ignite version 2.2.0.&#xA;This is how I am declaring the context:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val igniteContext:IgniteContext=new IgniteContext(sparkSession.sparkContext,()=&amp;gt;new IgniteConfiguration().setDiscoverySpi(new TcpDiscoverySpi().&#xA;      setLocalAddress(&quot;127.0.0.1&quot;).setLocalPort(48511).setIpFinder(new TcpDiscoveryMulticastIpFinder().&#xA;      setAddresses(new util.ArrayList[String]))).setCommunicationSpi(new TcpCommunicationSpi().setLocalPort(48512)),true)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However I get this error when the context is attempting to start:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;IgniteException: Failed to load class names properties file packaged with ignite binaries &#xA;[file=META-INF/classnames.properties, &#xA;ldr=org.apache.spark.util.MutableURLClassLoader@2bbaf4f0]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I read here &lt;a href=&quot;http://apache-ignite-developers.2346864.n4.nabble.com/classnames-properties-file-is-out-of-date-td2213.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://apache-ignite-developers.2346864.n4.nabble.com/classnames-properties-file-is-out-of-date-td2213.html&lt;/a&gt; that &quot;'classnames.properties' file is&#xA;located in ignite-core/META-INF folder. The file is internally used by &#xA;Ignite marshallers.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am not too familiar with Ignite's internals. &#xA;Could this be from the location of  my Ignite jar dependency? I used sbt assembly to build my .jar file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you for your time.&#xA;T&lt;/p&gt;&#xA;" OwnerUserId="4135691" LastActivityDate="2018-03-05T18:43:53.540" Title="Failed to load class name properties in Apache Ignite" Tags="&lt;java&gt;&lt;apache-spark&gt;&lt;sbt&gt;&lt;ignite&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="49072727" PostTypeId="1" CreationDate="2018-03-02T15:55:12.113" Score="0" ViewCount="33" Body="&lt;p&gt;I am running a py file through the command:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/opt/cloudera/parcels/SPARK2-2.2.0.cloudera2-1.cdh5.12.0.p0.232957/bin/spark2-submit --jars /home/jsonnt200/geomesa-hbase-spark-runtime_2.11-1.3.5.1cc.jar,/ccri/hbase-site.zip geomesa_klondike_enrichment2.py&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This results in the following error:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Traceback (most recent call last):   File&#xA;  &quot;/home/jsonnt200/geomesa_klondike_enrichment2.py&quot;, line 6306, in&#xA;  &#xA;      df2_500m.write.option('header', 'true').csv('/user/jsonnt200/klondike_201708_1m_500meter_testEQ_union4')&#xA;  File&#xA;  &quot;/opt/cloudera/parcels/SPARK2-2.1.0.cloudera2-1.cdh5.7.0.p0.171658/lib/spark2/python/pyspark/sql/readwriter.py&quot;,&#xA;  line 711, in csv&#xA;      self._jwrite.csv(path)   File &quot;/opt/cloudera/parcels/SPARK2-2.1.0.cloudera2-1.cdh5.7.0.p0.171658/lib/spark2/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py&quot;,&#xA;  line 1133, in &lt;strong&gt;call&lt;/strong&gt;   File&#xA;  &quot;/opt/cloudera/parcels/SPARK2-2.1.0.cloudera2-1.cdh5.7.0.p0.171658/lib/spark2/python/pyspark/sql/utils.py&quot;,&#xA;  line 79, in deco&#xA;      raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace) pyspark.sql.utils.IllegalArgumentException: u'Illegal pattern&#xA;  component: XXX'&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The biggest concern is if I submit this same py file through ipython, it runs correctly. Any ideas on what could be the issue? Unfortunately, I have to use the spark2-submit for tunnelling purposes.&lt;/p&gt;&#xA;" OwnerUserId="6369215" LastEditorUserId="8332344" LastEditDate="2018-03-06T14:52:18.883" LastActivityDate="2018-03-06T14:52:18.883" Title="Pyspark2 Writing to CSV Issue?" Tags="&lt;python&gt;&lt;csv&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49073533" PostTypeId="1" CreationDate="2018-03-02T16:41:15.273" Score="0" ViewCount="33" Body="&lt;p&gt;I tried following command on hive :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;set hive.execution.engine=spark;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;but getting error when I run any query after setting execution engine on amazon EMR:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: scala/collection/Iterable&#xA;        at org.apache.hadoop.hive.ql.parse.spark.GenSparkProcContext.&amp;lt;init&amp;gt;(GenSparkProcContext.java:163)&#xA;        at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.generateTaskTree(SparkCompiler.java:328)&#xA;        at ---------------&#xA;    Caused by: java.lang.ClassNotFoundException: scala.collection.Iterable&#xA;        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)&#xA;        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&#xA;        at ---------------&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I tried to copy jar from spark to hive path and spark conf files to hive conf , &#xA;still getting error, any fix ?&lt;/p&gt;&#xA;" OwnerUserId="5284326" LastEditorUserId="5766301" LastEditDate="2018-03-02T21:35:43.937" LastActivityDate="2018-03-03T06:19:01.957" Title="hive on spark on amazon EMR" Tags="&lt;apache-spark&gt;&lt;hive&gt;&lt;amazon-emr&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49073915" PostTypeId="1" AcceptedAnswerId="49074229" CreationDate="2018-03-02T17:03:00.197" Score="0" ViewCount="37" Body="&lt;p&gt;I have a dataframe like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+-----+------+&#xA;|A    |     B|&#xA;+-----+------+&#xA;|    1|     2|&#xA;|  200|     0|&#xA;|  300|     4| &#xA;+-----+------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to convert that to a list of 1s for each A and 0s for each B, create a list from them and calculate their standard deviation and add that as Column C to the dataframe. So for example, for the first row we would calculate the standard deviation of [1, 0, 0].&#xA;Is that possible in pyspark?&lt;/p&gt;&#xA;" OwnerUserId="9435177" LastEditorUserId="13860" LastEditDate="2018-03-02T17:03:46.577" LastActivityDate="2018-03-02T17:22:59.013" Title="convert values and calculate stddev" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="49074070" PostTypeId="1" AcceptedAnswerId="49077266" CreationDate="2018-03-02T17:11:49.483" Score="1" ViewCount="50" Body="&lt;p&gt;I have a Spark/Scala job in which I do this:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;1: Compute a big DataFrame &lt;code&gt;df1&lt;/code&gt; + &lt;code&gt;cache&lt;/code&gt; it into memory&lt;/li&gt;&#xA;&lt;li&gt;2: Use &lt;code&gt;df1&lt;/code&gt; to compute &lt;code&gt;dfA&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;3: Read raw data into &lt;code&gt;df2&lt;/code&gt; (again, its big) + &lt;code&gt;cache&lt;/code&gt; it&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;When performing (3), I do no longer need &lt;code&gt;df1&lt;/code&gt;. I want to make sure its space gets freed. I &lt;code&gt;cached&lt;/code&gt; at (1) because this DataFrame gets used in (2) and its the only way to make sure I do not recompute it each time but only once.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I need to free its space and make sure it gets freed. What are my options?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I thought of these, but it doesn't seem to be sufficient:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;df=null&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;df.unpersist()&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Can you document your answer with a proper Spark documentation link?&lt;/p&gt;&#xA;" OwnerUserId="7115301" LastEditorUserId="7115301" LastEditDate="2018-03-02T19:46:49.017" LastActivityDate="2018-03-02T20:56:33.743" Title="How to make sure my DataFrame frees its memory?" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;garbage-collection&gt;&lt;spark-dataframe&gt;" AnswerCount="2" CommentCount="7" />
  <row Id="49075356" PostTypeId="1" AcceptedAnswerId="49078575" CreationDate="2018-03-02T18:37:31.603" Score="0" ViewCount="28" Body="&lt;p&gt;I'm using maven to create a jar that i can launch.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So i do : &lt;code&gt;mvn clean install&lt;/code&gt; and i get the jar.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When i do : &lt;code&gt;java -jar target/ProjetMRS-0.0.1-SNAPSHOT.jar&lt;/code&gt; i got this error below :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: com/mongodb/MongoClient                                                       &#xA;        at iut.nantes.Projet.service.ServicePersonne.&amp;lt;init&amp;gt;(ServicePersonne.java:21)                                                  &#xA;        at iut.nantes.projetMRS.Api.&amp;lt;clinit&amp;gt;(Api.java:21)                                                                                &#xA;Caused by: java.lang.ClassNotFoundException: com.mongodb.MongoClient                                                                     &#xA;        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)                                                                    &#xA;        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)                                                                         &#xA;        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)                                                                 &#xA;        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)                                                                         &#xA;        ... 2 more&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And when i look at the file all import are up-to-date :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;package iut.nantes.projetMRS.service;&#xA;&#xA;import java.util.Calendar;&#xA;import java.util.Date;&#xA;import java.util.List;&#xA;&#xA;import org.bson.types.ObjectId;&#xA;import org.mongodb.morphia.Datastore;&#xA;import org.mongodb.morphia.Morphia;&#xA;import org.mongodb.morphia.query.Query;&#xA;import org.mongodb.morphia.query.UpdateOperations;&#xA;&#xA;import com.mongodb.MongoClient;&#xA;import iut.nantes.projetMRS.entity.EntityPersonne;&#xA;&#xA;public class ServicePersonne {&#xA;    /*Error here ==&amp;gt;*/ MongoClient client = new MongoClient(&quot;localhost&quot;, 27017); //connect to mongodb&#xA;[...]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And the last thing that's very strange is that when i launch my application on my own computer, every thing works fine, i got 0 error and MongoClient is found.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've looked here : &lt;code&gt;https://stackoverflow.com/questions/45045681/how-do-i-fix-a-noclassdeffounderror-for-mongoclient-in-my-sparkjava-app&lt;/code&gt; but nobody answered him&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've found some answer but none of them worked for my case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Below i show you my pom file with all dependencies up-to-date too :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&#xA;    xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&amp;gt;&#xA;    &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;&#xA;&#xA;    &amp;lt;groupId&amp;gt;iut.nantes&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;projetMRS&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;0.0.1-SNAPSHOT&amp;lt;/version&amp;gt;&#xA;    &amp;lt;packaging&amp;gt;jar&amp;lt;/packaging&amp;gt;&#xA;&#xA;    &amp;lt;name&amp;gt;projetMRS&amp;lt;/name&amp;gt;&#xA;    &amp;lt;url&amp;gt;http://maven.apache.org&amp;lt;/url&amp;gt;&#xA;&#xA;    &amp;lt;properties&amp;gt;&#xA;        &amp;lt;project.build.sourceEncoding&amp;gt;UTF-8&amp;lt;/project.build.sourceEncoding&amp;gt;&#xA;    &amp;lt;/properties&amp;gt;&#xA;&#xA;    &amp;lt;build&amp;gt;&#xA;        &amp;lt;plugins&amp;gt;&#xA;            &amp;lt;plugin&amp;gt;&#xA;                &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;&#xA;                &amp;lt;artifactId&amp;gt;maven-compiler-plugin&amp;lt;/artifactId&amp;gt;&#xA;                &amp;lt;version&amp;gt;3.3&amp;lt;/version&amp;gt;&#xA;                &amp;lt;configuration&amp;gt;&#xA;                    &amp;lt;source&amp;gt;1.8&amp;lt;/source&amp;gt;&#xA;                    &amp;lt;target&amp;gt;1.8&amp;lt;/target&amp;gt;&#xA;                &amp;lt;/configuration&amp;gt;&#xA;            &amp;lt;/plugin&amp;gt;&#xA;            &amp;lt;plugin&amp;gt;&#xA;                &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;&#xA;                &amp;lt;artifactId&amp;gt;maven-jar-plugin&amp;lt;/artifactId&amp;gt;&#xA;                &amp;lt;version&amp;gt;3.0.0&amp;lt;/version&amp;gt;&#xA;                &amp;lt;configuration&amp;gt;&#xA;                    &amp;lt;source&amp;gt;1.8&amp;lt;/source&amp;gt;&#xA;                    &amp;lt;target&amp;gt;1.8&amp;lt;/target&amp;gt;&#xA;                    &amp;lt;archive&amp;gt;&#xA;                        &amp;lt;manifest&amp;gt;&#xA;                            &amp;lt;addClasspath&amp;gt;true&amp;lt;/addClasspath&amp;gt;&#xA;                            &amp;lt;mainClass&amp;gt;iut.nantes.projetMRS.Api&amp;lt;/mainClass&amp;gt;&#xA;                        &amp;lt;/manifest&amp;gt;&#xA;                    &amp;lt;/archive&amp;gt;&#xA;                &amp;lt;/configuration&amp;gt;&#xA;            &amp;lt;/plugin&amp;gt;&#xA;        &amp;lt;/plugins&amp;gt;&#xA;    &amp;lt;/build&amp;gt;&#xA;&#xA;    &amp;lt;dependencies&amp;gt;&#xA;        &amp;lt;dependency&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;junit&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;junit&amp;lt;/artifactId&amp;gt;&#xA;            &amp;lt;version&amp;gt;3.8.1&amp;lt;/version&amp;gt;&#xA;            &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&#xA;        &amp;lt;/dependency&amp;gt;&#xA;        &amp;lt;dependency&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;org.mongodb&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;mongo-java-driver&amp;lt;/artifactId&amp;gt;&#xA;            &amp;lt;version&amp;gt;3.6.3&amp;lt;/version&amp;gt;&#xA;        &amp;lt;/dependency&amp;gt;&#xA;        &amp;lt;dependency&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;org.mongodb&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;mongodb-driver&amp;lt;/artifactId&amp;gt;&#xA;            &amp;lt;version&amp;gt;3.6.3&amp;lt;/version&amp;gt;&#xA;        &amp;lt;/dependency&amp;gt;&#xA;        &amp;lt;dependency&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;org.mongodb&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;mongodb-driver-core&amp;lt;/artifactId&amp;gt;&#xA;            &amp;lt;version&amp;gt;3.6.3&amp;lt;/version&amp;gt;&#xA;        &amp;lt;/dependency&amp;gt;&#xA;        &amp;lt;dependency&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;com.sparkjava&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;spark-core&amp;lt;/artifactId&amp;gt;&#xA;            &amp;lt;version&amp;gt;2.3&amp;lt;/version&amp;gt;&#xA;        &amp;lt;/dependency&amp;gt;&#xA;        &amp;lt;dependency&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;org.mongodb&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;bson&amp;lt;/artifactId&amp;gt;&#xA;            &amp;lt;version&amp;gt;3.6.3&amp;lt;/version&amp;gt;&#xA;        &amp;lt;/dependency&amp;gt;&#xA;        &amp;lt;dependency&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;org.mongodb.morphia&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;morphia&amp;lt;/artifactId&amp;gt;&#xA;            &amp;lt;version&amp;gt;1.3.2&amp;lt;/version&amp;gt;&#xA;        &amp;lt;/dependency&amp;gt;&#xA;        &amp;lt;dependency&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;com.google.code.gson&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;gson&amp;lt;/artifactId&amp;gt;&#xA;            &amp;lt;version&amp;gt;2.3.1&amp;lt;/version&amp;gt;&#xA;        &amp;lt;/dependency&amp;gt;&#xA;    &amp;lt;/dependencies&amp;gt;&#xA;&amp;lt;/project&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="7804640" LastActivityDate="2018-03-02T22:50:44.523" Title="NoClassDefFoundError for MongoClient in cloud9" Tags="&lt;java&gt;&lt;maven&gt;&lt;apache-spark&gt;&lt;cloud9&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" />
  <row Id="49075551" PostTypeId="1" CreationDate="2018-03-02T18:51:36.713" Score="0" ViewCount="24" Body="&lt;p&gt;I have the following error &lt;strong&gt;is not a member of Unit&lt;/strong&gt; with the following code:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I create two &lt;code&gt;DataFrame&lt;/code&gt; of a &lt;code&gt;jdbc DataSource&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val df_emp = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:oracle:thin:XXXXXX/XXXXXX@XXXXXX:1521:XXXXXX&quot;).option(&quot;driver&quot;, &quot;oracle.jdbc.driver.OracleDriver&quot;).option(&quot;dbtable&quot;, &quot;hr.employees&quot;).load()&#xA;val df_dep = spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, &quot;jdbc:oracle:thin:XXXXXX/XXXXXX@XXXXXX:1521:XXXXXX&quot;).option(&quot;driver&quot;, &quot;oracle.jdbc.driver.OracleDriver&quot;).option(&quot;dbtable&quot;, &quot;hr.departments&quot;).load()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I then create two views of the created DataFrame:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df_emp.createOrReplaceTempView(&quot;employees&quot;)        &#xA;df_dep.createOrReplaceTempView(&quot;departments&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With &lt;code&gt;spark.sql&lt;/code&gt; I execute a join of the two created views in a val &lt;code&gt;sql_join_empdep&lt;/code&gt; and show the results:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val sql_join_empdep = spark.sql(&quot;SELECT emp.employee_id, emp.first_name, emp.last_name, dep.department_name, dep.department_id from employees emp inner join departments dep on emp.department_id = dep.department_id&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;// But it throws the following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sql_join_empdep.show()&#xA;  &amp;lt;console&amp;gt;:30: error: value show is not a member of Unit&#xA;    sql_join_empdep.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;What is Unit?&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;When i use &lt;code&gt;spark.sql&lt;/code&gt; to execute a sql query to create &lt;code&gt;sql_join_empdep&lt;/code&gt;, &lt;strong&gt;what is the type of this variable (sql_join_empdep)?&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I think it's probably a conceptual error.&lt;/p&gt;&#xA;" OwnerUserId="9434987" LastEditorUserId="9297144" LastEditDate="2018-03-03T03:25:32.417" LastActivityDate="2018-03-03T03:25:32.417" Title="Sparl SQL - Value show is not a member of Unit" Tags="&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;spark-dataframe&gt;" AnswerCount="0" CommentCount="2" FavoriteCount="0" />
  <row Id="49075704" PostTypeId="1" AcceptedAnswerId="49075773" CreationDate="2018-03-02T19:01:44.577" Score="0" ViewCount="45" Body="&lt;p&gt;I'm trying to run the following command:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df = df.withColumn(&quot;DATATmp&quot;, to_date($&quot;DATA&quot;, &quot;yyyyMMdd&quot;))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And getting this error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;console&amp;gt;:34: error: too many arguments for method to_date: (e: org.apache.spark.sql.Column)org.apache.spark.sql.Column&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How could I specify the exactly function to import? Has another way to avoid this error?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: Spark version 2.1&lt;/p&gt;&#xA;" OwnerUserId="7143310" LastEditorUserId="7143310" LastEditDate="2018-03-03T18:12:09.927" LastActivityDate="2018-03-03T18:12:09.927" Title="Scala/Spark can't match function" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="49076052" PostTypeId="1" CreationDate="2018-03-02T19:27:02.297" Score="0" ViewCount="16" Body="&lt;p&gt;I'd like to use Spark &lt;code&gt;DataFrameWriter.partitionBy()&lt;/code&gt; to write to AWS S3. It, of course, writes a separate directory branch for each unique combination of partition column values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any way to get from Spark which partition column value combinations existed in the DataFrame, i.e. were written? Without querying the &quot;filesystem&quot; (AWS S3 object store).&lt;/p&gt;&#xA;" OwnerUserId="45051" LastActivityDate="2018-03-02T20:23:55.247" Title="How to Get Set of Partition Column(s) Values with Spark DataFrameWriter.partitionBy" Tags="&lt;apache-spark&gt;&lt;partitioning&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49076224" PostTypeId="1" CreationDate="2018-03-02T19:40:10.020" Score="0" ViewCount="11" Body="&lt;p&gt;Currently, I am able to see a bunch of metrics divided in this structure:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;(1,2,3,4,5)&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;CodeGenerator  &lt;/li&gt;&#xA;&lt;li&gt;HiveExternalCatalog  &lt;/li&gt;&#xA;&lt;li&gt;executor  &lt;/li&gt;&#xA;&lt;li&gt;jvm  &lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Driver  &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;BlockManager  &lt;/li&gt;&#xA;&lt;li&gt;CodeGenerator  &lt;/li&gt;&#xA;&lt;li&gt;DAGScheduler  &lt;/li&gt;&#xA;&lt;li&gt;ExecutorAllocationManager   &lt;/li&gt;&#xA;&lt;li&gt;HiveExternalCatalog  &lt;/li&gt;&#xA;&lt;li&gt;jvm&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I need to find out which metrics do I need to consider in order to get the total time elapsed, space usage and number of vcores being used for that spark job. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Once I have the right metrics, I need to compose the right query (for instance for disk usage, would involve aggregating on all executors) on Grafana for viewing these metrics in an insightful way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So far I have not come across much resources online which give a more detailed view on this specific problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help would be appreciated.&lt;/p&gt;&#xA;" OwnerUserId="3548509" LastEditorUserId="7128479" LastEditDate="2018-03-03T17:42:15.543" LastActivityDate="2018-03-03T17:42:15.543" Title="How to visualize time-elapsed and total space usage metrics on Grafana for a spark job using a metrics sink for Graphite" Tags="&lt;apache-spark&gt;&lt;grafana&gt;&lt;graphite&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49076794" PostTypeId="1" CreationDate="2018-03-02T20:24:05.113" Score="0" ViewCount="73" Body="&lt;p&gt;I am trying to convert about 1.5 GB of GZIPPED CSV into Parquet using AWS Glue. The script below is an autogenerated Glue job to accomplish that task. It seems to take a very long time (I've waited hours for 10 DPUs and never seen it end or produce any output data)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm wondering if anyone has any experience converting 1.5 GB + GZIPPED CSV into Parquet - is there a better way to accomplish this conversion? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have TB's of data to convert. It is concerning that it seems to take so long to convert GBs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My Glue Job Logs have thousands of entries like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;18/03/02 20:20:20 DEBUG Client: &#xA;client token: N/A&#xA;diagnostics: N/A&#xA;ApplicationMaster host: 172.31.58.225&#xA;ApplicationMaster RPC port: 0&#xA;queue: default&#xA;start time: 1520020335454&#xA;final status: UNDEFINED&#xA;tracking URL: http://ip-172-31-51-199.ec2.internal:20888/proxy/application_1520020149832_0001/&#xA;user: root&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;AWS Autogenerated Glue Job Code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import sys&#xA;from awsglue.transforms import *&#xA;from awsglue.utils import getResolvedOptions&#xA;from pyspark.context import SparkContext&#xA;from awsglue.context import GlueContext&#xA;from awsglue.job import Job&#xA;&#xA;## @params: [JOB_NAME]&#xA;args = getResolvedOptions(sys.argv, ['JOB_NAME'])&#xA;&#xA;sc = SparkContext()&#xA;glueContext = GlueContext(sc)&#xA;spark = glueContext.spark_session&#xA;job = Job(glueContext)&#xA;job.init(args['JOB_NAME'], args)&#xA;## @type: DataSource&#xA;## @args: [database = &quot;test_datalake_db&quot;, table_name = &quot;events2_2017_test&quot;, transformation_ctx = &quot;datasource0&quot;]&#xA;## @return: datasource0&#xA;## @inputs: []&#xA;datasource0 = glueContext.create_dynamic_frame.from_catalog(database = &quot;test_datalake_db&quot;, table_name = &quot;events2_2017_test&quot;, transformation_ctx = &quot;datasource0&quot;)&#xA;## @type: ApplyMapping&#xA;## @args: [mapping = [(&quot;sys_vortex_id&quot;, &quot;string&quot;, &quot;sys_vortex_id&quot;, &quot;string&quot;), (&quot;sys_app_id&quot;, &quot;string&quot;, &quot;sys_app_id&quot;, &quot;string&quot;), (&quot;sys_pq_id&quot;, &quot;string&quot;, &quot;sys_pq_id&quot;, &quot;string&quot;), (&quot;sys_ip_address&quot;, &quot;string&quot;, &quot;sys_ip_address&quot;, &quot;string&quot;), (&quot;sys_submitted_at&quot;, &quot;string&quot;, &quot;sys_submitted_at&quot;, &quot;string&quot;), (&quot;sys_received_at&quot;, &quot;string&quot;, &quot;sys_received_at&quot;, &quot;string&quot;), (&quot;device_id_type&quot;, &quot;string&quot;, &quot;device_id_type&quot;, &quot;string&quot;), (&quot;device_id&quot;, &quot;string&quot;, &quot;device_id&quot;, &quot;string&quot;), (&quot;timezone&quot;, &quot;string&quot;, &quot;timezone&quot;, &quot;string&quot;), (&quot;online&quot;, &quot;string&quot;, &quot;online&quot;, &quot;string&quot;), (&quot;app_version&quot;, &quot;string&quot;, &quot;app_version&quot;, &quot;string&quot;), (&quot;device_days&quot;, &quot;string&quot;, &quot;device_days&quot;, &quot;string&quot;), (&quot;device_sessions&quot;, &quot;string&quot;, &quot;device_sessions&quot;, &quot;string&quot;), (&quot;event_id&quot;, &quot;string&quot;, &quot;event_id&quot;, &quot;string&quot;), (&quot;event_at&quot;, &quot;string&quot;, &quot;event_at&quot;, &quot;string&quot;), (&quot;event_date&quot;, &quot;string&quot;, &quot;event_date&quot;, &quot;string&quot;), (&quot;int1&quot;, &quot;string&quot;, &quot;int1&quot;, &quot;string&quot;)], transformation_ctx = &quot;applymapping1&quot;]&#xA;## @return: applymapping1&#xA;## @inputs: [frame = datasource0]&#xA;applymapping1 = ApplyMapping.apply(frame = datasource0, mappings = [(&quot;sys_vortex_id&quot;, &quot;string&quot;, &quot;sys_vortex_id&quot;, &quot;string&quot;), (&quot;sys_app_id&quot;, &quot;string&quot;, &quot;sys_app_id&quot;, &quot;string&quot;), (&quot;sys_pq_id&quot;, &quot;string&quot;, &quot;sys_pq_id&quot;, &quot;string&quot;), (&quot;sys_ip_address&quot;, &quot;string&quot;, &quot;sys_ip_address&quot;, &quot;string&quot;), (&quot;sys_submitted_at&quot;, &quot;string&quot;, &quot;sys_submitted_at&quot;, &quot;string&quot;), (&quot;sys_received_at&quot;, &quot;string&quot;, &quot;sys_received_at&quot;, &quot;string&quot;), (&quot;device_id_type&quot;, &quot;string&quot;, &quot;device_id_type&quot;, &quot;string&quot;), (&quot;device_id&quot;, &quot;string&quot;, &quot;device_id&quot;, &quot;string&quot;), (&quot;timezone&quot;, &quot;string&quot;, &quot;timezone&quot;, &quot;string&quot;), (&quot;online&quot;, &quot;string&quot;, &quot;online&quot;, &quot;string&quot;), (&quot;app_version&quot;, &quot;string&quot;, &quot;app_version&quot;, &quot;string&quot;), (&quot;device_days&quot;, &quot;string&quot;, &quot;device_days&quot;, &quot;string&quot;), (&quot;device_sessions&quot;, &quot;string&quot;, &quot;device_sessions&quot;, &quot;string&quot;), (&quot;event_id&quot;, &quot;string&quot;, &quot;event_id&quot;, &quot;string&quot;), (&quot;event_at&quot;, &quot;string&quot;, &quot;event_at&quot;, &quot;string&quot;), (&quot;event_date&quot;, &quot;string&quot;, &quot;event_date&quot;, &quot;string&quot;), (&quot;int1&quot;, &quot;string&quot;, &quot;int1&quot;, &quot;string&quot;)], transformation_ctx = &quot;applymapping1&quot;)&#xA;## @type: ResolveChoice&#xA;## @args: [choice = &quot;make_struct&quot;, transformation_ctx = &quot;resolvechoice2&quot;]&#xA;## @return: resolvechoice2&#xA;## @inputs: [frame = applymapping1]&#xA;resolvechoice2 = ResolveChoice.apply(frame = applymapping1, choice = &quot;make_struct&quot;, transformation_ctx = &quot;resolvechoice2&quot;)&#xA;## @type: DropNullFields&#xA;## @args: [transformation_ctx = &quot;dropnullfields3&quot;]&#xA;## @return: dropnullfields3&#xA;## @inputs: [frame = resolvechoice2]&#xA;dropnullfields3 = DropNullFields.apply(frame = resolvechoice2, transformation_ctx = &quot;dropnullfields3&quot;)&#xA;## @type: DataSink&#xA;## @args: [connection_type = &quot;s3&quot;, connection_options = {&quot;path&quot;: &quot;s3://devops-redshift*****/prd/parquet&quot;}, format = &quot;parquet&quot;, transformation_ctx = &quot;datasink4&quot;]&#xA;## @return: datasink4&#xA;## @inputs: [frame = dropnullfields3]&#xA;datasink4 = glueContext.write_dynamic_frame.from_options(frame = dropnullfields3, connection_type = &quot;s3&quot;, connection_options = {&quot;path&quot;: &quot;s3://devops-redshift*****/prd/parquet&quot;}, format = &quot;parquet&quot;, transformation_ctx = &quot;datasink4&quot;)&#xA;job.commit()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="6138467" LastEditorUserId="6138467" LastEditDate="2018-03-02T20:42:19.223" LastActivityDate="2018-03-02T20:42:19.223" Title="AWS Glue Job - Convert CSV to Parquet" Tags="&lt;amazon-web-services&gt;&lt;apache-spark&gt;&lt;parquet&gt;&lt;aws-glue&gt;" AnswerCount="0" CommentCount="4" />
  <row Id="49077170" PostTypeId="1" CreationDate="2018-03-02T20:49:08.553" Score="3" ViewCount="38" Body="&lt;p&gt;I need to calculate the distance between two strings in R using sparklyr. Is there a way of using stringdist or any other package? I wanted to use cousine distance. This distance is used as a method of stringdist function.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#xA;" OwnerUserId="7856593" LastEditorUserId="7856593" LastEditDate="2018-03-02T20:59:02.740" LastActivityDate="2018-03-08T20:35:57.517" Title="How to calculate distance between strings using sparklyr?" Tags="&lt;r&gt;&lt;sparklyr&gt;&lt;stringdist&gt;" AnswerCount="1" CommentCount="3" FavoriteCount="1" />
  <row Id="49077265" PostTypeId="1" AcceptedAnswerId="49135550" CreationDate="2018-03-02T20:56:21.203" Score="1" ViewCount="33" Body="&lt;p&gt;I'm struggling to figure out an elegant solution to join a single dataframe to a separate sequence of 1 to N related dataframes. Initial attempt:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  val sources = program.attributes.map(attr =&amp;gt; {&#xA;    spark.read&#xA;      .option(&quot;header&quot;, value = true)&#xA;      .schema(program.GetSchema(attr))&#xA;      .csv(s&quot;${program.programRawHdfsDirectory}/${attr.sourceFile}&quot;)&#xA;  })&#xA;  val rawDf: DataFrame = sources.reduce((df1, df2) =&amp;gt; df1.join(df2, program.dimensionFields, &quot;full&quot;))&#xA;&#xA;  // Full of fail:&#xA;  val fullDf: DataFrame = program.dimensions.filter(d =&amp;gt; d.hierarchy != &quot;RAW&quot;).reduceLeft((d1, _) =&amp;gt; {&#xA;    val hierarchy = spark.read.parquet(d1.hierarchyLocation).where(d1.hierarchyFilter)&#xA;    rawDf.join(hierarchy, d1.hierarchyJoin)&#xA;  })&#xA;&#xA;  fullDf.selectExpr(program.outputFields:_*).write.parquet(program.programEtlHdfsDirectory)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The reduceLeft idea doesn't work because I'm iterating through a collection of configuration objects (the dimensions property), but what I want returned from each iteration is a dataframe. The error is a type mismatch, which is not surprising.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The core of the problem is that I have 1 to N &quot;dimension&quot; objects that define how to load an existing hierarchy table and also how to join that table to my &quot;raw&quot; dataframe I created earlier.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any idea how I might create these joins without some sort of horrible hack?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wonder is this might work? I have a common field name in each hierarchy dataframe that I'm joining to. If I renamed this common field to match the corresponding column in my &quot;raw&quot; dataframe, could I execute the joins in a fold without explicitly calling out the columns? Will Spark just default to the matching names?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  val rawDf = sources.reduce((df1, df2) =&amp;gt; df1.join(df2, program.dimensionFields, &quot;full&quot;))&#xA;&#xA;  val hierarchies = program.dimensions.map(dim =&amp;gt; {&#xA;    spark.read.parquet(dim.hierarchyLocation).where(dim.hierarchyFilter).withColumnRenamed(&quot;parent_hier_cd&quot;, dim.columnName)&#xA;  })&#xA;  val fullDf = hierarchies.foldLeft(rawDf) { (df1, df2) =&amp;gt; df1.join(df2) }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE 2&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;No, that does not work. Spark attempts a cross join.&lt;/p&gt;&#xA;" OwnerUserId="71551" LastEditorUserId="71551" LastEditDate="2018-03-06T16:21:57.023" LastActivityDate="2018-03-06T16:23:54.940" Title="Spark Join Single Dataframe to a Collection of Dataframes" Tags="&lt;scala&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="5" />
  <row Id="49078434" PostTypeId="1" CreationDate="2018-03-02T22:35:28.563" Score="0" ViewCount="25" Body="&lt;p&gt;can I use spark to do a regular python job in parallel? for example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def my_f(mylist1, mylist2):&#xA; #sophisticated stuff&#xA; return map(lambda x: x + 1, mylist1)&#xA;&#xA;&#xA;mylist3 = my_f(xrange(10), xrange(5))&#xA;#pass my list to a spark rdd for further processing&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The reason im asking is because in my pipeline I need to use a sklearn model on 2 arrays and I want to take those output for further manipulation in spark&lt;/p&gt;&#xA;" OwnerUserId="1871528" LastActivityDate="2018-03-02T22:35:28.563" Title="does spark have the ability to parallise regular python function" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49078449" PostTypeId="1" CreationDate="2018-03-02T22:37:06.830" Score="-2" ViewCount="17" Body="&lt;p&gt;I was working on DataFrames in scala/spark. I have two features in my dataset e.g( name,description).&#xA;I am able to tokenize and apply CountVectorModel on each of them separately but not able to combine them in single dataset.&lt;/p&gt;&#xA;" OwnerUserId="5687283" LastActivityDate="2018-03-02T22:37:06.830" Title="Merge two features in CountVectorizer in Scala/spark" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;spark-dataframe&gt;" AnswerCount="0" CommentCount="1" ClosedDate="2018-03-03T12:35:54.603" />
  <row Id="49078798" PostTypeId="1" CreationDate="2018-03-02T23:12:53.220" Score="0" ViewCount="27" Body="&lt;p&gt;I am running Spark-Shell with Scala and I want to set an environment variable to load data into Google bigQuery. The environment variable is &lt;code&gt;GOOGLE_APPLICATION_CREDENTIALS&lt;/code&gt; and it contains &lt;code&gt;/path/to/service/account.json&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In python environment I can easily do,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import os &#xA;os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = &quot;path/to/service/account.json&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, I cannot do this in Scala. I can print out the system environment variables using,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;scala&amp;gt; sys.env&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;scala&amp;gt; System.getenv()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;which returns me a map of String Key,Value pairs. However,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;scala&amp;gt; System.getenv(&quot;GOOGLE_APPLICATION_CREDENTIALS&quot;) = &quot;path/to/service/account.json&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;returns an error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;console&amp;gt;:26: error: value update is not a member of java.util.Map[String,String]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="8229534" LastActivityDate="2018-03-03T05:33:42.457" Title="Setting GOOGLE_APPLICATION_CREDENTIALS environment variable in Scala" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49079018" PostTypeId="1" CreationDate="2018-03-02T23:39:50.173" Score="0" ViewCount="29" Body="&lt;p&gt;I am developing a Spark-Kafka Streaming program where i need to capture the kafka partition offsets, inorder to handle failure scenarios.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Most of the devs are using Hbase as a storage for offsets, but how would it be if i use a file on hdfs or local disk to store offsets which is simple and easy?&#xA;I am trying to avoid using a Nosql for storing offsets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can i know what are the advantages and disadvantages of using a file over hbase for storing offsets?&lt;/p&gt;&#xA;" OwnerUserId="5539819" LastEditorUserId="535024" LastEditDate="2018-03-03T10:45:03.513" LastActivityDate="2018-03-03T16:14:53.680" Title="Storing Kafka Offsets in a File vs Hbase" Tags="&lt;apache-spark&gt;&lt;apache-kafka&gt;&lt;spark-streaming&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="49079087" PostTypeId="1" CreationDate="2018-03-02T23:48:45.487" Score="0" ViewCount="20" Body="&lt;p&gt;pyspark newbie. Here is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def sparkApp():&#xA;    spark = SparkSession \&#xA;        .builder \&#xA;        .appName(&quot;Python Spark SQL basic example&quot;) \&#xA;        .config(&quot;spark.sql.catalogImplementation&quot;, &quot;hive&quot;) \&#xA;        .config(&quot;spark.executor.memory&quot;, &quot;4g&quot;) \&#xA;        .config(&quot;spark.driver.memory&quot;, &quot;16g&quot;) \&#xA;        .config(&quot;spark.executor.instances&quot;, &quot;5&quot;) \&#xA;        .config(&quot;spark.executor.cores&quot;, &quot;5&quot;) \&#xA;        .getOrCreate()&#xA;    return spark&#xA;&#xA;def my_f(x, w):&#xA; return np.array(x).dot(w).sum()&#xA;&#xA;w = [1,2]&#xA;x = sparkApp().(&quot;select x1, x2 from my_table&quot;)&#xA;x.rdd.map(lambda row: my_f(row, w)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My questions are:&lt;br/&gt;&#xA; 1. I know this parallelizes the reading of x but will it parallelize the multiplication of x and w? if so, will it return the values in the same index as the input? if not, how can I run it parallel it?&lt;br/&gt;&#xA; 2. Do I broadcast w or pass it as a parameter?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank You&lt;/p&gt;&#xA;" OwnerUserId="1871528" LastEditorUserId="5880706" LastEditDate="2018-03-03T02:57:06.400" LastActivityDate="2018-03-03T02:57:06.400" Title="is this the right way to do multiplication on pyspark?" Tags="&lt;python-2.7&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;apache-spark-mllib&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49079133" PostTypeId="1" CreationDate="2018-03-02T23:53:17.623" Score="1" ViewCount="23" Body="&lt;h2&gt;Question&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;I am trying to run a Python DEAP genetic algorithm in a Jupyter notebook with pySpark for parallelization. With some research I understand that the essence is to register a spark based map function with toolbox to allow spark to do the fitness evaluations (code below). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is, how do I expose creator functions for the SparkContext?&#xA;Does Spark require any special treatment when compared to SCOOP and multiprocessing?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h2&gt;Code&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The essence of the code I am running is as follows with SparContext (sc) already created:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import deap as ea&#xA;from deap import creator, base, tools, algorithms&#xA;&#xA;ea.creator.create(&quot;FitnessMin&quot;, ea.base.Fitness, weights=(-1.0,))&#xA;ea.creator.create(&quot;Individual&quot;, list, fitness=ea.creator.FitnessMin)&#xA;&#xA;toolbox = ea.base.Toolbox()&#xA;def sparkMap(algorithm, *population):&#xA;    return sc.parallelize(population).map(algorithm)&#xA;&#xA;toolbox.register(&quot;map&quot;, sparkMap) #Set DEAP to run on a machine cluster using Spark&#xA;hallOfFame = tools.HallOfFame(2)&#xA;population = toolbox.population(n=POPULATION_SIZE)&#xA;tools.initIterate(list, partial(sample, range(MAX_NUMBER_OF_CLUSTERS), MAX_NUMBER_OF_CLUSTERS))&#xA;&#xA;gen = 0;&#xA;while gen &amp;lt; NUMBER_OF_GENERATOINS:&#xA;    # Update population&#xA;    population = toolbox.select(population, k=len(population))&#xA;    population = [toolbox.clone(ind) for ind in population]&#xA;    population = ea.algorithms.varAnd(population, toolbox, cxpb=cxpb, mutpb=mutpb, )&#xA;&#xA;    offspring = [individual for individual in population if not individual.fitness.valid]&#xA;    fits = toolbox.map(toolbox.evaluate, offspring).collect()&#xA;&#xA;    for fit, ind in zip(fits, offspring):&#xA;        ind.fitness.values = fit&#xA;&#xA;    #Update hall of fame to ensure we always know the best found solution&#xA;    hallOfFame.update(offspring)    &#xA;&#xA;    gen += 1&#xA;&#xA;best = hallOfFame[0]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;However, this results in an error stating that:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;AttributeError: Can't get attribute 'Individual' on module 'deap.creator' from '/gpfs/fs01/user/s093-7b1ca9741d3405-545a66b5b986/.local/lib/python3.5/site-packages/deap/creator.py'&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;My understanding is that with other parallelization setups like SCOOP and Python paralell the deap.creator method must be part of the global scope. Since I am working in a JUpyter notebook this should be the case with the code below. &#xA;Also &quot;%who&quot; shows that among many others, these are listed in the global scope:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;creator ea sparkMap toolbox&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h2&gt;Error message&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;Complete error message:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;--------------------------------------------------------------------------- Py4JJavaError                             Traceback (most recent call last) &amp;lt;ipython-input-45-49c0cc9cf0b4&amp;gt; in &amp;lt;module&amp;gt;()&#xA;     28 &#xA;     29     offspring = [individual for individual in population if not individual.fitness.valid]&#xA;---&amp;gt; 30     fits = toolbox.map(toolbox.evaluate, offspring).collect()&#xA;     31     #print('---------------------------------------------------------------------')&#xA;     32     #print('fits',fits)&#xA;&#xA;/usr/local/src/spark21master/spark/python/pyspark/rdd.py in collect(self)&#xA;    806         &quot;&quot;&quot;&#xA;    807         with SCCallSiteSync(self.context) as css:&#xA;--&amp;gt; 808             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())&#xA;    809         return list(_load_from_socket(port, self._jrdd_deserializer))&#xA;    810 &#xA;&#xA;/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args)    1131         answer = self.gateway_client.send_command(command)    1132         return_value&#xA;= get_return_value(&#xA;-&amp;gt; 1133             answer, self.gateway_client, self.target_id, self.name)    1134     1135         for temp_arg in temp_args:&#xA;&#xA;/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py in deco(*a, **kw)&#xA;     61     def deco(*a, **kw):&#xA;     62         try:&#xA;---&amp;gt; 63             return f(*a, **kw)&#xA;     64         except py4j.protocol.Py4JJavaError as e:&#xA;     65             s = e.java_exception.toString()&#xA;&#xA;/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)&#xA;    317                 raise Py4JJavaError(&#xA;    318                     &quot;An error occurred while calling {0}{1}{2}.\n&quot;.&#xA;--&amp;gt; 319                     format(target_id, &quot;.&quot;, name), value)&#xA;    320             else:&#xA;    321                 raise Py4JError(&#xA;&#xA;Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 10 times, most recent failure: Lost task&#xA;1.9 in stage 1.0 (TID 21, yp-spark-dal09-env5-0024, executor 6679c417-036c-45c6-9b7e-92e96c9751eb): org.apache.spark.api.python.PythonException: Traceback (most recent call last):   File &quot;/usr/local/src/spark21master/spark-2.1.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/worker.py&quot;, line 171, in main&#xA;    process()   File &quot;/usr/local/src/spark21master/spark-2.1.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/worker.py&quot;, line 166, in process&#xA;    serializer.dump_stream(func(split_index, iterator), outfile)   File &quot;/usr/local/src/spark21master/spark-2.1.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/serializers.py&quot;, line 268, in dump_stream&#xA;    vs = list(itertools.islice(iterator, batch))   File &quot;/usr/local/src/spark21master/spark-2.1.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/serializers.py&quot;, line 144, in load_stream&#xA;    yield self._read_with_length(stream)   File &quot;/usr/local/src/spark21master/spark-2.1.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/serializers.py&quot;, line 169, in _read_with_length&#xA;    return self.loads(obj)   File &quot;/usr/local/src/spark21master/spark-2.1.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/serializers.py&quot;, line 455, in loads&#xA;    return pickle.loads(obj, encoding=encoding) AttributeError: Can't get attribute 'Individual' on &amp;lt;module 'deap.creator' from '/gpfs/fs01/user/s093-7b1ca9741d3405-545a66b5b986/.local/lib/python3.5/site-packages/deap/creator.py'&amp;gt;&#xA;&#xA;    at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)   at org.apache.spark.api.python.PythonRunner$$anon$1.&amp;lt;init&amp;gt;(PythonRDD.scala:234)     at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)    at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:326)  at org.apache.spark.rdd.RDD.iterator(RDD.scala:290)     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)   at org.apache.spark.scheduler.Task.run(Task.scala:99)   at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:326)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)  at java.lang.Thread.run(Thread.java:785)&#xA;&#xA;Driver stacktrace:  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1442)    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1430)     at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1429)     at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)   at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)   at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1429)  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)     at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)     at scala.Option.foreach(Option.scala:257)   at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1657)     at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1612)   at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1601)   at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)  at java.lang.Thread.getStackTrace(Thread.java:1117)     at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)   at org.apache.spark.SparkContext.runJob(SparkContext.scala:1941)    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1954)    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1967)    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1981)    at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:956)     at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)   at org.apache.spark.rdd.RDD.withScope(RDD.scala:381)    at org.apache.spark.rdd.RDD.collect(RDD.scala:955)  at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)  at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)    at java.lang.reflect.Method.invoke(Method.java:507)     at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)     at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)   at py4j.Gateway.invoke(Gateway.java:280)    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)     at py4j.commands.CallCommand.execute(CallCommand.java:79)   at py4j.GatewayConnection.run(GatewayConnection.java:214)   at java.lang.Thread.run(Thread.java:785) Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):   File &quot;/usr/local/src/spark21master/spark-2.1.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/worker.py&quot;, line 171, in main&#xA;    process()   File &quot;/usr/local/src/spark21master/spark-2.1.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/worker.py&quot;, line 166, in process&#xA;    serializer.dump_stream(func(split_index, iterator), outfile)   File &quot;/usr/local/src/spark21master/spark-2.1.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/serializers.py&quot;, line 268, in dump_stream&#xA;    vs = list(itertools.islice(iterator, batch))   File &quot;/usr/local/src/spark21master/spark-2.1.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/serializers.py&quot;, line 144, in load_stream&#xA;    yield self._read_with_length(stream)   File &quot;/usr/local/src/spark21master/spark-2.1.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/serializers.py&quot;, line 169, in _read_with_length&#xA;    return self.loads(obj)   File &quot;/usr/local/src/spark21master/spark-2.1.2-bin-2.7.3/python/lib/pyspark.zip/pyspark/serializers.py&quot;, line 455, in loads&#xA;    return pickle.loads(obj, encoding=encoding) AttributeError: Can't get attribute 'Individual' on &amp;lt;module 'deap.creator' from '/gpfs/fs01/user/s093-7b1ca9741d3405-545a66b5b986/.local/lib/python3.5/site-packages/deap/creator.py'&amp;gt;&#xA;&#xA;    at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)   at org.apache.spark.api.python.PythonRunner$$anon$1.&amp;lt;init&amp;gt;(PythonRDD.scala:234)     at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)    at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:326)  at org.apache.spark.rdd.RDD.iterator(RDD.scala:290)     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)   at org.apache.spark.scheduler.Task.run(Task.scala:99)   at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:326)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)  ... 1 more&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="9436419" LastActivityDate="2018-03-02T23:53:17.623" Title="Python DEAP running on pySpark is unable to call creator function" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;deap&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49079222" PostTypeId="1" AcceptedAnswerId="49080206" CreationDate="2018-03-03T00:04:10.420" Score="1" ViewCount="37" Body="&lt;p&gt;Hie everyone, It look like the class StreamingContext is not found in the following code.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import org.apache.spark.streaming.{Seconds, StreamingContext}&#xA;import org.apache.spark.{SparkConf, SparkContext}&#xA;object Exemple {&#xA;  def main(args: Array[String]): Unit = {&#xA;    val conf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;Exemple&quot;)&#xA;    val sc = new SparkContext(conf)&#xA;    val ssc = new StreamingContext(sc, Seconds(2)) //this line throws error&#xA;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;here is the error: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/spark/streaming/StreamingContext&#xA;    at Exemple$.main(Exemple.scala:16)&#xA;    at Exemple.main(Exemple.scala)&#xA;Caused by: java.lang.ClassNotFoundException: org.apache.spark.streaming.StreamingContext&#xA;    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)&#xA;    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&#xA;    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)&#xA;    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&#xA;    ... 2 more&#xA;&#xA;Process finished with exit code 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I use the following build.sbt file: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;name := &quot;exemple&quot;&#xA;&#xA;version := &quot;1.0.0&quot;&#xA;&#xA;scalaVersion := &quot;2.11.11&quot;&#xA;&#xA;// https://mvnrepository.com/artifact/org.apache.spark/spark-sql&#xA;libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-sql&quot; % &quot;2.2.0&quot;&#xA;// https://mvnrepository.com/artifact/org.apache.spark/spark-streaming&#xA;libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-streaming&quot; % &quot;2.2.0&quot; % &quot;provided&quot;&#xA;// https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka-0-10&#xA;libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-streaming-kafka-0-10&quot; % &quot;2.2.0&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I run the Exemple class using intellij Run button and I get the error. in sbt shell it work fine. into my dependecies'module, I can find spark dependencies. The code compile in intellij. And I can see in the External Libraries spark dependies (inside the left project panel).&#xA;Do you have any idea. It seem not complicated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/90y3y.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/90y3y.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="6596760" LastEditorUserId="9231454" LastEditDate="2018-03-03T12:00:26.960" LastActivityDate="2018-03-03T12:00:26.960" Title="Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/spark/streaming/StreamingContext" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;intellij-idea&gt;&lt;sbt&gt;&lt;spark-streaming&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49079929" PostTypeId="1" AcceptedAnswerId="49080381" CreationDate="2018-03-03T02:05:20.917" Score="0" ViewCount="45" Body="&lt;p&gt;I have a large file(~5GB) which I have loaded into a dataframe. Now I have to get a value(fid) from each row and fetch the corresponding row in the same dataframe. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;var references = df.sqlContext.sql(&quot;Select authors,references,id from publications&quot;)&#xA;references.collect().foreach(ref =&amp;gt; ref.getSeq[String](1).foreach(id =&amp;gt; {&#xA;      val authors = ref.getSeq[String](0)&#xA;      val a = df.sqlContext.sql(s&quot;SELECT authors from publications  WHERE id='$id'&quot;)&#xA;&#xA;      }&#xA;))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have tried the above code I get an out of memory exception because of the collect action. I tried increasing the memory and all but still no success.&lt;/p&gt;&#xA;" OwnerUserId="2227185" LastActivityDate="2018-03-03T04:46:10.570" Title="Looping through a large dataframe and perform sql" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;rdd&gt;" AnswerCount="1" CommentCount="8" />
  <row Id="49080830" PostTypeId="1" CreationDate="2018-03-03T04:59:10.030" Score="0" ViewCount="22" Body="&lt;p&gt;I have following code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; test(&quot;&amp;lt;&amp;gt; on null values&quot;) {&#xA;    val spark = SparkSession.builder().enableHiveSupport().master(&quot;local&quot;).appName(&quot;&amp;lt;&amp;gt; on null values&quot;).getOrCreate()&#xA;    import spark.implicits._&#xA;    val data = Seq((&quot;A&quot;, &quot;a&quot;, &quot;a&quot;), (&quot;B&quot;, null.asInstanceOf[String], &quot;b&quot;), (&quot;C&quot;, &quot;c&quot;, null.asInstanceOf[String]), (&quot;D&quot;, null.asInstanceOf[String], null.asInstanceOf[String])).toDF(&quot;x&quot;, &quot;y&quot;,&quot;z&quot;)&#xA;&#xA;    /*&#xA;          +---+----+----+&#xA;      |x  |y   |z   |&#xA;      +---+----+----+&#xA;      |A  |a   |a   |&#xA;      |B  |null|b   |&#xA;      |C  |c   |null|&#xA;      |D  |null|null|&#xA;      +---+----+----+&#xA;     */&#xA;    data.show(truncate = false)&#xA;&#xA;&#xA;&#xA;    data.createOrReplaceTempView(&quot;t&quot;)&#xA;    spark.sql(&#xA;      &quot;&quot;&quot;&#xA;      select x from t where y &amp;lt;&amp;gt; z&#xA;      &quot;&quot;&quot;.stripMargin(' ')).show(truncate = false)&#xA;&#xA;    spark.stop()&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am to query the rows that &lt;code&gt;y&lt;/code&gt; is not equal to &lt;code&gt;z&lt;/code&gt; with &lt;code&gt;select x from t where y &amp;lt;&amp;gt; z&lt;/code&gt;,but there is no rows queried out,  it looks that &amp;lt;&gt; doesn't work on null values,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does it work as expected or there is something I have missed.&lt;/p&gt;&#xA;" OwnerUserId="4443784" LastEditorUserId="2308683" LastEditDate="2018-03-03T05:01:45.827" LastActivityDate="2018-03-03T05:01:45.827" Title="Doesn't &lt;&gt; work on null value" Tags="&lt;sql&gt;&lt;apache-spark&gt;&lt;apache-spark-sql&gt;" AnswerCount="0" CommentCount="0" ClosedDate="2018-03-03T05:31:46.287" />
  <row Id="49081079" PostTypeId="1" CreationDate="2018-03-03T05:40:16.143" Score="2" ViewCount="75" Body="&lt;p&gt;I have a Spark's Structured Streaming application with checkpointing to write output in parquet and using the default spark.sql.shuffle.partitions = 200. I need to change the shuffle partitions but the new value is not used. Here is the content of a checkpoint offset after the application is restarted:  &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{&quot;batchWatermarkMs&quot;:1520054221000,&quot;batchTimestampMs&quot;:1520054720003,&quot;conf&quot;:{&quot;spark.sql.shuffle.partitions&quot;:&quot;200&quot;}}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Do I need to set the number of partitions in the code instead of setting it with --conf?&lt;/p&gt;&#xA;" OwnerUserId="1373186" LastEditorUserId="5880706" LastEditDate="2018-03-03T05:55:07.993" LastActivityDate="2018-03-03T09:05:16.097" Title="new spark.sql.shuffle.partitions value not used after checkpointing" Tags="&lt;apache-spark&gt;&lt;spark-streaming&gt;&lt;structured-streaming&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49081180" PostTypeId="1" CreationDate="2018-03-03T05:56:27.310" Score="0" ViewCount="32" Body="&lt;p&gt;Hi I tried to install pyspark in a EC2 instance (standard Amazon linux image). I installed anaconda python 3.6 and used &quot;pip install pyspark&quot; to install spark. It worked just fine. But when I try to enter pyspark with command &quot;pyspark&quot;, I got the following error message. What could have gone wrong? Thanks!&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19)&#xA;[GCC 7.2.0] on linux&#xA;Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.&#xA;Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties&#xA;Setting default log level to &quot;WARN&quot;.&#xA;To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).&#xA;18/03/03 05:47:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&#xA;18/03/03 05:47:13 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:&#xA;org.apache.spark.api.java.JavaSparkContext.&amp;lt;init&amp;gt;(JavaSparkContext.scala:58)&#xA;sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&#xA;sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#xA;sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#xA;java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#xA;py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)&#xA;py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#xA;py4j.Gateway.invoke(Gateway.java:236)&#xA;py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)&#xA;py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)&#xA;py4j.GatewayConnection.run(GatewayConnection.java:214)&#xA;java.lang.Thread.run(Thread.java:748)&#xA;Traceback (most recent call last):&#xA;  File &quot;/home/ec2-user/anaconda3/lib/python3.6/site-packages/pyspark/python/pyspark/shell.py&quot;, line 45, in &amp;lt;module&amp;gt;&#xA;    spark = SparkSession.builder\&#xA;  File &quot;/home/ec2-user/anaconda3/lib/python3.6/site-packages/pyspark/sql/session.py&quot;, line 169, in getOrCreate&#xA;    sc = SparkContext.getOrCreate(sparkConf)&#xA;  File &quot;/home/ec2-user/anaconda3/lib/python3.6/site-packages/pyspark/context.py&quot;, line 334, in getOrCreate&#xA;    SparkContext(conf=conf or SparkConf())&#xA;  File &quot;/home/ec2-user/anaconda3/lib/python3.6/site-packages/pyspark/context.py&quot;, line 118, in __init__&#xA;    conf, jsc, profiler_cls)&#xA;  File &quot;/home/ec2-user/anaconda3/lib/python3.6/site-packages/pyspark/context.py&quot;, line 180, in _do_init&#xA;    self._jsc = jsc or self._initialize_context(self._conf._jconf)&#xA;  File &quot;/home/ec2-user/anaconda3/lib/python3.6/site-packages/pyspark/context.py&quot;, line 273, in _initialize_context&#xA;    return self._jvm.JavaSparkContext(jconf)&#xA;  File &quot;/home/ec2-user/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py&quot;, line 1401, in __call__&#xA;  File &quot;/home/ec2-user/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py&quot;, line 319, in get_return_value&#xA;py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.&#xA;: java.lang.ExceptionInInitializerError&#xA;        at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546)&#xA;        at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:373)&#xA;        at org.apache.spark.api.java.JavaSparkContext.&amp;lt;init&amp;gt;(JavaSparkContext.scala:58)&#xA;        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&#xA;        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#xA;        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#xA;        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#xA;        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)&#xA;        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#xA;        at py4j.Gateway.invoke(Gateway.java:236)&#xA;        at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)&#xA;        at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)&#xA;        at py4j.GatewayConnection.run(GatewayConnection.java:214)&#xA;        at java.lang.Thread.run(Thread.java:748)&#xA;Caused by: java.net.UnknownHostException: 10-236-108-194: 10-236-108-194: Temporary failure in name resolution&#xA;        at java.net.InetAddress.getLocalHost(InetAddress.java:1505)&#xA;        at org.apache.spark.util.Utils$.findLocalInetAddress(Utils.scala:891)&#xA;        at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress$lzycompute(Utils.scala:884)&#xA;        at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress(Utils.scala:884)&#xA;        at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941)&#xA;        at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941)&#xA;        at scala.Option.getOrElse(Option.scala:121)&#xA;        at org.apache.spark.util.Utils$.localHostName(Utils.scala:941)&#xA;        at org.apache.spark.internal.config.package$.&amp;lt;init&amp;gt;(package.scala:204)&#xA;        at org.apache.spark.internal.config.package$.&amp;lt;clinit&amp;gt;(package.scala)&#xA;        ... 14 more&#xA;Caused by: java.net.UnknownHostException: 10-236-108-194: Temporary failure in name resolution&#xA;        at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)&#xA;        at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928)&#xA;        at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323)&#xA;        at java.net.InetAddress.getLocalHost(InetAddress.java:1500)&#xA;        ... 23 more&#xA;&#xA;&#xA;During handling of the above exception, another exception occurred:&#xA;&#xA;Traceback (most recent call last):&#xA;  File &quot;/home/ec2-user/anaconda3/lib/python3.6/site-packages/pyspark/python/pyspark/shell.py&quot;, line 54, in &amp;lt;module&amp;gt;&#xA;    spark = SparkSession.builder.getOrCreate()&#xA;  File &quot;/home/ec2-user/anaconda3/lib/python3.6/site-packages/pyspark/sql/session.py&quot;, line 169, in getOrCreate&#xA;    sc = SparkContext.getOrCreate(sparkConf)&#xA;  File &quot;/home/ec2-user/anaconda3/lib/python3.6/site-packages/pyspark/context.py&quot;, line 334, in getOrCreate&#xA;    SparkContext(conf=conf or SparkConf())&#xA;  File &quot;/home/ec2-user/anaconda3/lib/python3.6/site-packages/pyspark/context.py&quot;, line 118, in __init__&#xA;    conf, jsc, profiler_cls)&#xA;  File &quot;/home/ec2-user/anaconda3/lib/python3.6/site-packages/pyspark/context.py&quot;, line 180, in _do_init&#xA;    self._jsc = jsc or self._initialize_context(self._conf._jconf)&#xA;  File &quot;/home/ec2-user/anaconda3/lib/python3.6/site-packages/pyspark/context.py&quot;, line 273, in _initialize_context&#xA;    return self._jvm.JavaSparkContext(jconf)&#xA;  File &quot;/home/ec2-user/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py&quot;, line 1401, in __call__&#xA;  File &quot;/home/ec2-user/anaconda3/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py&quot;, line 319, in get_return_value&#xA;py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.&#xA;: java.lang.NoClassDefFoundError: Could not initialize class org.apache.spark.internal.config.package$&#xA;        at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546)&#xA;        at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:373)&#xA;        at org.apache.spark.api.java.JavaSparkContext.&amp;lt;init&amp;gt;(JavaSparkContext.scala:58)&#xA;        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&#xA;        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&#xA;        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#xA;        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)&#xA;        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)&#xA;        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#xA;        at py4j.Gateway.invoke(Gateway.java:236)&#xA;        at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)&#xA;        at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)&#xA;        at py4j.GatewayConnection.run(GatewayConnection.java:214)&#xA;        at java.lang.Thread.run(Thread.java:748)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="5236286" LastActivityDate="2018-03-03T13:47:20.210" Title="install pyspark in EC2 instance (amazon linux)" Tags="&lt;amazon-ec2&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49081211" PostTypeId="1" CreationDate="2018-03-03T06:01:39.200" Score="0" ViewCount="34" Body="&lt;p&gt;Im trying to submit the following python script into Spark Cluster. I have 2  slaves running &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn import grid_search, datasets&#xA;from sklearn.ensemble import RandomForestClassifier&#xA;# Use spark_sklearn’s grid search instead:&#xA;from spark_sklearn.grid_search import GridSearchCV&#xA;digits = datasets.load_digits()&#xA;X, y = digits.data, digits.target&#xA;param_grid = {&quot;max_depth&quot;: [3, None],&#xA;              &quot;max_features&quot;: [1, 3, 10],&#xA;              &quot;min_samples_split&quot;: [2, 3, 10],&#xA;              &quot;min_samples_leaf&quot;: [1, 3, 10],&#xA;              &quot;bootstrap&quot;: [True, False],&#xA;              &quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;],&#xA;              &quot;n_estimators&quot;: [10, 20, 40, 80]}&#xA;gs = grid_search.GridSearchCV(RandomForestClassifier(), param_grid=param_grid)&#xA;gs.fit(X, y)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm using following command from shell to submit the application&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;./bin/spark-submit --master spark://122.138.1.66:7077 '/script/trainspark.py'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However I dont see that in &quot;Running Applications&quot; section in the Master GUI. Am I missing anything?&lt;/p&gt;&#xA;" OwnerUserId="903521" LastEditorUserId="9297144" LastEditDate="2018-03-03T18:18:26.687" LastActivityDate="2018-03-03T23:17:57.960" Title="Submit Python Script into Spark Cluster" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;&lt;spark-submit&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49081803" PostTypeId="1" AcceptedAnswerId="49114478" CreationDate="2018-03-03T07:27:43.523" Score="2" ViewCount="44" Body="&lt;p&gt;I am trying to use spark to process a large cassandra table (~402 million entries and 84 columns) but I am getting inconsistent results. Initially the requirement was to copy some columns from this table to another table. After copying the data, I noticed that some entries in the new table were missing. To verify that I took count of the large source table but I am getting different values each time. I tried the queries on a smaller table (~7 million records) and the results were fine.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Initially, I attempted to take count using pyspark. Here is my pyspark script:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark = SparkSession.builder.appName(&quot;Datacopy App&quot;).getOrCreate() &#xA;df = spark.read.format(&quot;org.apache.spark.sql.cassandra&quot;).options(table=sourcetable, keyspace=sourcekeyspace).load().cache() &#xA;df.createOrReplaceTempView(&quot;data&quot;) &#xA;query = (&quot;select count(1) from data &quot; ) &#xA;vgDF = spark.sql(query) &#xA;vgDF.show(10)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Spark submit command is as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;~/spark-2.1.0-bin-hadoop2.7/bin/spark-submit --master spark://10.128.0.18:7077 --packages datastax:spark-cassandra-connector:2.0.1-s_2.11 --conf spark.cassandra.connection.host=&quot;10.128.1.1,10.128.1.2,10.128.1.3&quot; --conf &quot;spark.storage.memoryFraction=1&quot; --conf spark.local.dir=/media/db/ --executor-memory 10G --num-executors=6 --executor-cores=2 --total-executor-cores 18 pyspark_script.py&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The above spark submit process takes ~90 minutes to complete. I ran it three times and here are the counts I got:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Spark iteration 1:  402273852&lt;/li&gt;&#xA;&lt;li&gt;Spark iteration 2:  402273884&lt;/li&gt;&#xA;&lt;li&gt;Spark iteration 3:  402274209&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Spark does not show any error or exception during the entire process. I ran the same query in cqlsh thrice and got different results again:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Cqlsh iteration 1:   402273598&lt;/li&gt;&#xA;&lt;li&gt;Cqlsh iteration 2:   402273499&lt;/li&gt;&#xA;&lt;li&gt;Cqlsh iteration 3:   402273515&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I am unable to find out why I am getting different outcomes from the same query. Cassandra system logs (/var/log/cassandra/system.log) has shown the following error message just once:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ERROR [SSTableBatchOpen:3] 2018-02-27 09:48:23,592 CassandraDaemon.java:226 - Exception in thread Thread[SSTableBatchOpen:3,5,main]&#xA;java.lang.AssertionError: Stats component is missing for sstable /media/db/datakeyspace/sensordata1-acfa7880acba11e782fd9bf3ae460699/mc-58617-big&#xA;        at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:460) ~[apache-cassandra-3.9.jar:3.9]&#xA;        at org.apache.cassandra.io.sstable.format.SSTableReader.open(SSTableReader.java:375) ~[apache-cassandra-3.9.jar:3.9]&#xA;        at org.apache.cassandra.io.sstable.format.SSTableReader$4.run(SSTableReader.java:536) ~[apache-cassandra-3.9.jar:3.9]&#xA;        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_131]&#xA;        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]&#xA;        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]&#xA;        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_131]&#xA;        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Versions:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Cassandra 3.9.&lt;/li&gt;&#xA;&lt;li&gt;Spark 2.1.0.&lt;/li&gt;&#xA;&lt;li&gt;Datastax's spark-cassandra-connector 2.0.1&lt;/li&gt;&#xA;&lt;li&gt;Scala version 2.11&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Cluster:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Spark setup with 3 workers and 1 master node.&lt;/li&gt;&#xA;&lt;li&gt;3 worker nodes also have a cassandra cluster installed.&lt;/li&gt;&#xA;&lt;li&gt;Each worker node has 8 CPU cores and 40 GB RAM.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Any help will be greatly appreciated. &lt;/p&gt;&#xA;" OwnerUserId="7878496" LastEditorUserId="7878496" LastEditDate="2018-03-03T08:35:37.823" LastActivityDate="2018-03-05T15:57:47.780" Title="Cassandra/Spark showing incorrect entries count for large table" Tags="&lt;apache-spark&gt;&lt;cassandra&gt;&lt;pyspark&gt;&lt;spark-cassandra-connector&gt;" AnswerCount="1" CommentCount="5" />
  <row Id="49082208" PostTypeId="1" AcceptedAnswerId="49086053" CreationDate="2018-03-03T08:25:27.303" Score="0" ViewCount="38" Body="&lt;p&gt;Sorry for my poor english, I have the following dataframe:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+--------------------+---+---+-----+----+--------+----+&#xA;|                  ak| 1 | 2 |  3  | 4  |   5    |  6 |&#xA;+--------------------+---+---+-----+----+--------+----+&#xA;|8dce120638dbdf438   |  2|  1|    0|   0|       0|   0|&#xA;|3fd28484316249e95   |  1|  0|    3|   1|       4|   5|&#xA;|3636b43f64db33889   |  9|  3|    3|   4|      18|  11|&#xA;+--------------------+---+---+-----+----+--------+----+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and I want transpose it to the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ak                 depth    user_count&#xA;8dce120638dbdf438    1       2&#xA;8dce120638dbdf438    2       1&#xA;8dce120638dbdf438    3       0&#xA;8dce120638dbdf438    4       0&#xA;8dce120638dbdf438    5       0&#xA;8dce120638dbdf438    6       0&#xA;3fd28484316249e95    1       1&#xA;3fd28484316249e95    2       0&#xA;3fd28484316249e95    3       3&#xA;3fd28484316249e95    4       1&#xA;3fd28484316249e95    5       4&#xA;3fd28484316249e95    6       5&#xA;3fd28484316249e95    1       9&#xA;3fd28484316249e95    2       3&#xA;3fd28484316249e95    3       3&#xA;3fd28484316249e95    4       4&#xA;3fd28484316249e95    5       18&#xA;3fd28484316249e95    6       11&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;how to do it in scala?&lt;/p&gt;&#xA;" OwnerUserId="5829526" LastActivityDate="2018-03-03T15:53:38.563" Title="How to transpose a dataframe in spark?" Tags="&lt;scala&gt;&lt;spark-dataframe&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49082238" PostTypeId="1" CreationDate="2018-03-03T08:30:09.233" Score="-1" ViewCount="12" Body="&lt;p&gt;I want to serve mp4 files coming from a Database through sparkjava. It works for all browsers expept all Safari-Based (macOS, iOS etc.)  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I understand that Safari needs the video-media delivered «in parts»: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Range: bytes=0-1  «accept range»  &#xA;HTTP 206  -  etc... &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Question: Should I do this «by hand» or is there a trick using Resource Handlers by configuring the underlying Jetty WebServer? And if: how?   &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Solution:&lt;br&gt;&#xA;I did it «by Hand» - &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;        get(&quot;/videoraw/:id&quot;, (req, res) -&amp;gt; {&#xA;&#xA;        Optional&amp;lt;Range&amp;gt; range = requestRange(req); &#xA;&#xA;        if (range.isPresent()) {&#xA;            int from = range.get().getFrom();&#xA;            int to = range.get().getTo();&#xA;            if (range.get().isSafariFirstBytesRequest()) {&#xA;                int videoLength = dao.getVideoLength(Integer.parseInt(req.params(&quot;:id&quot;)));&#xA;                String contentRangeHeader = Range.generateContentRange(from, to, videoLength);&#xA;                res.header(&quot;Content-Range&quot;, contentRangeHeader);&#xA;                res.type(&quot;video/mp4&quot;);&#xA;                res.header(&quot;Accept-Ranges&quot;, &quot;bytes&quot;);&#xA;                res.status(206);&#xA;                return Range.toEmptyBytes();        &#xA;            } else {&#xA;                byte[] video = dao.getVideo(Integer.parseInt(req.params(&quot;:id&quot;)));&#xA;                String contentRangeHeader = Range.generateContentRange(from, to, video.length);&#xA;                res.header(&quot;Content-Range&quot;, contentRangeHeader);&#xA;                res.type(&quot;video/mp4&quot;);&#xA;                res.header(&quot;Accept-Ranges&quot;, &quot;bytes&quot;);&#xA;                res.status(206);&#xA;                if (range.get().isWholeVideo(video.length)) {&#xA;                    //Browser wollte alles&#xA;                    System.err.println(&quot;Browser requested whole film, not cropping&quot;);&#xA;                    return video; &#xA;                }&#xA;                //uneffizient&#xA;                return Arrays.copyOfRange(video, from, to+1);           &#xA;            }&#xA;&#xA;        }&#xA;&#xA;        res.type(&quot;video/mp4&quot;);&#xA;        res.header(&quot;Accept-Ranges&quot;, &quot;bytes&quot;);&#xA;        return dao.getVideo(Integer.parseInt(req.params(&quot;:id&quot;)));&#xA;    });&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Helper Range Class: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;public class Range {&#xA;&#xA;private int from;&#xA;private int to;&#xA;&#xA;public Range(int from, int to) {&#xA;    this.setFrom(from);&#xA;    this.setTo(to);     &#xA;}&#xA;&#xA;&#xA;boolean isSafariFirstBytesRequest() {&#xA;    return from == 0 &amp;amp;&amp;amp; to == 1;            &#xA;}&#xA;&#xA;public static String generateContentRange(int reqfrom, int reqto, int videoLength) {&#xA;    return String.format(&quot;bytes %s-%s/%s&quot; , reqfrom, reqto, videoLength); &#xA;}&#xA;&#xA;public static byte[] toEmptyBytes() {&#xA;    return new byte[] {0, 0}; &#xA;}&#xA;&#xA;public boolean isWholeVideo(int length) {&#xA;    return from == 0 &amp;amp;&amp;amp; to + 1 == length;&#xA;}&#xA;&#xA;private static Optional&amp;lt;Range&amp;gt; requestRange(Request req) {&#xA;    if (req.headers().contains(&quot;Range&quot;)) {&#xA;&#xA;        String rangeHeader = req.headers(&quot;Range&quot;);&#xA;&#xA;        Matcher match = Pattern.compile(&quot;bytes=(\\d*)-(\\d*)&quot;).matcher(rangeHeader);&#xA;        match.matches();&#xA;        int from = Integer.parseInt(match.group(1));&#xA;&#xA;        String t = match.group(2);&#xA;        if (t.isEmpty()) {&#xA;            System.err.println(String.format(&quot;Browser requested Range from %s to - | ignoring&quot;, from));&#xA;            return Optional.empty();&#xA;        } else {&#xA;            int to = Integer.parseInt(t);&#xA;            System.err.println(String.format(&quot;Browser requested Range from %s to %s&quot;, from, to));&#xA;            return Optional.of(new Range(from, to));&#xA;        }&#xA;&#xA;    } else&#xA;        return Optional.empty();&#xA;&#xA;}&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="1800814" LastEditorUserId="1800814" LastEditDate="2018-03-03T22:44:10.137" LastActivityDate="2018-03-03T22:44:10.137" Title="Sparkjava: delivering ByteRange Partial Content HTTP 206" Tags="&lt;spark-java&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49082286" PostTypeId="1" AcceptedAnswerId="49189334" CreationDate="2018-03-03T08:35:43.467" Score="1" ViewCount="55" Body="&lt;p&gt;I know SparkLauncher is used to launch spark application programmatically instead of using &lt;code&gt;spark-submit&lt;/code&gt; script, but I am feeling a bit confused when to use SparkLauncher or what's the benefit.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Following code is using SparkLauncher to launch a spark application whose main class is &lt;code&gt;&quot;org.apache.spark.launcher.WordCountApp&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The code is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;object WordCountSparkLauncher {&#xA;  def main(args: Array[String]) {&#xA;    val proc = new SparkLauncher()&#xA;      .setAppName(&quot;WordCountSparkLauncherApp&quot;)&#xA;      .setMaster(&quot;local&quot;)&#xA;      .setSparkHome(&quot;D:/spark-2.2.0-bin-hadoop2.7&quot;)&#xA;      .setAppResource(&quot;file:///d:/spark-2.2.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.2.0.jar&quot;)&#xA;      .setVerbose(true)&#xA;      .setMainClass(&quot;org.apache.spark.launcher.WordCountApp&quot;)&#xA;      .launch()&#xA;&#xA;    new Thread(new IORunnable(proc.getInputStream, &quot;proc-input-stream&quot;)).start()&#xA;&#xA;    new Thread(new IORunnable(proc.getErrorStream, &quot;proc-error-input-stream&quot;)).start()&#xA;&#xA;    proc.waitFor()&#xA;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It is working fine,but there is another choice:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Create a runnable fat jar using maven shade plugin to pack all the spark related dependencies into one jar, and in this way, I could still run the spark application with &lt;code&gt;java -jar thefatjar&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the benefits of &lt;code&gt;SparkLauncher&lt;/code&gt; vs a fat runnable jar?&lt;/p&gt;&#xA;" OwnerUserId="4443784" LastEditorUserId="1305344" LastEditDate="2018-03-09T08:18:20.210" LastActivityDate="2018-03-09T10:13:49.553" Title="What are the benefits of SparkLauncher vs java -jar fat-jar?" Tags="&lt;apache-spark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49083253" PostTypeId="1" CreationDate="2018-03-03T10:38:47.883" Score="2" ViewCount="70" Body="&lt;p&gt;I have a Spark application with logic flow as below:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val conf = new SparkConf()&#xA;val sc = new SparkContext(conf)&#xA;val begin = 1&#xA;val end = 100&#xA;val arr = begin.until(end).toArray&#xA;val obj_arr = for(e &amp;lt;- arr) yield {val obj = new MyClass; obj.preprocess(e); obj}&#xA;rdd = sc.parallelize(obj_arr, 4)&#xA;rdd.map{x=&amp;gt;{x.transform()}}.collect() &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;MyClass&lt;/code&gt; is a class I defined to handle data, it has methods &lt;code&gt;preprocess()&lt;/code&gt; and &lt;code&gt;transform()&lt;/code&gt;. &lt;code&gt;preprocess()&lt;/code&gt; prepares the data for &lt;code&gt;transform()&lt;/code&gt;. The data into RDD is an array of &lt;code&gt;MyClass&lt;/code&gt; instances as you can see. So far, it can work as my expectation to execute &lt;code&gt;transform()&lt;/code&gt; on the executors in parallel.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I want to improve it for reducing runtime. In the implementation above, the &lt;code&gt;obj_arr&lt;/code&gt; is created all at once with &lt;code&gt;obj.preprocess(e)&lt;/code&gt; executed on the driver machine. Because both &lt;code&gt;preprocess()&lt;/code&gt; and &lt;em&gt;transform()&lt;/em&gt; are very slow, it takes a lot of time to finish creating obj_arr on the driver machine while during that period the executors are free. I hope that anytime an element in obj_arr is ready (i.e. its &lt;code&gt;preprocess()&lt;/code&gt; is done), its associated task can be distributed to be executed on the executor immediately but not wait until the whole &lt;code&gt;obj_arr&lt;/code&gt; is ready. &lt;strong&gt;Since &lt;code&gt;preprocess()&lt;/code&gt; needs a very huge data file stored on driver machine, I don't want to have it executed on executor to avoid transferring huge data over network. The huge data file cannot be sliced by Spark using default way, I have to load it by special model and retrieve data unit dynamically.&lt;/strong&gt; If the element creation for &lt;code&gt;obj_arr&lt;/code&gt; can be made lazy, it may work, but I don't know how to do this way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;preprocess()&lt;/code&gt; needs data from a file whose size is more than 100G. This file is not a normal txt format or others that can be sliced by Spark. I have to load this file by a special model and retrieve data unit from it dynamically. The model with data loaded is more than 100G as well. If I included the whole model into every map task, a task will be more than 100G as well, I think it is crazy. Alternatively, I can broadcast this model. However, I thing that broadcasting a &gt;100G variable to many executors may be still very slow.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Therefore, I can not create a rdd out of raw data and let Spark slice raw data for distribution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can you suggest how to implement the improvement?&lt;/p&gt;&#xA;" OwnerUserId="7633344" LastEditorUserId="5880706" LastEditDate="2018-03-05T03:01:21.343" LastActivityDate="2018-03-05T03:01:21.343" Title="How to pre-process rdd elements on driver machine and transform ready elements on executors concurrently" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;rdd&gt;" AnswerCount="0" CommentCount="2" FavoriteCount="2" />
  <row Id="49083411" PostTypeId="1" CreationDate="2018-03-03T10:59:29.383" Score="0" ViewCount="9" Body="&lt;p&gt;This is my streaming code &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;session = get_session(SparkConf())&#xA;&#xA;lookup = '/Users/vahagn/stream'&#xA;userSchema = StructType().add(&quot;auction_id&quot;, &quot;string&quot;).add(&quot;dma&quot;, &quot;string&quot;)&#xA;auctions = session.readStream.schema(userSchema).json(&quot;/Users/vahagn/stream/&quot;)&#xA;inputDF = auctions.groupBy(&quot;auction_id&quot;).count()&#xA;print inputDF.isStreaming&#xA;&#xA;inputDF.printSchema()&#xA;inputDF.writeStream.outputMode(&quot;update&quot;).format(&quot;console&quot;).start().awaitTermination()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;After reading first file I'm getting error, which doesn't explain anything.&#xA;Any ideas ?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;/Users/vahagn/hydra/spark/structured_streaming.py&quot;, line 257, in &amp;lt;module&amp;gt;&#xA;    inputDF.writeStream.outputMode(&quot;update&quot;).format(&quot;console&quot;).start().awaitTermination()&#xA;  File &quot;/Users/vahagn/Downloads/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/streaming.py&quot;, line 106, in awaitTermination&#xA;  File &quot;/Users/vahagn/Downloads/spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py&quot;, line 1160, in __call__&#xA;  File &quot;/Users/vahagn/Downloads/spark-2.3.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/utils.py&quot;, line 75, in deco&#xA;pyspark.sql.utils.StreamingQueryException: u'null\n=== Streaming Query ===\nIdentifier: [id = 2f4b442a-38f9-41f1-a3d4-52e0a48427c0, runId = b843f25f-4132-4d52-ae64-f3be5e85a3d9]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {FileStreamSource[file:/Users/vahagn/stream]: {&quot;logOffset&quot;:0}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nAggregate [auction_id#0], [auction_id#0, count(1) AS count#7L]\n+- StreamingExecutionRelation FileStreamSource[file:/Users/vahagn/stream], [auction_id#0, dma#1]\n'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="1696972" LastActivityDate="2018-03-03T10:59:29.383" Title="Spark structured streaming query exception" Tags="&lt;spark-streaming&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49083584" PostTypeId="1" CreationDate="2018-03-03T11:22:01.820" Score="-1" ViewCount="46" Body="&lt;p&gt;I have two types of files data file which is 5GB long and metadata file which is 5MB long. I am supposed to perform two types of experiments &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;to access data file directly &lt;/li&gt;&#xA;&lt;li&gt;to access data file via metadata file which has index in it.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;So calculating the corresponding offset i have to access the data file via metadata file. I'm facing this error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;java.lang.UnsupportedOperationException: empty collection&#xA;      at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)&#xA;      at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$36.apply(RDD.scala:1028)&#xA;      at scala.Option.getOrElse(Option.scala:121)&#xA;      at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1028)&#xA;      at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&#xA;      at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)&#xA;      at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)&#xA;      at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)&#xA;      at $anonfun$compare$2.apply$mcI$sp(&amp;lt;console&amp;gt;:47)&#xA;      at $anonfun$compare$2.apply(&amp;lt;console&amp;gt;:45)&#xA;      at $anonfun$compare$2.apply(&amp;lt;console&amp;gt;:45)&#xA;      at time(&amp;lt;console&amp;gt;:29)&#xA;      at compare(&amp;lt;console&amp;gt;:45)&#xA;      at $anonfun$3.apply(&amp;lt;console&amp;gt;:62)&#xA;      at $anonfun$3.apply(&amp;lt;console&amp;gt;:50)&#xA;      at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)&#xA;      at scala.collection.immutable.List.foreach(List.scala:381)&#xA;      at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;the code which gives me this error.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val metaRecordSize = 300&#xA;val recordSize = 300 * 1024&#xA;val recordLimit = 10000&#xA;val numFiles = 100&#xA;val filterFractions = Seq((1, 100), (1, 50), (1, 30), (1, 20), (1, 10), (1, 5), (1, 3), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10))&#xA;for ((filterN, filterIn) &amp;lt;- filterFractions) {&#xA;    val filterStr = filterN + &quot;/&quot; + filterIn&#xA;    val filterFunc = (idx:Long) =&amp;gt; idx % filterIn &amp;lt;= filterN&#xA;    compare(s&quot;meta_$metaRecordSize-recordsize_$recordSize-${numFiles}_files-filerecordlimit_$recordLimit.txt&quot;, s&quot;filter=$filterStr&quot;, filterFunc)&#xA;    {&#xA;        ASInputFormat.setInputPath(sc.hadoopConfiguration, &quot;/user/purohit/newmetatest/*&quot;)&#xA;        ASInputFormat.setRecordSize(sc.hadoopConfiguration, metaRecordSize)&#xA;        ASInputFormat.setFileRecordLimit(sc.hadoopConfiguration, recordLimit)&#xA;    } {&#xA;        ASInputFormat.setInputPath(sc.hadoopConfiguration, &quot;/user/purohit/newtest/*&quot;)&#xA;        ASInputFormat.setRecordSize(sc.hadoopConfiguration, recordSize)&#xA;        ASInputFormat.setFileRecordLimit(sc.hadoopConfiguration, recordLimit)&#xA;    } { (idx, filename) =&amp;gt;&#xA;        val recordFileName = filename&#xA;        val recordFile = &quot;/user/purohit/newtest/&quot; + recordFileName&#xA;        val offset = idx * recordSize&#xA;        accessRecord(recordFile, recordSize, offset)&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="8088148" LastEditorUserId="4420967" LastEditDate="2018-03-03T12:59:14.810" LastActivityDate="2018-03-03T12:59:14.810" Title="UnsupportedOperationException : empty collection in scala" Tags="&lt;java&gt;&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49083717" PostTypeId="1" AcceptedAnswerId="49085351" CreationDate="2018-03-03T11:39:05.187" Score="0" ViewCount="36" Body="&lt;p&gt;I have a file which contains map structure which needs to be processed.I have used below code.I got the intermediate result as RDD[ROW].Data shown below.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val conf=new SparkConf().setAppName(&quot;student-example&quot;).setMaster(&quot;local&quot;)&#xA;    val sc = new SparkContext(conf)&#xA;    val sqlcontext = new org.apache.spark.sql.SQLContext(sc)&#xA;    val studentdataframe = sqlcontext.read.parquet(&quot;C:\\student_marks.parquet&quot;)&#xA;    studentdataframe.take(4).foreach(println)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Data looks like this.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  [(&quot;Name=aaa&quot;,&quot;sub=math&quot;,Map(&quot;weekly&quot; -&amp;gt; Array(25,24,23),&quot;quaterly&quot; -&amp;gt; Array(25,20,19),&quot;annual&quot; -&amp;gt; Array(90,95,97)),&quot;2018-02-03&quot;)],&#xA;  [(&quot;Name=bbb&quot;,&quot;sub=science&quot;,Map(&quot;weekly&quot; -&amp;gt; Array(25,24,23),&quot;quaterly&quot; -&amp;gt; Array(25,20,19)),&quot;2018-02-03&quot;)],&#xA;  [(&quot;Name=ccc&quot;,&quot;sub=math&quot;,Map(&quot;weekly&quot; -&amp;gt; Array(20,21,18),&quot;quaterly&quot; -&amp;gt; Array(25,16,25)),&quot;2018-02-03&quot;)],&#xA;  [(&quot;Name=ddd&quot;,&quot;sub=math&quot;,Map(&quot;weekly&quot; -&amp;gt; Array(25,24,23),&quot;quaterly&quot; -&amp;gt; Array(21,19,15),&quot;annual&quot; -&amp;gt; Array(91,86,64)),&quot;2018-02-03&quot;)]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Data is in RDD[ROW] format.Here I want the sum of only annual marks.I want to skip the record if annual marks are not there.I want output like this.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Name=aaa|sub=math|282&#xA;Name=ddd|sub=math|241&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Please help me.&lt;/p&gt;&#xA;" OwnerUserId="7255439" LastEditorUserId="5880706" LastEditDate="2018-03-03T14:38:12.307" LastActivityDate="2018-03-03T14:45:36.320" Title="processing map structure using spark" Tags="&lt;apache-spark&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="9" FavoriteCount="1" />
  <row Id="49083783" PostTypeId="1" CreationDate="2018-03-03T11:47:08.187" Score="0" ViewCount="36" Body="&lt;p&gt;I cannot use the &lt;code&gt;input_file_name()&lt;/code&gt; function in Spark 1.6.0 views. It works in select statements or in &lt;code&gt;df.withColumn(&quot;path&quot;, input_file_name())&lt;/code&gt;, but not in a view.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example:  &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;CREATE VIEW v_test AS SELECT *, input_file_name() FROM table&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;fails. It also fails when i use &lt;code&gt;INPUT__FILE__NAME&lt;/code&gt; instead. Just: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SELECT *, input_file_name() FROM table&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;works as expected. Is this a known bug or am i doing something wrong?&#xA;PS: I can create the view in Hive, but cannot access it from Spark as it fails with the same error: unknown function...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt;&lt;br&gt;&#xA;I use Zeppelin with livy interpreter and Scala API.&#xA;The error i get from the above query to create the view is:&lt;br&gt;&#xA;&lt;code&gt;invalid function input_file_name&lt;/code&gt;&lt;br&gt;&#xA;I also tried to import the function, but it has no effect&lt;/p&gt;&#xA;" OwnerUserId="6162321" LastEditorUserId="5880706" LastEditDate="2018-03-03T12:53:51.210" LastActivityDate="2018-03-03T12:53:51.210" Title="Input file name in Spark 1.6.0 View" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;hive&gt;&lt;apache-spark-sql&gt;&lt;apache-zeppelin&gt;" AnswerCount="1" CommentCount="6" />
  <row Id="49084306" PostTypeId="1" AcceptedAnswerId="49091724" CreationDate="2018-03-03T12:51:26.300" Score="1" ViewCount="33" Body="&lt;p&gt;I am trying to submit a spark job on Dataproc cluster. The job needs multiple system properties. I am able to pass just one as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;gcloud dataproc jobs submit spark \                                   &#xA;    --cluster &amp;lt;cluster_name&amp;gt; \&#xA;    --class &amp;lt;class_name&amp;gt; \&#xA;    --properties spark.driver.extraJavaOptions=-Dhost=127.0.0.1  \&#xA;    --jars spark_job.jar&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How do I pass multiple properties?&#xA;I tried as follow, even this didn't work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;--properties ^#^spark.driver.extraJavaOptions=-Dhost=127.0.0.1,-Dlimit=10&lt;/code&gt;&lt;/p&gt;&#xA;" OwnerUserId="1315878" LastEditorUserId="4482491" LastEditDate="2018-03-05T14:39:01.193" LastActivityDate="2018-03-05T14:39:01.193" Title="Passing multiple system properties to google dataproc cluster job" Tags="&lt;apache-spark&gt;&lt;google-cloud-platform&gt;&lt;gcloud&gt;&lt;google-cloud-dataproc&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49084373" PostTypeId="1" CreationDate="2018-03-03T12:59:04.670" Score="0" ViewCount="36" Body="&lt;p&gt;I am trying to setup an environment for apache-spark and found out it is incompatible to Java9. (Well I regret for not finding out this earlier.).I am not able to make spark work or java9 uninstalled. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried both approaches &lt;a href=&quot;https://www.java.com/en/download/help/mac_uninstall_java.xml&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://stackoverflow.com/questions/46436879/spark-shell-failed-to-initialize-compiler-error-on-a-mac&quot;&gt;here&lt;/a&gt;  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;None of these are yielding any results. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I run '&lt;code&gt;java --version&lt;/code&gt;' in my terminal following is the output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;java 9.0.4&#xA;Java(TM) SE Runtime Environment (build 9.0.4+11)&#xA;Java HotSpot(TM) 64-Bit Server VM (build 9.0.4+11, mixed mode)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My issue now is to uninstall Java9, reinstall Java8 and the reconfigure spark. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any leads/help on this to this? &lt;/p&gt;&#xA;" OwnerUserId="5120817" LastActivityDate="2018-03-03T13:42:21.063" Title="uninstall java9 in High Sierra" Tags="&lt;java&gt;&lt;apache-spark&gt;&lt;macos-high-sierra&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49084409" PostTypeId="1" AcceptedAnswerId="49085487" CreationDate="2018-03-03T13:03:04.370" Score="0" ViewCount="49" Body="&lt;p&gt;For below Dataset I need to get a Summary Data based on Selected Column The sample Dataset contains Following Below Data.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+---------+----------+--------+---------+&#xA;| Column1 | Column2  | Expend | Expend2 |&#xA;+---------+----------+--------+---------+&#xA;| School1 | Student1 | 5      | 10      |&#xA;+---------+----------+--------+---------+&#xA;| School1 | Student2 | 11     | 12      |&#xA;+---------+----------+--------+---------+&#xA;| School2 | Student1 | 6      | 8       |&#xA;+---------+----------+--------+---------+&#xA;| School2 | Student2 | 7      | 8       |&#xA;+---------+----------+--------+---------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I need to get Summary Data for Column2 as below,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Required Format&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+---------+----------+--------+---------+&#xA;| Column1 | Column2  | Expend | Expend2 |&#xA;+---------+----------+--------+---------+&#xA;| School1 | Total    | 16     | 22      |&#xA;+---------+----------+--------+---------+&#xA;| School1 | Student1 | 5      | 10      |&#xA;+---------+----------+--------+---------+&#xA;| School1 | Student2 | 11     | 12      |&#xA;+---------+----------+--------+---------+&#xA;| School2 | Total    | 13     | 16      |&#xA;+---------+----------+--------+---------+&#xA;| School2 | Student1 | 6      | 8       |&#xA;+---------+----------+--------+---------+&#xA;| School2 | Student2 | 7      | 8       |&#xA;+---------+----------+--------+---------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I tried using cube function on dataset but that didn't give me expected results. &#xA;I get &lt;code&gt;null&lt;/code&gt; values in place of &lt;code&gt;Total&lt;/code&gt; which is also okay, but the data I dont get in above format.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wanted to try using &lt;code&gt;dataset.cube(&quot;Column2&quot;).agg(sum(&quot;Expend1&quot;),sum(&quot;Expend2&quot;))&lt;/code&gt;;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But this above line of code gives me only data for Column2 , How Can I retrieve Column1 values with above return data.&lt;/p&gt;&#xA;" OwnerUserId="8854940" LastEditorUserId="8854940" LastEditDate="2018-03-05T06:57:17.883" LastActivityDate="2018-03-05T06:57:17.883" Title="Summary of a Column (Achieving a Cube Function on Spark Dataset)" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;rdd&gt;&lt;apache-spark-dataset&gt;" AnswerCount="1" CommentCount="6" />
  <row Id="49084489" PostTypeId="1" CreationDate="2018-03-03T13:10:11.940" Score="0" ViewCount="24" Body="&lt;p&gt;I have a spark job that moves data from Postgres to Redshift on regular basis. I' using &lt;code&gt;jdbc.read&lt;/code&gt; function with &lt;code&gt;lowerBound&lt;/code&gt; and &lt;code&gt;upperBound&lt;/code&gt; params:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df = spark.read.jdbc(url=jdbc_url, \&#xA;          table='some_table',\&#xA;          column='id',\&#xA;          lowerBound=1,\&#xA;          upperBound=20000000, \&#xA;          numPartitions=50)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;At the current moment &lt;code&gt;upperBound&lt;/code&gt; is hardcoded, but the size of table growing every day, so I need somehow update &lt;code&gt;upperBound&lt;/code&gt; value dynamically to reflect the size of the table at the start of next job run. How can I make &lt;code&gt;upperBound&lt;/code&gt; value equal to current size of the table?&lt;/p&gt;&#xA;" OwnerUserId="1345788" LastEditorUserId="1345788" LastEditDate="2018-03-04T01:15:18.530" LastActivityDate="2018-03-04T01:15:18.530" Title="How to amend properties in Spark read jdbc according to growing size of table?" Tags="&lt;postgresql&gt;&lt;apache-spark&gt;&lt;etl&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49084883" PostTypeId="1" AcceptedAnswerId="49085596" CreationDate="2018-03-03T13:53:32.733" Score="0" ViewCount="61" Body="&lt;p&gt;can anyone help me&#xA;I have an RDD of BitSets like&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Array[scala.collection.mutable.BitSet] = Array(BitSet(1, 2), BitSet(1, 7), BitSet(8, 9, 10, 11), BitSet(1, 2, 3, 4),BitSet(8,9,10),BitSet(1,2,3))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want rdd of &lt;code&gt;(BitSet(1,2),BitSet(1,7),BitSet(8,9,10))&lt;/code&gt;&#xA;that is i require least subset or the BitSet which is not subset to any BitSet&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Explanation:&#xA;here &lt;code&gt;(1,2),(1,2,3)(1,2,3,4)&lt;/code&gt; -----here least subset is &lt;code&gt;(1,2)&lt;/code&gt; and&#xA;&lt;code&gt;(1,7)&lt;/code&gt; is not subset to other BitSets ---so &lt;code&gt;(1,7)&lt;/code&gt; also present in the result&lt;/p&gt;&#xA;" OwnerUserId="4407887" LastEditorUserId="5880706" LastEditDate="2018-03-03T13:56:03.083" LastActivityDate="2018-03-03T15:09:38.723" Title="finding least subset in an rdd in spark" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49085068" PostTypeId="1" CreationDate="2018-03-03T14:15:39.520" Score="0" ViewCount="36" Body="&lt;p&gt;First Situation:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have downloaded spark on ubuntu 14. But when am running it below result is coming. Its not able to start properly and asking root user password.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;root@sueeze-Lenovo-G580:~/Spark/spark-2.3.0-bin-hadoop2.6/sbin# ./start-all.sh&#xA;starting org.apache.spark.deploy.master.Master, logging to /home/sueeze/Spark/spark-2.3.0-bin-hadoop2.6/logs/spark-root-org.apache.spark.deploy.master.Master-1-ashish-Lenovo-G580.out&#xA;root@localhost's password: &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;May I know please:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) What is this ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) Why its happening ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;3) Why its asking root's password ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Situation 2:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have fixed situation one by adding localhost to in slave file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now both master and worker is running. I want to know what was missed but when I add localhost it started running ?&#xA;But it still asks me root's password when it starts worker node. I mean first it starts Master but when the moment it goes to worker to start it asks for root's password. Why ? And how can I fix that...?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm running spark only on my local machine so master and slave for both my local machine is available. Correct...?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;" OwnerUserId="9392085" LastActivityDate="2018-03-03T15:19:29.753" Title="apache saprk not able to start properly" Tags="&lt;apache-spark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49086041" PostTypeId="1" CreationDate="2018-03-03T15:52:45.050" Score="0" ViewCount="38" Body="&lt;p&gt;I'm learning apache spark using scala. I'm searching for what are the reasons that makes user defined functions to fail from executing. I think I can understand more by posting this question here instead of googling it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is the udf I used&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val evaluator = new BinaryClassificationEvaluator()&#xA;  .setLabelCol(&quot;label&quot;)&#xA;  .setRawPredictionCol(&quot;prediction&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and getting&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ERROR Executor: Exception in task 0.0 in stage 516.0 (TID 880)&#xA;org.apache.spark.SparkException: Failed to execute user defined&#xA;function($anonfun$5: (string) =&amp;gt; double) &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sorry for my poor explanation. Im new to spark and scala. Actually the following code is to make churn prediction. I referred this &lt;a href=&quot;https://mapr.com/blog/churn-prediction-sparkml/&quot; rel=&quot;nofollow noreferrer&quot;&gt;website&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The class is &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;case class Account(state: String, len: Integer, acode: String,&#xA;intlplan: String, vplan: String, numvmail: Double,&#xA;tdmins: Double, tdcalls: Double, tdcharge: Double,&#xA;temins: Double, tecalls: Double, techarge: Double,&#xA;tnmins: Double, tncalls: Double, tncharge: Double,&#xA;timins: Double, ticalls: Double, ticharge: Double,&#xA;numcs: Double, churn: String)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The Schema is &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val schema = StructType(Array(&#xA;StructField(&quot;state&quot;, StringType, true),&#xA;StructField(&quot;len&quot;, IntegerType, true),&#xA;StructField(&quot;acode&quot;, StringType, true),&#xA;StructField(&quot;intlplan&quot;, StringType, true),&#xA;StructField(&quot;vplan&quot;, StringType, true),&#xA;StructField(&quot;numvmail&quot;, DoubleType, true),&#xA;StructField(&quot;tdmins&quot;, DoubleType, true),&#xA;StructField(&quot;tdcalls&quot;, DoubleType, true),&#xA;StructField(&quot;tdcharge&quot;, DoubleType, true),&#xA;StructField(&quot;temins&quot;, DoubleType, true),&#xA;StructField(&quot;tecalls&quot;, DoubleType, true),&#xA;StructField(&quot;techarge&quot;, DoubleType, true),&#xA;StructField(&quot;tnmins&quot;, DoubleType, true),&#xA;StructField(&quot;tncalls&quot;, DoubleType, true),&#xA;StructField(&quot;tncharge&quot;, DoubleType, true),&#xA;StructField(&quot;timins&quot;, DoubleType, true),&#xA;StructField(&quot;ticalls&quot;, DoubleType, true),&#xA;StructField(&quot;ticharge&quot;, DoubleType, true),&#xA;StructField(&quot;numcs&quot;, DoubleType, true),&#xA;StructField(&quot;churn&quot;, StringType, true)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The indexers, assembler and tree used are&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val ipindexer = new StringIndexer()&#xA;  .setInputCol(&quot;intlplan&quot;)&#xA;  .setOutputCol(&quot;iplanIndex&quot;)&#xA;val labelindexer = new StringIndexer()&#xA;  .setInputCol(&quot;churn&quot;)&#xA;  .setOutputCol(&quot;label&quot;)&#xA;val featureCols = Array(&quot;len&quot;, &quot;iplanIndex&quot;, &quot;numvmail&quot;, &quot;tdmins&quot;, &quot;tdcalls&quot;, &quot;temins&quot;, &quot;tecalls&quot;, &quot;tnmins&quot;, &quot;tncalls&quot;, &quot;timins&quot;, &quot;ticalls&quot;, &quot;numcs&quot;)&#xA;&#xA;val assembler = new VectorAssembler()&#xA;  .setInputCols(featureCols)&#xA;  .setOutputCol(&quot;features&quot;)&#xA;&#xA;val dTree = new DecisionTreeClassifier().setLabelCol(&quot;label&quot;)&#xA;  .setFeaturesCol(&quot;features&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Chaining indexers and tree in a pipeline&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val pipeline = new Pipeline()&#xA;  .setStages(Array(ipindexer, labelindexer, assembler, dTree))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Searching through decision tree's maxDepth parameter for best model&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val paramGrid = new ParamGridBuilder().addGrid(dTree.maxDepth, Array(2, 3, 4, 5, 6, 7)).build()&#xA;&#xA;val evaluator = new BinaryClassificationEvaluator() //I thought this was the udf&#xA;  .setLabelCol(&quot;label&quot;)&#xA;  .setRawPredictionCol(&quot;prediction&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Setting up 3-fold cross validation&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val crossval = new CrossValidator().setEstimator(pipeline)&#xA;  .setEvaluator(evaluator)&#xA;  .setEstimatorParamMaps(paramGrid).setNumFolds(3)&#xA;&#xA;val cvModel = crossval.fit(ntrain)&#xA;&#xA;val bestModel = cvModel.bestModel&#xA;println(&quot;The Best Model and Parameters:\n--------------------&quot;)&#xA;println(bestModel.asInstanceOf[org.apache.spark.ml.PipelineModel].stages(3))&#xA;bestModel.asInstanceOf[org.apache.spark.ml.PipelineModel]&#xA;  .stages(3)&#xA;  .extractParamMap&#xA;&#xA;val treeModel = bestModel.asInstanceOf[org.apache.spark.ml.PipelineModel].stages(3).asInstanceOf[DecisionTreeClassificationModel]&#xA;println(&quot;Learned classification tree model:\n&quot; + treeModel.toDebugString)&#xA;&#xA;val predictions = cvModel.transform(test)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And when I execute the next line Im getting error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val accuracy = evaluator.evaluate(predictions)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The error is (slightly big)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    18/03/04 19:57:11 ERROR Executor: Exception in task 0.0 in stage 486.0 (TID 468)&#xA;    org.apache.spark.SparkException: Failed to execute user defined function($anonfun$5: (string) =&amp;gt; double)&#xA;at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)&#xA;at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)&#xA;at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)&#xA;at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)&#xA;at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)&#xA;at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)&#xA;at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)&#xA;at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)&#xA;at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)&#xA;at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)&#xA;at org.apache.spark.scheduler.Task.run(Task.scala:108)&#xA;at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)&#xA;at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;at java.lang.Thread.run(Thread.java:748)&#xA;    Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.&#xA;at org.apache.spark.ml.feature.StringIndexerModel$$anonfun$5.apply(StringIndexer.scala:213)&#xA;at org.apache.spark.ml.feature.StringIndexerModel$$anonfun$5.apply(StringIndexer.scala:208)&#xA;... 15 more&#xA;    18/03/04 19:57:11 WARN TaskSetManager: Lost task 0.0 in stage 486.0 (TID 468, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$5: (string) =&amp;gt; double)&#xA;at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)&#xA;at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)&#xA;at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)&#xA;at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)&#xA;at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)&#xA;at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)&#xA;at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)&#xA;at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)&#xA;at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)&#xA;at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)&#xA;at org.apache.spark.scheduler.Task.run(Task.scala:108)&#xA;at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)&#xA;at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;at java.lang.Thread.run(Thread.java:748)&#xA;    Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.&#xA;at org.apache.spark.ml.feature.StringIndexerModel$$anonfun$5.apply(StringIndexer.scala:213)&#xA;at org.apache.spark.ml.feature.StringIndexerModel$$anonfun$5.apply(StringIndexer.scala:208)&#xA;... 15 more&#xA;&#xA;    18/03/04 19:57:11 ERROR TaskSetManager: Task 0 in stage 486.0 failed 1 times; aborting job&#xA;    org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 486.0 failed 1 times, most recent failure: Lost task 0.0 in stage 486.0 (TID 468, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$5: (string) =&amp;gt; double)&#xA;at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)&#xA;at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)&#xA;at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)&#xA;at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)&#xA;at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)&#xA;at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)&#xA;at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)&#xA;at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)&#xA;at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)&#xA;at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)&#xA;at org.apache.spark.scheduler.Task.run(Task.scala:108)&#xA;at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)&#xA;at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;at java.lang.Thread.run(Thread.java:748)&#xA;    Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.&#xA;at org.apache.spark.ml.feature.StringIndexerModel$$anonfun$5.apply(StringIndexer.scala:213)&#xA;at org.apache.spark.ml.feature.StringIndexerModel$$anonfun$5.apply(StringIndexer.scala:208)&#xA;... 15 more&#xA;&#xA;    Driver stacktrace:&#xA;  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)&#xA;  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)&#xA;  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)&#xA;  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&#xA;  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)&#xA;  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)&#xA;  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)&#xA;  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)&#xA;  at scala.Option.foreach(Option.scala:257)&#xA;  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)&#xA;  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)&#xA;  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)&#xA;  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)&#xA;  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)&#xA;  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)&#xA;  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)&#xA;  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)&#xA;  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)&#xA;  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)&#xA;  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)&#xA;  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&#xA;  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)&#xA;  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)&#xA;  at org.apache.spark.rdd.RDD.collect(RDD.scala:935)&#xA;  at org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4$lzycompute(BinaryClassificationMetrics.scala:192)&#xA;  at org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4(BinaryClassificationMetrics.scala:146)&#xA;  at org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:148)&#xA;  at org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:148)&#xA;  at org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:223)&#xA;  at org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:86)&#xA;  at org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:97)&#xA;  at org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:87)... 58 elided&#xA;&#xA;    Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$5: (string) =&amp;gt; double)&#xA;  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)&#xA;  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)&#xA;  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)&#xA;  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)&#xA;  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)&#xA;  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)&#xA;  at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)&#xA;  at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)&#xA;  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)&#xA;  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)&#xA;  at org.apache.spark.scheduler.Task.run(Task.scala:108)&#xA;  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)&#xA;  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;  at java.lang.Thread.run(Thread.java:748)&#xA;    Caused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.&#xA;  at org.apache.spark.ml.feature.StringIndexerModel$$anonfun$5.apply(StringIndexer.scala:213)&#xA;  at org.apache.spark.ml.feature.StringIndexerModel$$anonfun$5.apply(StringIndexer.scala:208)... 15 more&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="9298476" LastEditorUserId="9298476" LastEditDate="2018-03-04T15:03:50.683" LastActivityDate="2018-03-04T15:03:50.683" Title="Why does &quot;failed to execute user defined function&quot; error occurs in spark scala?" Tags="&lt;sql&gt;&lt;scala&gt;&lt;apache-spark&gt;&lt;machine-learning&gt;&lt;apache-spark-mllib&gt;" AnswerCount="0" CommentCount="8" ClosedDate="2018-03-04T15:42:13.170" />
  <row Id="49086245" PostTypeId="1" CreationDate="2018-03-03T16:09:30.460" Score="0" ViewCount="15" Body="&lt;p&gt;When I am configuring spark environment in Scala Eclipse. I am not able to use this import&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import org.apache.spark (showing error cant find spark)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have tried multiple jars and diff combinations can anyone please let me know how to proceed. I can't find appropriate Jar for this as I am stuck in this for &lt;code&gt;org.apache.spark&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried adding jars from &lt;code&gt;usr/lib/spark/lib&lt;/code&gt;, but didn't work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone tell me which jars I need to import for this and where to find them as I am using top-gear hadoop-cloudera environment.&lt;/p&gt;&#xA;" OwnerUserId="9152242" LastEditorUserId="181087" LastEditDate="2018-03-04T02:12:13.177" LastActivityDate="2018-03-04T02:12:13.177" Title="Issues while configuring spark on scala eclipse IDE" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;bigdata&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49086470" PostTypeId="1" CreationDate="2018-03-03T16:30:39.303" Score="0" ViewCount="40" Body="&lt;p&gt;I am building a Text classifier pipeline in spark ml with the following stages:&#xA;Ngram, Vectorizer, IDF, Logistic regression.&#xA;Since we're not using a distributed file system I've to save the generated model on the file system by deserializing the model as a java/scala object. (Spark's default model persistence doesn't work on a cluster without a distributed FS).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For predictions, I'm reading in that serialized model file and using that on a dataset.But I keep getting the following errors/exceptions in that prediction process.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;org.apache.spark.SparkException: Failed to get broadcast_40_piece0 of broadcast_40&#xA;...&#xA;ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)&#xA;org.apache.spark.SparkException: Failed to execute user defined function($anonfun$8: (array&amp;lt;string&amp;gt;) =&amp;gt; vector)&#xA;...&#xA;ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job&#xA;Exception in thread &quot;main&quot; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$8: (array&amp;lt;string&amp;gt;) =&amp;gt; vector)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've filtered the full stack trace as I thought it is not needed. &quot;Failed to get the broadcast piece&quot; comes a couple of times as spark retries it. And particularly, the &lt;code&gt;udf&lt;/code&gt; error &lt;code&gt;(array&amp;lt;string&amp;gt;) =&amp;gt; vector)&lt;/code&gt; might be the Vectorization stage that I've in my model but I can't be sure.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I didn't find anything similar online regarding this. So far, I have done the following with no luck.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;I tried saving the model the usual spark way (with model.save) on a single node and I was able to run the predictions fine showing that the code is fine.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;In order to make sure that the model is getting properly serialized, I created a CustomModel class that extends serializable with the pipeline model as a parameter. It is still giving the same issue&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I know that this question is long now but any ideas on how this can be fixed are appreciated. Any ideas on getting around this issue of saving the model without having a DFS are also welcome.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT:&#xA;The code I am using to save and read the model is as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// Model creating &amp;amp; saving&#xA;val cvModel: PipelineModel =  cv.fit(training).bestModel.asInstanceOf[PipelineModel]&#xA;val model = new Model(cvModel)&#xA;val fileOutputStream = new FileOutputStream(&quot;model.model&quot;)&#xA;val outputStream = new ObjectOutputStream(fileOutputStream)&#xA;outputStream.writeObject(model)&#xA;outputStream.close()&#xA;fileOutputStream.close()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Where &lt;code&gt;Model&lt;/code&gt; is a class that extends &lt;code&gt;Serializable&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;class Model(val model: PipelineModel) extends Serializable&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And then for reading the model back&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// Deserializing the model&#xA;val fileInputStream: FileInputStream = new FileInputStream(&quot;model.model&quot;)&#xA;val objectInputStream: ObjectInputStream = new ObjectInputStream(fileInputStream)&#xA;val model: predictive.Model = objectInputStream.readObject.asInstanceOf[predictive.Model]&#xA;fileInputStream.close()&#xA;objectInputStream.close()&#xA;val pipelineModel = model.model&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;After I have the model deserialized, I predict on a dataset by calling the &lt;code&gt;transform&lt;/code&gt; method.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;model.transform(dataset)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="3542989" LastEditorUserId="3542989" LastEditDate="2018-03-03T17:20:54.233" LastActivityDate="2018-03-03T17:20:54.233" Title="Deserialization of a Spark model causes &quot;Spark exception : Failed to get broadcast_40_piece0 of broadcast_40&quot;" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;serialization&gt;&lt;deserialization&gt;&lt;apache-spark-ml&gt;" AnswerCount="0" CommentCount="4" FavoriteCount="1" />
  <row Id="49088401" PostTypeId="1" CreationDate="2018-03-03T19:37:14.923" Score="0" ViewCount="70" Body="&lt;p&gt;I am trying to use spark for processing JSON data with variable structure(nested JSON). Input JSON data could be very large with more than 1000 of keys per row and one batch could be more than 20 GB. &#xA;Entire batch has been generated from 30 data sources and 'key2' of each JSON can be used to identify the source and structure for each source is predefined.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What would be the best approach for processing such data?&#xA;I have tried using from_json like below but it works only with fixed schema and to use it first I need to group the data based on each source and then apply the schema. &#xA;Due to large data volume my preferred choice is to scan the data only once and extract required values from each source, based on predefined schema.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import org.apache.spark.sql.types._ &#xA;import spark.implicits._&#xA;&#xA;val data = sc.parallelize(&#xA;    &quot;&quot;&quot;{&quot;key1&quot;:&quot;val1&quot;,&quot;key2&quot;:&quot;source1&quot;,&quot;key3&quot;:{&quot;key3_k1&quot;:&quot;key3_v1&quot;}}&quot;&quot;&quot;&#xA;    :: Nil)&#xA;val df = data.toDF&#xA;&#xA;&#xA;val schema = (new StructType)&#xA;    .add(&quot;key1&quot;, StringType)&#xA;    .add(&quot;key2&quot;, StringType)&#xA;    .add(&quot;key3&quot;, (new StructType)&#xA;    .add(&quot;key3_k1&quot;, StringType))&#xA;&#xA;&#xA;df.select(from_json($&quot;value&quot;,schema).as(&quot;json_str&quot;))&#xA;  .select($&quot;json_str.key3.key3_k1&quot;).collect&#xA;res17: Array[org.apache.spark.sql.Row] = Array([xxx])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="7468028" LastEditorUserId="7468028" LastEditDate="2018-03-04T01:48:29.183" LastActivityDate="2018-03-04T03:54:01.613" Title="Spark from_json with dynamic schema" Tags="&lt;json&gt;&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="1" />
  <row Id="49088796" PostTypeId="1" CreationDate="2018-03-03T20:17:15.640" Score="1" ViewCount="37" Body="&lt;p&gt;I am using Spark Structured streaming; My DataFrame has the following schema &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;root &#xA; |-- data: struct (nullable = true) &#xA; |    |-- zoneId: string (nullable = true) &#xA; |    |-- deviceId: string (nullable = true) &#xA; |    |-- timeSinceLast: long (nullable = true) &#xA; |-- date: date (nullable = true) &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How can I do a writeStream with Parquet format and write the data &#xA;(containing zoneId, deviceId, timeSinceLast; everything except date) and partition the data by date ? I tried the following code and the partition by clause did &#xA;not work &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val query1 = df1 &#xA;  .writeStream &#xA;  .format(&quot;parquet&quot;) &#xA;  .option(&quot;path&quot;, &quot;/Users/abc/hb_parquet/data&quot;) &#xA;  .option(&quot;checkpointLocation&quot;, &quot;/Users/abc/hb_parquet/checkpoint&quot;) &#xA;  .partitionBy(&quot;data.zoneId&quot;) &#xA;  .start() &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="452332" LastActivityDate="2018-03-05T00:09:25.027" Title="Parquet data and partition issue in Spark Structured streaming" Tags="&lt;apache-spark&gt;&lt;parquet&gt;&lt;structured-streaming&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="1" />
  <row Id="49089601" PostTypeId="1" CreationDate="2018-03-03T21:52:16.500" Score="0" ViewCount="14" Body="&lt;p&gt;I'm having trouble understanding how to utilize the power of parallel processing in my python script.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a billion row database of food items, e.g:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;date,item,category,number_sold&#xA;2017-01-01,bagel,bread,10&#xA;2017-03-03,skittles,candy,5&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There are 100 categories. (bread, candy, etc.) my script:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1. Iterates list of 100 categories&#xA;2. Filter pyspark dataframe for the subset (e.g. category == 'bread')&#xA;3. Run aggregate calculations on subset&#xA;4. Generate 1 row of stats and appends to summary file&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What do I need to add into the iterative loop to trigger multi-processing? Does pyspark do this automatically? When I ran this only using Pandas, the script was held up (not doing anything) while waiting to query each category subset. Ideally, the process should filter the dataframe for one category and run calculations for another category at the same time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance!&lt;/p&gt;&#xA;" OwnerUserId="7542835" LastActivityDate="2018-03-03T22:07:18.677" Title="Python/PySpark parallel processing example" Tags="&lt;python&gt;&lt;pandas&gt;&lt;parallel-processing&gt;&lt;pyspark&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49089809" PostTypeId="1" CreationDate="2018-03-03T22:16:43.853" Score="0" ViewCount="34" Body="&lt;p&gt;I have a table in SparkServer that looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;--------------------------------------&#xA;OrderID  OrderDate  OrderNum  Date&#xA;--------------------------------------&#xA;121      1/1/2018   A1        1/3/2018&#xA;122      1/2/2018   A2        1/3/2018&#xA;121      1/1/2018   A1        1/4/2018&#xA;122      1/2/2018   A2        1/4/2018&#xA;123      1/4/2018   A3        1/4/2018&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now if you closely look at the data, you will notice that each &lt;strong&gt;Date&lt;/strong&gt; partition has all of historical records plus the new records that came on that date.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now what I'd like to do is write a query that always extracts the records from latest Date partition without using max(Date) function or hardcoding the &lt;strong&gt;Date='1/4/2018'&lt;/strong&gt; in WHERE clause. I want to avoid aggregate function such as max(Date) due to performance issue and I can not hardcode Date partition value because I'd like this query to automatically fetch the records to refresh a dashboard.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a way to do it?&lt;/p&gt;&#xA;" OwnerUserId="1992172" LastActivityDate="2018-03-06T09:37:48.740" Title="Write a query that always takes latest date partition to provide results without using max() aggregate function" Tags="&lt;apache-spark-sql&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49089866" PostTypeId="1" CreationDate="2018-03-03T22:22:02.737" Score="2" ViewCount="36" Body="&lt;p&gt;&lt;a href=&quot;https://people.csail.mit.edu/matei/papers/2013/sosp_spark_streaming.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;Apache Spark&lt;/a&gt; brags that its operators (nodes) are &quot;stateless&quot;. This allows Spark's architecture to use simpler protocols for things like recovery, load balancing, and handling stragglers.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the other hand &lt;a href=&quot;http://asterios.katsifodimos.com/assets/publications/flink-deb.pdf&quot; rel=&quot;nofollow noreferrer&quot;&gt;Apache Flink&lt;/a&gt; describes its operators as &quot;stateful&quot;, and claim that statefulness is necessary for applications like machine learning. Yet Spark programs are able to pass forward information and maintain application data in RDDs without maintaining &quot;state&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is happening here? Is Spark not a true stateless system? Or is Flink's assertion that statefulness is essential for machine learning and similar application incorrect? Or is there some additional nuance here?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't feel like I truly grok the difference between &quot;stateful&quot; and &quot;stateless&quot; systems, and I would appreciate if they could be explained.&lt;/p&gt;&#xA;" OwnerUserId="3180238" LastEditorUserId="3180238" LastEditDate="2018-03-04T04:41:37.347" LastActivityDate="2018-03-04T04:41:37.347" Title="What is the difference between a &quot;stateful&quot; and &quot;stateless&quot; system?" Tags="&lt;apache-spark&gt;&lt;streaming&gt;&lt;spark-streaming&gt;&lt;state&gt;&lt;apache-flink&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49089935" PostTypeId="1" CreationDate="2018-03-03T22:33:21.743" Score="0" ViewCount="4" Body="&lt;p&gt;I'm getting this warning when invoking createOrReplaceTempView with a R data frame:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;createOrReplaceTempView (as.Data.Frame(products), &quot;prod&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Do I should ignore this warning? This is inefficient?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;" OwnerUserId="285739" LastActivityDate="2018-03-03T22:33:21.743" Title="SparkR Stage X contains a task of very large size" Tags="&lt;apache-spark&gt;&lt;sparkr&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49090042" PostTypeId="1" CreationDate="2018-03-03T22:47:26.630" Score="0" ViewCount="28" Body="&lt;p&gt;I'm new to Apache-Spark and looking for a way to solve an issue i'm faced with. I have a Cassandra cluster set up with 6 nodes. I installed Spark and Thrift Server so that we could connect to the Cassandra DB using Tableu. We have a set up of 1 Master Spark Node and 2 Worker Nodes. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I start the thrift server as below&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/usr/local/spark/sbin/start-thriftserver.sh —-master spark://&amp;lt;spark-&#xA;master&amp;gt;:7077 –hiveconf hive.server2.thrift.port 10015 --jars &#xA;/var/spark-connector/jars/spark-cassandra-connector-2.0.0-M2-s_2.11.jar &#xA;--driver-class-path /var/spark-connector/jars/spark-cassandra-&#xA;connector-2.0.0-M2-s_2.11.jar --conf spark.cassandra.connection.host=&quot;&#xA;&amp;lt;i.p1&amp;gt;,&amp;lt;i.p2&amp;gt;,&amp;lt;i.p3&amp;gt;,&amp;lt;i.p4&amp;gt;,&amp;lt;i.p5&amp;gt;,&amp;lt;i.p6&amp;gt;&quot; --driver-memory 5G --&#xA;executor-memory 5G&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm able to go to &lt;code&gt;http://spark-master:8080/&lt;/code&gt; and view the Thrift Server that i just started. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, I'm connecting to beeline using &lt;code&gt;!connect jdbc:hive2://spark-master:10000&lt;/code&gt;, I can create views in spark for keyspaces in Cassandra and run queries but none of them are actually using the spark cluster. Meaning, I don't see any jobs in the Spark Master UI. Am I missing something really basic here? Can i even have beeline use the Spark Master? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note: I have Thrift Server, Spark Master running on the same node. &lt;/p&gt;&#xA;" OwnerUserId="2051904" LastActivityDate="2018-03-03T22:47:26.630" Title="Can we connect to Spark Master using Beeline" Tags="&lt;apache-spark&gt;&lt;cassandra&gt;&lt;beeline&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49090385" PostTypeId="1" CreationDate="2018-03-03T23:36:38.093" Score="0" ViewCount="35" Body="&lt;p&gt;I am slowly dipping my toe into graph theory to try to map out a network. I have a list of start vertices(addresses) and end vertices(amplifiers) and have successfully converted my dataframe that contains them into a graphframe in spark. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am also able to successfully run bfs on the start vertices and the end vertices to find the nearest amplifier(end) to each house(start) When I use a specific address id it returns the nearest amplifier no matter how far away it is. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val dev_paths: DataFrame = dev_gf.bfs.fromExpr(&quot;id = 526027779&quot;).toExpr(&quot;attr= 'amplifier'&quot;).run()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However when I try to run against all address IDs it only returns those that have only 1 vertex inbetween the start and end.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val dev_paths1: DataFrame = dev_gf.bfs.fromExpr(&quot;attr = 'address'&quot;).toExpr(&quot;attr='amplifier'&quot;).run()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm not sure what I am doing wrong. I have tried adding the maxPathLength() but that didn't change anything. And the default is supposed to be 10, and I am only getting 3 to begin with. Does anyone know what I am doing wrong or a way I could get the nearest end(amp) id for each start(address) id? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit:I looked at the documentation and now realize that it will only return the edges with the smallest path once it finds out(so if the closest path is 3 for a single edge, it will only return paths that have 3). But I need to find the shortest path for each address to the nearest amplifier. Any ideas?&lt;/p&gt;&#xA;" OwnerUserId="1308667" LastEditorUserId="1308667" LastEditDate="2018-03-05T01:06:52.660" LastActivityDate="2018-03-05T01:06:52.660" Title="Spark graphframe BFS only returning subset of start vertices" Tags="&lt;apache-spark&gt;&lt;breadth-first-search&gt;&lt;graphframes&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49090507" PostTypeId="1" CreationDate="2018-03-03T23:54:20.447" Score="0" ViewCount="43" Body="&lt;p&gt;I have to compare two dataframes to find out the columns differences based on one or more key fields using pyspark in a most performance efficient approach since I have to deal with huge dataframes&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have already built a solution for comparing two dataframes using hash match without key field matching like &lt;code&gt;data_compare.df_subtract(self.df_db1_hash,self.df_db2_hash)&lt;/code&gt;&#xA;but scenario is different if I want to use key field match&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note: I have provided sample expected dataframe. Actual requirement is any differences from DataFrame 2 in any columns should be retrieved in output/expected dataframe.&lt;/em&gt;&lt;/strong&gt; &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;DataFrame 1:&#xA;&#xA;+------+---------+--------+----------+-------+--------+&#xA;|emp_id| emp_city|emp_name| emp_phone|emp_sal|emp_site|&#xA;+------+---------+--------+----------+-------+--------+&#xA;|     3|  Chennai|  rahman|9848022330|  45000|SanRamon|&#xA;|     1|Hyderabad|     ram|9848022338|  50000|      SF|&#xA;|     2|Hyderabad|   robin|9848022339|  40000|      LA|&#xA;|     4|  sanjose|   romin|9848022331|  45123|SanRamon|&#xA;+------+---------+--------+----------+-------+--------+&#xA;&#xA;DataFrame 2:&#xA;&#xA;+------+---------+--------+----------+-------+--------+&#xA;|emp_id| emp_city|emp_name| emp_phone|emp_sal|emp_site|&#xA;+------+---------+--------+----------+-------+--------+&#xA;|     3|  Chennai|  rahman|9848022330|  45000|SanRamon|&#xA;|     1|Hyderabad|     ram|9848022338|  50000|      SF|&#xA;|     2|Hyderabad|   robin|9848022339|  40000|      LA|&#xA;|     4| sandiego|  romino|9848022331|  45123|SanRamon|&#xA;+------+---------+--------+----------+-------+--------+&#xA;&#xA;Expected dataframe after comparing dataframe 1 and 2&#xA;&#xA;&#xA;+------+---------+--------+----------+&#xA;|emp_id| emp_city|emp_name| emp_phone|&#xA;+------+---------+--------+----------+&#xA;|     4| sandiego|  romino|9848022331|&#xA;+------+---------+--------+----------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="8742569" LastEditorUserId="8742569" LastEditDate="2018-03-05T13:01:19.283" LastActivityDate="2018-03-06T12:57:02.783" Title="pyspark dataframe comparison to find columns difference based on key fields" Tags="&lt;python-3.x&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;comparison&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="6" />
  <row Id="49090508" PostTypeId="1" CreationDate="2018-03-03T23:54:26.707" Score="-1" ViewCount="41" Body="&lt;p&gt;I have a RDD with &lt;code&gt;Any&lt;/code&gt; type, example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Array(List(Mathematical Sciences, ListBuffer(applications, asymptotic, largest, enable, stochastic)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to convert it to RDD of type &lt;code&gt;RDD[(String, Seq[String])]&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val rdd = sc.makeRDD(strList)&#xA;case class X(titleId: String, terms: List[String])&#xA;&#xA;val df = rdd.map { case Array(s0, s1) =&amp;gt; X(s0, s1) }.toDF()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I passed a long time to try without success&lt;/p&gt;&#xA;" OwnerUserId="9439941" LastEditorUserId="5344058" LastEditDate="2018-03-03T23:58:16.827" LastActivityDate="2018-03-05T22:28:12.537" Title="convert RDD Array[Any] = Array(List([String], ListBuffer([string])) to RDD(String, Seq[String])" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;rdd&gt;" AnswerCount="3" CommentCount="2" />
  <row Id="49091355" PostTypeId="1" CreationDate="2018-03-04T02:29:57.487" Score="0" ViewCount="20" Body="&lt;p&gt;I am new to PySpark and beg your pardon, if my question looks silly. I have tried finding it on the Forum and now asking as could not find it there. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a dataset in the following format, which needs to by dynamically aggregated for 6 months. I am pasting input as well as output datasets for your reference.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Input:&#xA;Customer_ID Product_ID  Txn_date    Net_Amt&#xA;  Cust1       prod1     13/2/2017     9.7&#xA;  Cust1       Prod2     24/7/2017     8.4&#xA;  Cust1       Prod1     24/7/2017     17.2&#xA;  Cust1       Prod 1    20/9/2017     8.5&#xA;  Cust1       Prod2     8/11/2017     6.23&#xA;  Cust1       Prod1     23/1/2018     2.25&#xA;  Cust1       Prod2     30/1/2018     9.19&#xA;  Cust2       Prod1     5/5/2017      10.45&#xA;  Cust2       Prod1     10/8/2017     15.2&#xA;  Cust2       Prod1     12/1/2018     5.7&#xA;  Cust2       Prod3     10/3/2017     9.4&#xA;&#xA;Output:&#xA;Customer_ID Product_ID  Txn_date    Amount Spent in buying same product in last 6 months    No of times same product is bought in last 6 months&#xA;  Cust1       Prod1     23/1/2018          27.95                                                     3&#xA;  Cust1       Prod1     20/9/2017          25.7                                                      2&#xA;  Cust1       Prod1     24/7/2017          26.9                                                      2&#xA;  Cust1       Prod      13/2/2017          9.7                                                       1&#xA;  Cust1       Prod2     30/1/2018          15.42                                                     2&#xA;  Cust1       Prod2     8/11/2017          14.63                                                     2&#xA;  Cust1       Prod2     24/72017           17.2                                                      1                                                    &#xA;  Cust2       Prod1     12/1/2018          20.9                                                      2                                                    &#xA;  Cust2       Prod1     10/8/2017          25.65                                                     2&#xA;  Cust2       Prod1     5/5/2017           10.45                                                     1&#xA;  Cust3       Prod3     10/3/2017          9.4                                                       1&#xA;&#xA;Output:&#xA;Customer_ID Product_ID  Txn_date    Amount Spent in buying same product in last 6 months&#xA;   Cust1       Prod1    23/1/2018        27.95&#xA;   Cust1       Prod1    20/9/2017        25.7&#xA;   Cust1       Prod1    24/7/2017        26.9&#xA;   Cust1       Prod     13/2/2017        9.7&#xA;   Cust1       Prod2    30/1/2018        15.42&#xA;   Cust1       Prod2    8/11/2017        14.63&#xA;   Cust1       Prod2    24/72017         17.2&#xA;   Cust2       Prod1    12/1/2018        20.9&#xA;   Cust2       Prod1    10/8/2017        25.65&#xA;   Cust2       Prod1    5/5/2017         10.45&#xA;   Cust3       Prod3    10/3/2017        9.4&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Hence, dataset may have more combination for same set of customer and product, but aggregation will happen only on the last six months of the data from the date of choice. &#xA;In the above example: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;First row in the output is the aggregation of NET_AMT summed up on date (23/1/2018, 20/9/2017 and 24/07/2017) for the combination cust1 and prod1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Similarly in the second row for same combination aggregation will happen for 20/9/2017 and 24/07/2017.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help will be much appreciated. Thanks in advance&lt;/p&gt;&#xA;" OwnerUserId="6621596" LastEditorUserId="7358392" LastEditDate="2018-03-05T12:56:31.150" LastActivityDate="2018-03-05T12:56:31.150" Title="Dynamic Aggregation from any date in last 6 months in pyspark" Tags="&lt;pyspark&gt;&lt;aggregation&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49092111" PostTypeId="1" AcceptedAnswerId="49096610" CreationDate="2018-03-04T04:55:01.620" Score="1" ViewCount="112" Body="&lt;p&gt;Stackoverflow!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I wonder if there is a fancy way in Spark 2.0 to solve the situation below.&#xA;The situation is like this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Dataset1 (TargetData) has this schema and has about 20 milion records.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;id (String)&lt;/li&gt;&#xA;&lt;li&gt;vector of embedding result (Array, 300 dim)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Dataset2 (DictionaryData) has this schema and has about 9,000 records.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;dict key (String)&lt;/li&gt;&#xA;&lt;li&gt;vector of embedding result (Array, 300 dim)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;For each vector of records in dataset 1, I want to find the dict key that will be the maximum when I compute cosine similarity it with dataset 2.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Initially, I tried cross-join dataset1 and dataset2 and calculate cosine simliarity of all records, but the amount of data is too large to be available in my environment.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have not tried it yet, but I thought of collecting dataset2 as a list and then applying udf.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are there any other method in this situation?&#xA;Thanks,&lt;/p&gt;&#xA;" OwnerUserId="2408419" LastActivityDate="2018-03-04T14:47:10.910" Title="Are there alternative solution without cross-join in Spark 2?" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;user-defined-functions&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49092417" PostTypeId="1" CreationDate="2018-03-04T05:48:58.843" Score="1" ViewCount="42" Body="&lt;p&gt;I'm trying to understand lazy evaluation in Apache spark.&#xA;My understanding says:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Lets say am having Text file in hardrive.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Steps:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) First I'll create RDD1, that is nothing but a data definition right now.(No data loaded into memory right now)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;2) I apply some transformation logic on RDD1 and creates RDD2, still here RDD2 is data definition (Still no data loaded into memory)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;3) Then I apply filter on RDD2 and creates RDD3 (Still no data loaded into memory and RDD3 is also an data definition)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;4) I perform an action so that I could get RDD3 output in text file. So the moment I perform this action where am expecting output something from memory, then spark loads data into memory creates RDD1, 2 and 3 and produce output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So laziness of RDDs in spark says just keep making the roadmap(RDDs) until they dont get the approval to make it or produce it live.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is my understanding correct upto here...?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My second question here is, its said that its(Lazy Evaluation) one of the reason that the spark is powerful than Hadoop, May I know please how because am not much aware of Hadoop ? What happens in hadoop in this scenario ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks :) &lt;/p&gt;&#xA;" OwnerUserId="9392085" LastActivityDate="2018-03-04T06:24:18.510" Title="lazy evaluation in Apache Spark" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49092854" PostTypeId="1" CreationDate="2018-03-04T07:02:04.597" Score="0" ViewCount="13" Body="&lt;p&gt;Just started working on twitter spark streaming on eclipse IDE. I updated my &lt;strong&gt;pom.xml&lt;/strong&gt; file:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; &#xA;    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&#xA;    xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 &#xA;    http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&amp;gt;&#xA;    &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;please.help&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;please&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;0.0.1-SNAPSHOT&amp;lt;/version&amp;gt;&#xA;&#xA;    &amp;lt;dependencies&amp;gt;&#xA;        &amp;lt;dependency&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;spark-core_2.10&amp;lt;/artifactId&amp;gt;&#xA;            &amp;lt;version&amp;gt;2.2.1&amp;lt;/version&amp;gt;&#xA;        &amp;lt;/dependency&amp;gt;&#xA;        &amp;lt;dependency&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;spark-streaming_2.10&amp;lt;/artifactId&amp;gt;&#xA;            &amp;lt;version&amp;gt;2.2.1&amp;lt;/version&amp;gt;&#xA;        &amp;lt;/dependency&amp;gt;&#xA;        &amp;lt;dependency&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;spark-streaming-twitter_2.10&amp;lt;/artifactId&amp;gt;&#xA;            &amp;lt;version&amp;gt;1.6.1&amp;lt;/version&amp;gt;&#xA;        &amp;lt;/dependency&amp;gt;&#xA;        &amp;lt;dependency&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;spark-examples_2.10&amp;lt;/artifactId&amp;gt;&#xA;            &amp;lt;version&amp;gt;1.0.2&amp;lt;/version&amp;gt;&#xA;        &amp;lt;/dependency&amp;gt;&#xA;    &amp;lt;/dependencies&amp;gt;&#xA;&#xA;    &amp;lt;properties&amp;gt;&#xA;        &amp;lt;maven.compiler.source&amp;gt;1.8&amp;lt;/maven.compiler.source&amp;gt;&#xA;        &amp;lt;maven.compiler.target&amp;gt;1.8&amp;lt;/maven.compiler.target&amp;gt;&#xA;    &amp;lt;/properties&amp;gt;&#xA;&amp;lt;/project&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And I created a java class named &lt;strong&gt;TweetStream.java&lt;/strong&gt; with the following codes:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;public static void main(String[] args) {&#xA;    final String consumerKey = &quot;****&quot;;&#xA;    final String consumerSecret = &quot;****&quot;;&#xA;    final String accessToken = &quot;****&quot;;&#xA;    final String accessTokenSecret = &quot;****&quot;;&#xA;&#xA;    SparkConf conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;SparkTwitterHelloWorldExample&quot;);&#xA;    JavaStreamingContext jssc = new JavaStreamingContext(conf, new Duration(30000));&#xA;&#xA;    System.setProperty(&quot;twitter4j.oauth.consumerKey&quot;, consumerKey);&#xA;    System.setProperty(&quot;twitter4j.oauth.consumerSecret&quot;, consumerSecret);&#xA;    System.setProperty(&quot;twitter4j.oauth.accessToken&quot;, accessToken);&#xA;    System.setProperty(&quot;twitter4j.oauth.accessTokenSecret&quot;, accessTokenSecret);&#xA;&#xA;    JavaReceiverInputDStream&amp;lt;Status&amp;gt; twitterStream = TwitterUtils.createStream(jssc);&#xA;&#xA;    // Without filter: Output text of all tweets&#xA;    JavaDStream&amp;lt;String&amp;gt; statuses = twitterStream.map(&#xA;            new Function&amp;lt;Status, String&amp;gt;() {&#xA;                public String call(Status status) { return status.getText(); }&#xA;            }&#xA;    );&#xA;    statuses.print();&#xA;    jssc.start();&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Follwoing that, I did &lt;strong&gt;Run as &gt; Maven install&lt;/strong&gt; which built successfully. Then &lt;strong&gt;Run as &gt; Run as Java application&lt;/strong&gt; which gave the error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties&#xA;18/03/04 10:39:02 INFO SparkContext: Running Spark version 2.2.1&#xA;18/03/04 10:39:02 WARN SparkContext: Support for Scala 2.10 is deprecated as of Spark 2.1.0&#xA;18/03/04 10:39:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&#xA;18/03/04 10:39:03 INFO SparkContext: Submitted application: SparkTwitterHelloWorldExample&#xA;18/03/04 10:39:03 INFO SecurityManager: Changing view acls to: Xian&#xA;18/03/04 10:39:03 INFO SecurityManager: Changing modify acls to: Xian&#xA;18/03/04 10:39:03 INFO SecurityManager: Changing view acls groups to: &#xA;18/03/04 10:39:03 INFO SecurityManager: Changing modify acls groups to: &#xA;18/03/04 10:39:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Xian); groups with view permissions: Set(); users  with modify permissions: Set(Xian); groups with modify permissions: Set()&#xA;18/03/04 10:39:04 INFO Utils: Successfully started service 'sparkDriver' on port 60829.&#xA;18/03/04 10:39:04 INFO SparkEnv: Registering MapOutputTracker&#xA;18/03/04 10:39:04 INFO SparkEnv: Registering BlockManagerMaster&#xA;18/03/04 10:39:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information&#xA;18/03/04 10:39:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up&#xA;18/03/04 10:39:04 INFO DiskBlockManager: Created local directory at C:\Users\User\AppData\Local\Temp\blockmgr-72db40d5-23de-4b88-98c1-30114f48828c&#xA;18/03/04 10:39:04 INFO MemoryStore: MemoryStore started with capacity 901.8 MB&#xA;18/03/04 10:39:04 INFO SparkEnv: Registering OutputCommitCoordinator&#xA;18/03/04 10:39:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.&#xA;18/03/04 10:39:04 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.100.4:4040&#xA;18/03/04 10:39:04 INFO Executor: Starting executor ID driver on host localhost&#xA;18/03/04 10:39:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60842.&#xA;18/03/04 10:39:04 INFO NettyBlockTransferService: Server created on 192.168.100.4:60842&#xA;18/03/04 10:39:04 INFO BlockManager: Using     org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy&#xA;18/03/04 10:39:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.100.4, 60842, None)&#xA;18/03/04 10:39:04 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.100.4:60842 with 901.8 MB RAM, BlockManagerId(driver, 192.168.100.4, 60842, None)&#xA;18/03/04 10:39:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.100.4, 60842, None)&#xA;18/03/04 10:39:04 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.100.4, 60842, None)&#xA;Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/spark/Logging&#xA;        at java.lang.ClassLoader.defineClass1(Native Method)&#xA;    at java.lang.ClassLoader.defineClass(ClassLoader.java:763)&#xA;    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)&#xA;    at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)&#xA;    at java.net.URLClassLoader.access$100(URLClassLoader.java:73)&#xA;    at java.net.URLClassLoader$1.run(URLClassLoader.java:368)&#xA;    at java.net.URLClassLoader$1.run(URLClassLoader.java:362)&#xA;    at java.security.AccessController.doPrivileged(Native Method)&#xA;    at java.net.URLClassLoader.findClass(URLClassLoader.java:361)&#xA;    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&#xA;    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)&#xA;    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&#xA;    at org.apache.spark.streaming.twitter.TwitterUtils$.createStream(TwitterUtils.scala:44)&#xA;    at org.apache.spark.streaming.twitter.TwitterUtils$.createStream(TwitterUtils.scala:56)&#xA;    at org.apache.spark.streaming.twitter.TwitterUtils.createStream(TwitterUtils.scala)&#xA;    at please.TweetStream.main(TweetStream.java:27)&#xA;Caused by: java.lang.ClassNotFoundException: org.apache.spark.Logging&#xA;    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)&#xA;    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&#xA;    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)&#xA;    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&#xA;    ... 16 more&#xA;18/03/04 10:39:05 INFO SparkContext: Invoking stop() from shutdown hook&#xA;18/03/04 10:39:05 INFO SparkUI: Stopped Spark web UI at http://192.168.100.4:4040&#xA;18/03/04 10:39:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!&#xA;18/03/04 10:39:05 INFO MemoryStore: MemoryStore cleared&#xA;18/03/04 10:39:05 INFO BlockManager: BlockManager stopped&#xA;18/03/04 10:39:05 INFO BlockManagerMaster: BlockManagerMaster stopped&#xA;18/03/04 10:39:05 INFO     OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!&#xA;18/03/04 10:39:05 INFO SparkContext: Successfully stopped SparkContext&#xA;18/03/04 10:39:05 INFO ShutdownHookManager: Shutdown hook called&#xA;18/03/04 10:39:05 INFO ShutdownHookManager: Deleting directory C:\Users\User\AppData\Local\Temp\spark-b93e2698-f4fb-4d1a-b690-6f1128f4759c&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The application started successfully, but a &lt;strong&gt;logging problem&lt;/strong&gt; occurred and the application just skipped the codes and closed the connection.&lt;/p&gt;&#xA;" OwnerUserId="9440664" LastActivityDate="2018-03-04T07:02:04.597" Title="Twitter Spark Streaming using Apache Spark 2.2.1 - Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/spark/Logging" Tags="&lt;java&gt;&lt;maven&gt;&lt;apache-spark&gt;&lt;twitter&gt;&lt;spark-streaming&gt;" AnswerCount="0" CommentCount="2" ClosedDate="2018-03-04T07:18:10.357" />
  <row Id="49092979" PostTypeId="1" AcceptedAnswerId="49093185" CreationDate="2018-03-04T07:19:35.133" Score="0" ViewCount="32" Body="&lt;p&gt;I have a file which is file1snappy.parquet. It is having a complex data structure like a map, array inside that.After processing that I got final result.while writing that results to csv I am getting some error saying&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&quot;Exception in thread &quot;main&quot; java.lang.UnsupportedOperationException: CSV data source does not support map&amp;lt;string,bigint&amp;gt; data type.&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Code which I have used:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val conf=new SparkConf().setAppName(&quot;student-example&quot;).setMaster(&quot;local&quot;)&#xA;    val sc = new SparkContext(conf)&#xA;    val sqlcontext = new org.apache.spark.sql.SQLContext(sc)&#xA;    val datadf = sqlcontext.read.parquet(&quot;C:\\file1.snappy.parquet&quot;)&#xA;    def sumaggr=udf((aggr: Map[String, collection.mutable.WrappedArray[Long]]) =&amp;gt; if (aggr.keySet.contains(&quot;aggr&quot;)) aggr(&quot;aggr&quot;).sum else 0)&#xA;datadf.select(col(&quot;neid&quot;),sumaggr(col(&quot;marks&quot;)).as(&quot;sum&quot;)).filter(col(&quot;sum&quot;) =!= 0).show(false)&#xA;    datadf.write.format(&quot;com.databricks.spark.csv&quot;).option(&quot;header&quot;, &quot;true&quot;).save(&quot;C:\\myfile.csv&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I tried converting datadf.toString() but still I am facing same issue.&#xA;How can write that result to CSV.&lt;/p&gt;&#xA;" OwnerUserId="7255439" LastEditorUserId="6551426" LastEditDate="2018-03-04T10:43:15.447" LastActivityDate="2018-03-04T10:43:15.447" Title="Write dataframe to csv with datatype map&lt;string,bigint&gt; in Spark" Tags="&lt;apache-spark&gt;&lt;spark-dataframe&gt;&lt;rdd&gt;" AnswerCount="2" CommentCount="3" />
  <row Id="49093112" PostTypeId="1" CreationDate="2018-03-04T07:38:24.893" Score="0" ViewCount="12" Body="&lt;p&gt;I have downloaded some data from the Mainframe (.DATA format) and I need to parse it to create a PySpark DataFrame and perform some operations on it. Before doing that, I created a sample file and read it using read_fwf() feature of Pandas. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I was able to read and create the DataFrame but I encountered some problems like&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Padding of &quot;0&quot; in the first column of some of the Rows &lt;/li&gt;&#xA;&lt;li&gt;Repeating Headers while reading the Data&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;These were some of the issues I can handle, however the key challenge I am facing is in identifying the widths of the columns. I currently have 65 columns but in order to create a PySpark DataFrame, I would require to know the widths of these columns. Can read_fwf() tell what is the widths it is using for each column ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And is there a read_fwf() like function in PySpark ? Or we would have to write a MapRed code for it ?&lt;/p&gt;&#xA;" OwnerUserId="7891945" LastActivityDate="2018-03-04T07:38:24.893" Title="How to find widths of a Flat File using read_fwf() in Pandas?" Tags="&lt;pyspark&gt;&lt;flat-file&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49093140" PostTypeId="1" CreationDate="2018-03-04T07:42:30.763" Score="1" ViewCount="23" Body="&lt;p&gt;I have a separate rdd that needs to be refreshed periodically in my spark streaming job. There has been some solutions proposed (i.e. &lt;a href=&quot;https://stackoverflow.com/questions/37638519/spark-streaming-how-to-periodically-refresh-cached-rdd&quot;&gt;Spark Streaming: How to periodically refresh cached RDD?&lt;/a&gt;), however they all involve reconstructing the rdd in a stream.foreachRDD() loop. This will create a spark context not serializable error for me as I need to create my rdd with an sc.parallelize() method&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In other words, I can't do sc.parallelize() inside a foreachRDD() loop. Any advice on alternative ways to approach this? Thanks!&lt;/p&gt;&#xA;" OwnerUserId="9395367" LastActivityDate="2018-03-08T03:36:05.513" Title="Load External RDD Periodically during Spark Streaming" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;spark-streaming&gt;&lt;rdd&gt;" AnswerCount="0" CommentCount="0" ClosedDate="2018-03-10T15:45:07.037" />
  <row Id="49095354" PostTypeId="1" AcceptedAnswerId="49098572" CreationDate="2018-03-04T12:23:59.203" Score="0" ViewCount="54" Body="&lt;p&gt;I have three columns in df&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Col1,col2,col3&#xA;&#xA;X,x1,x2&#xA;&#xA;Z,z1,z2&#xA;&#xA;Y,&#xA;&#xA;X,x3,x4&#xA;&#xA;P,p1,p2&#xA;&#xA;Q,q1,q2&#xA;&#xA;Y&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to do the following&#xA;when col1=x,store the value of col2 and col3&#xA;and assign those column values  to next row when col1=y&#xA;expected output &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;X,x1,x2&#xA;&#xA;Z,z1,z2&#xA;&#xA;Y,x1,x2&#xA;&#xA;X,x3,x4&#xA;&#xA;P,p1,p2&#xA;&#xA;Q,q1,q2&#xA;&#xA;Y,x3,x4&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any help would be appreciated&#xA;Note:-spark 1.6&lt;/p&gt;&#xA;" OwnerUserId="5362193" LastEditorUserId="5362193" LastEditDate="2018-03-04T18:56:44.880" LastActivityDate="2018-03-04T20:46:42.483" Title="How to update column of spark dataframe based on the values of previous record" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;spark-dataframe&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="49095525" PostTypeId="1" CreationDate="2018-03-04T12:44:21.783" Score="0" ViewCount="33" Body="&lt;p&gt;I want to source few hundreds of gigabytes from a database via JDBC and then process it using Spark SQL. Currently I am doing some partitioning at that data and process is by batches of milion records. The thing is that I would like also to apply some deduplication to my dataframes and I was going to leave that idea of separated batches processing and try to process those hundreds of gigabytes using a one dataframe partitioned accordingly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The main concern is: how will .distinct() work in such case? Will Spark SQL firstly try to load ALL the data into the RAM and then apply deduplication involving many shuffles and repartitioning? Do I have to ensure that a cluster has enough of RAM to contain that raw data or maybe it will be able to help itself with HDD storage (thus killing the performance)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or maybe I should do it without Spark - move the data to the target storage and there apply distinct counts and detect duplicates and get rid off them?&lt;/p&gt;&#xA;" OwnerUserId="2576267" LastActivityDate="2018-03-04T17:58:20.063" Title="Spark SQL .distinct() performance" Tags="&lt;sql&gt;&lt;apache-spark&gt;&lt;duplicates&gt;&lt;distinct&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49095587" PostTypeId="1" CreationDate="2018-03-04T12:51:04.700" Score="0" ViewCount="14" Body="&lt;p&gt;Is there Spark MLlib example to scan json or text document(s) to extract keywords and numerical data based on specific pattern(s)?&lt;/p&gt;&#xA;" OwnerUserId="9441528" LastActivityDate="2018-03-04T12:51:04.700" Title="Is there an example of Spark MLlib to scan json or documents for keywords/data with of a specific pattern(s)?" Tags="&lt;scala&gt;&lt;spark-dataframe&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49095641" PostTypeId="1" AcceptedAnswerId="49096210" CreationDate="2018-03-04T12:56:45.713" Score="0" ViewCount="21" Body="&lt;p&gt;I have a log file which has lines as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Caused by: hudson.plugins.git.GitException: &#xA;...&#xA;...&#xA;ERROR: Error Repo issue&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I would like to get the value '&lt;em&gt;Error Repo issue&lt;/em&gt;' value from the below log.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val failures=logs.filter(value=&amp;gt;value.contains(&quot;ERROR&quot;))&#xA;val filteredValues=failures.map(value=&amp;gt;value.split(&quot;: &quot;))&#xA;for(i&amp;lt;-filteredValues){&#xA;      println(i(1))&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I tried to loop the &lt;em&gt;filteredValues&lt;/em&gt;,using the &lt;code&gt;i(1)&lt;/code&gt; and got the values.&#xA;Is there a better way to do it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help would be appreciated. &lt;/p&gt;&#xA;" OwnerUserId="5279133" LastEditorUserId="6551426" LastEditDate="2018-03-06T08:46:05.217" LastActivityDate="2018-03-06T08:46:05.217" Title="How to split String in RDD and retrieve it" Tags="&lt;apache-spark&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="49095685" PostTypeId="1" CreationDate="2018-03-04T13:02:32.833" Score="1" ViewCount="37" Body="&lt;p&gt;I am new to spark programming and scala and I am not able to understand the difference between map and flatMap. While using flatMap, why is &quot;Option&quot; used in method is working fine &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def parseNames(line: String) : Option[(Int,String)]  = {&#xA;  var fields = line.split('\&quot;')&#xA;  if (fields.length &amp;gt;1) {&#xA;    return Some(fields(0).trim().toInt,fields(1) )&#xA;  }&#xA;  else {&#xA;    return None&#xA;  }&#xA;}&#xA;def main(args: Array[String]) {&#xA;  val sc = new SparkContext(&quot;local[*]&quot;,&quot;DemoHero&quot;)&#xA;  val txt= sc.textFile(&quot;../marvel-names1.txt&quot;)  &#xA;  val rdd = txt.flatMap(parseNames)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;but without &quot;Option&quot;, an error is coming:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def parseNames(line: String) : (Int, String)  = {&#xA;  var fields = line.split('\&quot;')    &#xA;  (fields(0).trim().toInt,fields(1) )&#xA;}&#xA;&#xA;def main(args: Array[String]) {&#xA;  val sc = new SparkContext(&quot;local[*]&quot;,&quot;DemoHero&quot;)   &#xA;  val txt= sc.textFile(&quot;../marvel-names1.txt&quot;)  &#xA;  val rdd = txt.flatMap(parseNames)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As per my understanding, flatmap make Rdd in to collection for String/Int Rdd. I was thinking that in this case both should work without any error. Please let me know where I am making the mistake.&lt;/p&gt;&#xA;" OwnerUserId="9441568" LastEditorUserId="312172" LastEditDate="2018-03-04T13:28:22.533" LastActivityDate="2018-03-04T20:13:00.730" Title="Spark --Error :type mismatch; found : (Int, String) required: TraversableOnce[?]" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="3" CommentCount="1" />
  <row Id="49095905" PostTypeId="1" CreationDate="2018-03-04T13:26:02.277" Score="0" ViewCount="39" Body="&lt;p&gt;In spark / scala, how to do data driven instantiation of the case classes?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Explanation:&#xA;Let's say we have an input dataset of some kind of contracts (e.g. telecom subscriptions) and those contracts need to be somehow evaluated. Input dataset contains values like date of creation, start of the validity of contract, end of validity, some amounts, additional options, family discount etc. which all don't have to be filled (e.g. some contracts don't have additional options)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does it make sense to model all type of contracts using case classes? So, one input row coming from the dataset could be a contract for fixed line, or mobile number or some other service. Then i'd try to deduct the most details the input row has and instantiate appropriate case class using match? Each of these case classes would have a functions that returns a value of the contract based on this data and some static data coming from elsewhere (a lookup table, maybe k,v map). This function would then be used in a call to dataset 'map'. Better way to do this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given that the case classes idea makes sense, each class could also do simulations on the same input data. E.g. what if customer downgrades his internet speed, what would then be estimated income for this contract? So for one input row, i'd have to return 2 new columns: value of the contract and simulated value of the contract. Doing 'what if' scenarios, it could also be that for one input row i do several scenarios (at once?) which would than return several rows (e.g. 1. what if the customer buys something more; 2. what if customer downgrades; 3. what if customer cancels all additional options on the contract). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this even the right approach to problem? How to make these evaluations 'data driven' since input values drive which case class it is and configuration/run options drive how many times a 'map' on the dataset should be triggered?&lt;/p&gt;&#xA;" OwnerUserId="2035749" LastActivityDate="2018-03-04T13:26:02.277" Title="scala/spark different case class based on input data" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49095971" PostTypeId="1" CreationDate="2018-03-04T13:32:23.233" Score="0" ViewCount="28" Body="&lt;p&gt;I have data in following format :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[(('a','s1'),1),(('c','s5'),1),(('e','s2'),1),(('a','s1'),1),(('e','s3'),1)]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now, I would need to count the occurrences of the key-value pairs and then partition my output based on key and sort by value within each partition. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is what I did so far for counting the occurrences : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;filterRDD = fileRDD.map(lambda line: line.split(&quot;\t&quot;))&#xA;           .filter(lambda cols: len(cols) == columns)&#xA;           .map(lambda tuple: ((tuple[0],tuple[1]),1))&#xA;           .reduceByKey(lambda p, c: p + c)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Output : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[((u'a', u's1'), 2), ((u'c', u's5'), 1), ((u'e', u's2'), 1),(('e','s3'),1)]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;To sort by value in the key-value pair and partitionby the key, I am looking for an efficient solution. I did take a look at this - &lt;a href=&quot;https://stackoverflow.com/questions/36376369/what-is-the-most-efficient-way-to-do-a-sorted-reduce-in-pyspark&quot;&gt;What is the most efficient way to do a sorted reduce in PySpark?&lt;/a&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;and tried : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;filterRDD = fileRDD.map(lambda line: line.split(&quot;\t&quot;))&#xA;               .filter(lambda cols: len(cols) == columns)&#xA;               .map(lambda tuple: ((tuple[0],tuple[1]),1))&#xA;               .reduceByKey(lambda p, c: p + c)&#xA;               .map(lambda tuple: (tuple[0], sorted(tuple[1], &#xA;               key=lambda x: (x[1],x[2])))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;, but it fails with :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;File &quot;&amp;lt;stdin&amp;gt;&quot;, line 1, in &amp;lt;lambda&amp;gt;&#xA;TypeError: 'int' object is not iterable&#xA;&#xA;    at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)&#xA;    at org.apache.spark.api.python.PythonRunner$$anon$1.&amp;lt;init&amp;gt;(PythonRDD.scala:234)&#xA;    at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)&#xA;    at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)&#xA;    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)&#xA;    at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Am I missing something obvious here? And is this the best way to achieve the sortBy value and partitionBy on fields using RDDs? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My output needs to look something like if it helps for clarity : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;file1: &#xA;    'a','s1',2&#xA;&#xA;file2:   &#xA;    'c','s5',1&#xA;&#xA;file3:    &#xA;    'e','s2',1&#xA;    'e','s3',1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;EDIT : Here is what I did by which I helped me move further -&lt;/em&gt;&lt;/strong&gt; &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;filterRDD = fileRDD.map(lambda line: line.split(&quot;\t&quot;))&#xA;                   .filter(lambda cols: len(cols) == columns)&#xA;                   .map(lambda tuple: ((tuple[0],tuple[1]),1))&#xA;                   .reduceByKey(lambda p, c: p + c)&#xA;&#xA;aggregatedDF = filterRDD.map(lambda fields: (fields[0][0],fields[0][1],fields[1])).toDF(['col1','col1','col3'])  &#xA;orderedDF = aggregatedDF.orderBy(&quot;col1&quot;,&quot;col2&quot;)&#xA;orderedDF.show()&#xA;+-------+--------+----------------+&#xA;|col1   |col2    |col3            |&#xA;+-------+--------+----------------+&#xA;|      a|      s1|               2|&#xA;|      c|      s5|               1|&#xA;|      e|      s2|               1|&#xA;|      e|      s3|               1|&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is this the optimal solution? Also now I am using following to write output on col1 partition :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;orderedDF.write.partitionBy(&quot;col1&quot;).format(&quot;parquet&quot;).save(&quot;/spark/output&quot;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But this operation seem to be writing to files :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;part-00000-somealphanumeric.snappy.parquet -&amp;gt; This contains intermediate data?&#xA;part-00001-somealphanumeric.snappy.parquet -&amp;gt; This contains my final output&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What is this intermediate file? How can I remove it?&lt;/p&gt;&#xA;" OwnerUserId="2305003" LastEditorUserId="2305003" LastEditDate="2018-03-05T04:00:32.027" LastActivityDate="2018-03-05T04:00:32.027" Title="pyspark partitionBy one column and sortBy one column in efficient way?" Tags="&lt;sorting&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;rdd&gt;&lt;partition&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49096436" PostTypeId="1" CreationDate="2018-03-04T14:24:23.857" Score="0" ViewCount="40" Body="&lt;p&gt;I have a near real time spark streaming application for image recognition where receiver gets the input frames from kafka. I have 6 receivers per executor, 5 executors in total, I can see 30 active tasks per iteration on Spark UI.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My problem is spark able to read 850 frames/sec from kafka but processes task very slowly, which is why i am facing backpressure related issues. Within each batch, the task is expected to run few tensorflow models by first loading them using keras.model_loads and then performs other related processing in order to get the prediction from the model. The output of 1st tensorflow model is the input to 2nd tensorflow model which in turns also load another model and perform prediction on top of it. Now finally output of #2 is the input to model #3 which do the same thing, load the model and perform prediction. The final prediction is send back to kafka to another topic. Having this process flow for each task, overall latency to process a single task is coming somewhere between 10 to 15 seconds which is huge for a spark streaming application &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone help me, how can I make this program fast? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Remember I have to use these custom tensorflow models in my program to get the final output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have the following thoughts in my mind:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Option 1 - Replace spark streaming with structured streaming&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Option 2 - Break sequential processing and put each sub process in separate RDD i.e. model #1 processing in RDD1, model #2 processing in RDD2 and so on&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Option 3 - Rewrite custom tensorflow functionality in spark only, currently that is a single python program which I am using with each task. However I am not so sure about this option yet and not even check the feasibility so far. But what I am assuming that if I am able to do that i will have full control over the distribution of models. Therefore may get the fast processing of these task on GPU machines on AWS cluster which is not happening currently.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="9441744" LastEditorUserId="1765189" LastEditDate="2018-03-04T15:01:33.320" LastActivityDate="2018-03-04T17:34:32.883" Title="Huge latency in spark streaming job" Tags="&lt;apache-spark&gt;&lt;tensorflow&gt;&lt;pyspark&gt;&lt;apache-kafka&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49096683" PostTypeId="1" CreationDate="2018-03-04T14:56:16.637" Score="0" ViewCount="20" Body="&lt;p&gt;I have the following csv file.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Index,Arrival_Time,Creation_Time,x,y,z,User,Model,Device,gt&#xA;0,1424696633908,1424696631913248572,-5.958191,0.6880646,8.135345,a,nexus4,nexus4_1,stand&#xA;1,1424696633909,1424696631918283972,-5.95224,0.6702118,8.136536,a,nexus4,nexus4_1,stand&#xA;2,1424696633918,1424696631923288855,-5.9950867,0.6535491999999999,8.204376,a,nexus4,nexus4_1,stand&#xA;3,1424696633919,1424696631928385290,-5.9427185,0.6761626999999999,8.128204,a,nexus4,nexus4_1,stand&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to read it and generate a RDD with the following structure.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;User,Model,gt,media(x,y,z),desviacion(x,y,z),max(x,y,z),min(x,y,z)&#xA;a, nexus4,stand,-3.0,0.7,8.2,2.8,0.14,0.0,-1.0,0.8,8.2,-5.0,0.6,8.2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Where user model and gt columns are my primary key columns.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Do you know any documentation where can I generate it correctly, because I'm not finding any information&lt;/p&gt;&#xA;" OwnerUserId="6297869" LastActivityDate="2018-03-04T14:56:16.637" Title="Creating primary key from CSV - pyspark" Tags="&lt;pyspark&gt;&lt;rdd&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49097321" PostTypeId="1" CreationDate="2018-03-04T16:01:45.167" Score="0" ViewCount="15" Body="&lt;p&gt;spark-shell on yarn(client mode) uses the yarn configurations specified in the $SPARK_HOME/conf/spark-defaults.conf (creates 5 executors)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark.master                            yarn&#xA;spark.driver.memory                     3g&#xA;spark.executor.cores                    1&#xA;spark.executor.memory                   3g&#xA;spark.executor.instances                5&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;but spark-submit on yarn (client mode) does not (spark-submit simply creates to 2 executors)&lt;/p&gt;&#xA;" OwnerUserId="4958502" LastActivityDate="2018-03-04T21:10:05.240" Title="spark-shell works with appropriate yarn resource settings but not spark-submit" Tags="&lt;apache-spark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49098055" PostTypeId="1" CreationDate="2018-03-04T17:12:03.270" Score="-1" ViewCount="18" Body="&lt;p&gt;I have built an Apache Spark's ALS model in Scala that takes an Int value as an input and predicts customer ratings. However, i don't know how to pass an input value from a PHP script and then get the predicted results back into PHP so it can be displayed to the user. Anyone can help me, direct me or suggest any helpful resources.&lt;/p&gt;&#xA;" OwnerUserId="2975923" LastActivityDate="2018-03-04T17:12:03.270" Title="How to pass an argument to Apache Spark's ALS model from a PHP script" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;collaborative-filtering&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49098064" PostTypeId="1" AcceptedAnswerId="49103246" CreationDate="2018-03-04T17:13:12.020" Score="0" ViewCount="39" Body="&lt;p&gt;My dataset schema looks like this&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;root&#xA; |-- col1: string (nullable = false)&#xA; |-- col2: string (nullable = false)&#xA; |-- col3: timestamp (nullable = false)&#xA; |-- col4: map (nullable = false)&#xA; |    |-- key: string&#xA; |    |-- value: map (valueContainsNull = true)&#xA; |    |    |-- key: integer&#xA; |    |    |-- value: long (valueContainsNull = true)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;col4&lt;/code&gt; is a map type, however, inside it, there's another map.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can use &lt;code&gt;explode(col4)&lt;/code&gt; to get &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;root&#xA; |-- col1: string (nullable = false)&#xA; |-- col2: string (nullable = false)&#xA; |-- col3: timestamp (nullable = false)&#xA; |-- key: string (nullable = false)&#xA; |-- value: map (nullable = true)&#xA; |    |-- key: integer&#xA; |    |-- value: long (valueContainsNull = true)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How am I supposed to explode the &lt;code&gt;value&lt;/code&gt; column now?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A stupid way would be renaming the &lt;code&gt;key&lt;/code&gt; column, and explode the &lt;code&gt;value&lt;/code&gt; column again... but I'm trying to do it in a nice way.&lt;/p&gt;&#xA;" OwnerUserId="2429136" LastActivityDate="2018-03-05T03:53:08.950" Title="Explode map type column that has another map type inside in spark" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49098114" PostTypeId="1" CreationDate="2018-03-04T17:18:09.570" Score="0" ViewCount="54" Body="&lt;p&gt;I have the following row with two tuples,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[(('a', 'nexus4', 'stand'), ('-5.958191', '0.6880646', '8.135345'))]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The first part is a tuple which is formed with 'a' person index, 'nexus4' mobile model, and 'stand' what the person is doing , and the other tuple are the x,y,z coordinates from where the person is.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have to group all by the first tuple, and the calculate min, max and avg from the first tuple to all the data x,y,z corresponding to him.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is an example from the output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; User,Model,gt,media(x,y,z),desviacion(x,y,z),max(x,y,z),min(x,y,z)&#xA;&amp;gt; nexus4,stand,-3.0,0.7,8.2,2.8,0.14,0.0,-1.0,0.8,8.2,-5.0,0.6,8.2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any idea about how to access to each element grouped from my first tuple to calculate min , max, and avg from it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the code I have developed:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# Data loading&#xA;&#xA;    lectura = sc.textFile(&quot;Phones_accelerometer.csv&quot;)&#xA;&#xA;    datos = lectura.map(lambda x: ((x.split(&quot;,&quot;)[6], x.split(&quot;,&quot;)[7], x.split(&quot;,&quot;)[9]),(x.split(&quot;,&quot;)[3], x.split(&quot;,&quot;)[4], x.split(&quot;,&quot;)[5])))&#xA;&#xA;    sumCount = datos.combineByKey(lambda value: (value, 1), lambda x, value: (x[0] + value, x[1] + 1), lambda x, y: (x[0] + y[0], x[1] + y[1]))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="6297869" LastActivityDate="2018-03-04T17:18:09.570" Title="Operating with 2 tuples in pyspark - spark python" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="0" CommentCount="2" FavoriteCount="0" ClosedDate="2018-03-06T22:31:41.317" />
  <row Id="49098202" PostTypeId="1" AcceptedAnswerId="49101388" CreationDate="2018-03-04T17:28:53.767" Score="1" ViewCount="21" Body="&lt;p&gt;I am learning Scala now, and notice something that I don't understand why, I have a result to be generated registered Temp Table via sqlContext on a DataFrame derived from a RDD, the RDD is from a hdfs file exported from mysql.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Raw hdfs data can be retrieved from here (2MB):&#xA;&lt;a href=&quot;https://github.com/mdivk/175Scala/tree/master/data/part-00000&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/mdivk/175Scala/tree/master/data/part-00000&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the script used on mysql:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; select avg(order_item_subtotal) from order_items;&#xA;+--------------------------+&#xA;| avg(order_item_subtotal) |&#xA;+--------------------------+&#xA;|       199.32066922046081 |&#xA;+--------------------------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;On Scala:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;sqlContext:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; scala&amp;gt; val res = sqlContext.sql(&quot;select avg(order_item_subtotal) from&#xA;&amp;gt; order_items&quot;)&#xA;&#xA;&amp;gt; +------------------+ &#xA;  |               _c0|&#xA;  +------------------+ &#xA;  |199.32066922046081|&#xA;  +------------------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So they are the same, exactly same, which is expected;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;RDD (please use the data file from &lt;a href=&quot;https://github.com/mdivk/175Scala/tree/master/data/part-00000&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/mdivk/175Scala/tree/master/data/part-00000&lt;/a&gt;):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val orderItems = sc.textFile(&quot;/public/retail_db/order_items&quot;)&#xA;&#xA;val orderItemsRevenue = orderItems.map(oi =&amp;gt; oi.split(&quot;,&quot;)(4).toFloat)&#xA;&#xA;val totalRev = orderItemsRevenue.reduce((total, revenue) =&amp;gt; total + revenue)&#xA;res4: Float = 3.4326256E7&#xA;&#xA;val cnt = orderItemsRevenue.count &#xA;&#xA;val avgRev = totalRev/cnt&#xA;avgRev: Float = 199.34178&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As you can see, the avgRev is 199.34178, not what we calculated above in mysql and sqlContext 199.32066922046081&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I do not think this is an acceptable discrepancy but I could be wrong, am I missing anything here?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It would be appreciated if you can help me understand this. Thank you.&lt;/p&gt;&#xA;" OwnerUserId="9074574" LastEditorUserId="187261" LastEditDate="2018-03-04T18:14:49.537" LastActivityDate="2018-03-04T22:55:58.117" Title="Script result in sqlContext on registered temp table in Scala has minor difference than using Reduce in RDD" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49098252" PostTypeId="1" AcceptedAnswerId="49098272" CreationDate="2018-03-04T17:33:04.680" Score="0" ViewCount="9" Body="&lt;p&gt;I have a data frame in &lt;code&gt;pyspark&lt;/code&gt;. I want to select some columns from that &lt;code&gt;data frame&lt;/code&gt; and converts them to lowercase.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# selecting columns after the second column in data frame df as a list:&#xA;df1 = df.schema.names[2:]&#xA;&#xA;# convert the list to lowercase&#xA;test_list = [c.lower() for c in df1]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am able to achieve what I want but I would like to do this in a single step instead of two steps&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I do that?&lt;/p&gt;&#xA;" OwnerUserId="9367133" LastActivityDate="2018-03-04T17:34:09.080" Title="select and convert column names in pyspark data frame" Tags="&lt;pandas&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49098938" PostTypeId="1" CreationDate="2018-03-04T18:37:02.563" Score="-2" ViewCount="38" Body="&lt;p&gt;I have the following input files,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Input1.--&amp;gt; first.csv&#xA;Input2 --&amp;gt; second.csv&#xA;&#xA; val firstcsvData = spark.read.option(&quot;header&quot;, &quot;true&quot;).option(&quot;inferSchema&quot;, &quot;false&quot;).csv(&quot;first.csv&quot;)&#xA; val  secondcsvData = spark.read.option(&quot;header&quot;, &quot;true&quot;).option(&quot;inferSchema&quot;, &quot;false&quot;).csv(&quot;second.csv&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The below DF contains two check columns one is firstcheck and another is second check pin is additional column and i have key column.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;firstCheck and  secondCheck is 2 List values making a Data frame as shown below pin is additional column added to df no significance as of now just need to display.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val secondProviderInputcolumns = List(List(&quot;firstCheck&quot;),List(&quot;secondCheck&quot;))&#xA;&#xA;firstCheck  secondCheck     pin     key &#xA;    124          134        1001&#xA;    125          135        1002&#xA;    126          136        1003&#xA;    127          137        1004&#xA;    128          138        1005&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now i have another dataframe as below, having again 2 list of values which together making a data frame, here i will have the key column with values.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From the first list i have columns &quot;first&quot;,&quot;second&quot;,&quot;third&quot; &quot;Key&quot; now i need to match the data from previous data frame of &quot;firstCheck&quot; column with all the 3 columns(&quot;first&quot;,&quot;second&quot;,&quot;third&quot;) of second data frame. If any one column data is matched with firstCheck data then i need to update the key value from firstProviderInputcolumns as it has the key value in to secondProviderInputcolumns.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val firstProviderInputcolumns = List(List(&quot;first&quot;, &quot;second&quot;, &quot;third&quot;, &quot;key&quot;),List(&quot;fourth&quot;,&quot;key&quot;)) &#xA;&#xA;first   second  third   fourth  key&#xA;124     200     200     565     110901&#xA;181     201     201     201     110902&#xA;182     167     202     202     110903&#xA;183     203     203     203     110904&#xA;184     204     204     137     511090&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Expected output below,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;firstCheck  secondCheck  pin    key &#xA;124          134        1001    110901&#xA;125          135        1002&#xA;126          136        1003&#xA;127          137        1004    511090&#xA;128          138        1005&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;After updating the key we have to filter out those records which are got key value updated. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now the same process should repeat for secondCheck. secondCheck will be performed on the data which is filtered from non matching data previously.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Secondcheck data will be matched with fourth column if any data is matched in second pass then we have to update the key in firstdatafram again we have to filter the non matched values. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If we have 3rd list then we have to use the thirdcheck and need to filter out the result recursively. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val result  : org.apache.spark.sql.DataFrame  = myRecursiveMethod(firstProviderInputcolumns, secondProviderInputcolumns, firstcsvData,secondcsvData)&#xA; result.show()&#xA;def myRecursiveMethod(firstProviderInputcolumns: List[List[String]], secondProviderInputcolumns: List[List[String]], firstcsvData : org.apache.spark.sql.DataFrame,secondcsvData : org.apache.spark.sql.DataFrame): org.apache.spark.sql.DataFrame = {&#xA;val ongoingResult = doCalculation(firstProviderInputcolumns.head, secondProviderInputcolumns.head, firstcsvData, secondcsvData)&#xA;if(firstProviderInputcolumns.tail.nonEmpty &amp;amp;&amp;amp; secondProviderInputcolumns.tail.nonEmpty)&#xA;myRecursiveMethod(firstProviderInputcolumns.tail, secondProviderInputcolumns.tail, ongoingResult,secondcsvData)&#xA;else&#xA;ongoingResult&#xA;}&#xA;&#xA;import org.apache.spark.sql.functions._&#xA;def doCalculation(firstProviderInputcolumns : List[String],secondProviderInputcolumns : List[String],firstcsvData : org.apache.spark.sql.DataFrame,secondcsvData:  org.apache.spark.sql.DataFrame) : org.apache.spark.sql.DataFrame={&#xA;val firstColumn = firstProviderInputcolumns.map(name =&amp;gt; col(name))&#xA;val selectedPassOnePDF = firstcsvData.select(firstColumn:_*)&#xA;val secondColumn = secondProviderInputcolumns.map(name =&amp;gt; col(name))&#xA;val selectedPassTwoPDF = secondcsvData.select(secondColumn:_*)&#xA;val columnsToCheckForOne = selectedPassOnePDF.columns.toSet - &quot;key&quot; toList&#xA;val columnsToCheckForTwo = selectedPassTwoPDF.columns.toSet - &quot;key&quot; toList&#xA;val tempSelectedDetailsDF = selectedPassOnePDF.select(array(columnsToCheckForOne.map(col): _*).as(&quot;array&quot;), col(&quot;key&quot;).as(&quot;key2&quot;))&#xA;val tempSelectedSecondDetailsDF = selectedPassTwoPDF.select(array(columnsToCheckForTwo.map(col): _*).as(&quot;array2&quot;))&#xA;val arrayContains = udf((array: collection.mutable.WrappedArray[String], array2: collection.mutable.WrappedArray[String]) =&amp;gt; array.contains(array2))&#xA;val tempDF = tempSelectedSecondDetailsDF.join(tempSelectedDetailsDF, arrayContains($&quot;array&quot;, $&quot;array2&quot;), &quot;left&quot;).drop(&quot;array&quot;,&quot;key&quot;)&#xA;val ongoingResult = tempDF.withColumn(&quot;key&quot;, (col(&quot;key2&quot;))).drop(&quot;key2&quot;).na.fill(&quot;&quot;)&#xA;val tempProviderOne = tempDF.filter(col(&quot;key&quot;)!==&quot;&quot;)&#xA;return tempDF&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I had written the above code but i am not getting the expected output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please note second check will be performed on the data which is filtered in firstcheck.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please help.&lt;/p&gt;&#xA;" OwnerUserId="5405954" LastEditorUserId="5405954" LastEditDate="2018-03-05T04:46:22.523" LastActivityDate="2018-03-06T13:27:29.310" Title="How to match the data from two data frames provided with key and value in the list" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49100218" PostTypeId="1" AcceptedAnswerId="49101573" CreationDate="2018-03-04T20:40:06.040" Score="1" ViewCount="23" Body="&lt;p&gt;I am trying to drop two columns from a dataframe but I am facing an error as&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;**Error:**&#xA;drop() takes 2 positional arguments but 3 were given&#xA;&#xA;***Code:***&#xA; excl_columns= row['exclude_columns'].split(',')&#xA; df=df.drop(*excl_columns)&#xA;&#xA;#print(excl_columns)&#xA;#['year_of_birth', 'ethnicity']&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="8742569" LastEditorUserId="9209546" LastEditDate="2018-03-04T23:43:56.027" LastActivityDate="2018-03-04T23:43:56.027" Title="Pyspark dataframe drop columns issue" Tags="&lt;python&gt;&lt;python-3.x&gt;&lt;pyspark&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="49100919" PostTypeId="1" CreationDate="2018-03-04T21:56:48.660" Score="1" ViewCount="35" Body="&lt;p&gt;I have a spark job which reads, deduplicates and joins datasets stored in S3. The stored data is in ORC format and zlib compressed. In the first stage(the reading and deduplicating part), a small number of straggler tasks take up a great amount of time to complete. I analysed the metrics and found the following:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;The tasks are processing nearly the same amount of data.&lt;/li&gt;&#xA;&lt;li&gt;Shuffle writes for the tasks are nearly the same.&lt;/li&gt;&#xA;&lt;li&gt;GC duration for each task is negligible.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Please find some screenshots for reference. One of the screenshots shows the metrics. The other depicts the time taken (30 mins / 4.1 mins) for two tasks with barely any difference in shuffle writes(9.2 mb/10.3 mb) or data skew(6.4M/7.2M) and without any considerable GC lag(5s/1s).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am lost here, no idea what could be causing this to happen. Any help would be appreciated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Best Regards&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note: IPs have been removed from the fifth column in the second image.&#xA;&lt;a href=&quot;https://i.stack.imgur.com/5WhKP.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/5WhKP.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&#xA;&lt;a href=&quot;https://i.stack.imgur.com/QILAC.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/QILAC.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="2487397" LastEditorUserId="2487397" LastEditDate="2018-03-05T14:03:22.793" LastActivityDate="2018-03-05T14:03:22.793" Title="Apache Spark - Few straggler tasks slowing down a stage and the job" Tags="&lt;apache-spark&gt;&lt;amazon-s3&gt;&lt;orc&gt;" AnswerCount="0" CommentCount="0" FavoriteCount="1" />
  <row Id="49101046" PostTypeId="1" CreationDate="2018-03-04T22:13:12.197" Score="0" ViewCount="27" Body="&lt;p&gt;I am trying to learn to use Apache Spark in Java for machine learning.&#xA;As a start, I am trying to reproduce an example of Poisson (GLM) regression from this website &lt;a href=&quot;https://onlinecourses.science.psu.edu/stat504/node/169&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://onlinecourses.science.psu.edu/stat504/node/169&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the dependent variable is satellites and the sole independent variable is width.&lt;/li&gt;&#xA;&lt;li&gt;I would like to initialize a DataFrame programmatically (and not from a file) since this will be my use case down the line.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Here is the relevant code. The variables &lt;code&gt;satellites&lt;/code&gt; and &lt;code&gt;width&lt;/code&gt; are &lt;code&gt;List&amp;lt;Double&amp;gt;&lt;/code&gt; that were filled with the values from the file provided at the above website. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SparkSession spark = SparkSession&#xA;            .builder()&#xA;            .appName(&quot;example&quot;)&#xA;            .config(&quot;spark.master&quot;, &quot;local&quot;)&#xA;            .getOrCreate();&#xA;StructType schema = new StructType(new StructField[]{&#xA;            new StructField(&quot;satellites&quot;,&#xA;                    DataTypes.DoubleType, false,&#xA;                    Metadata.empty()),&#xA;            new StructField(&quot;width&quot;,&#xA;                    new VectorUDT(), false,&#xA;                    Metadata.empty())});&#xA; List&amp;lt;Row&amp;gt; rows = new ArrayList&amp;lt;&amp;gt;();&#xA; for (int i=0;i&amp;lt;satellites.size();++i) {&#xA;        double s = satellites.get(i);&#xA;        double w = width.get(i);&#xA;        rows.add(RowFactory.create(s,Vectors.dense(w)));&#xA; }&#xA; Dataset&amp;lt;Row&amp;gt; df = spark.createDataFrame(rows, schema);&#xA;    GeneralizedLinearRegression glr = new GeneralizedLinearRegression()&#xA;            .setFamily(&quot;poisson&quot;)&#xA;            .setLink(&quot;log&quot;)&#xA;            .setFeaturesCol(&quot;width&quot;)&#xA;            .setLabelCol(&quot;satellites&quot;)&#xA;            .setMaxIter(1)&#xA;            .setRegParam(0.0);&#xA;&#xA;// Fit the model&#xA;    GeneralizedLinearRegressionModel model = glr.fit(df);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This does not converge and the code throws a &lt;code&gt;java.lang.StackOverflowError&lt;/code&gt;  Exception after 60 secs&lt;/p&gt;&#xA;" OwnerUserId="9443120" LastEditorUserId="9443120" LastEditDate="2018-03-05T19:57:41.003" LastActivityDate="2018-03-05T19:57:41.003" Title="Poisson regression with Apache Spark" Tags="&lt;java&gt;&lt;apache-spark-mllib&gt;&lt;apache-spark-ml&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49101947" PostTypeId="1" CreationDate="2018-03-05T00:23:11.423" Score="0" ViewCount="18" Body="&lt;p&gt;Does PySpark support multiple named windows in the same query? I want to calculate moving averages of various sizes in the same query.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;seconds_per_day = 86400&#xA;seconds_per_minute = 60&#xA;sql('''&#xA;    SELECT datetime,&#xA;           symbol,&#xA;           price,&#xA;           AVG (price) OVER past_7_days AS price_7_day_avg,&#xA;           AVG (price) OVER past_1_hour AS price_1_hour_avg&#xA;      FROM data_formatted&#xA;    WINDOW past_7_days AS (PARTITION BY symbol &#xA;           ORDER BY CAST(datetime AS long)&#xA;           RANGE BETWEEN 7 * {days} PRECEDING AND 1 * {minutes} PRECEDING)&#xA;    WINDOW past_1_hour AS (PARTITION BY symbol &#xA;           ORDER BY CAST(datetime AS long)&#xA;           RANGE BETWEEN 60 * {minutes} PRECEDING AND 1 * {minutes} PRECEDING)&#xA;     ORDER BY symbol ASC, datetime DESC&#xA;      '''.format(&#xA;        days=seconds_per_day,&#xA;        minutes=seconds_per_minute)).show(1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However my code produces the following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;: org.apache.spark.sql.catalyst.parser.ParseException: &#xA;mismatched input 'ORDER' expecting {&amp;lt;EOF&amp;gt;, ',', 'LIMIT'}(line 14, pos 5)&#xA;&#xA;== SQL ==&#xA;&#xA;    SELECT datetime,&#xA;           symbol,&#xA;           price,&#xA;           AVG (price) OVER past_7_days AS price_7_day_avg,&#xA;           AVG (price) OVER past_1_hour AS price_1_hour_avg&#xA;      FROM data_formatted&#xA;    WINDOW past_7_days AS (PARTITION BY symbol &#xA;           ORDER BY CAST(datetime AS long)&#xA;           RANGE BETWEEN 7 * 86400 PRECEDING AND 1 * 60 PRECEDING)&#xA;    WINDOW past_1_hour AS (PARTITION BY symbol &#xA;           ORDER BY CAST(datetime AS long)&#xA;           RANGE BETWEEN 60 * 60 PRECEDING AND 1 * 60 PRECEDING)&#xA;     ORDER BY symbol ASC, datetime DESC&#xA;-----^^^&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Taking out the second named window (and the column that uses it) causes the code to run without errors, but I have to calculate lots of moving averages and I don't want to create a separate table for each column. &lt;/p&gt;&#xA;" OwnerUserId="554481" LastActivityDate="2018-03-05T00:23:11.423" Title="Multiple named windows allowed in Spark?" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;&lt;apache-spark-sql&gt;&lt;spark-dataframe&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49102214" PostTypeId="1" CreationDate="2018-03-05T01:12:32.080" Score="0" ViewCount="45" Body="&lt;p&gt;I have a dataset that is transformed by the following operations:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;dataset.groupBy(&quot;userID&quot;, &quot;itemID&quot;, &quot;relevance&quot;)&#xA;    .agg(functions.max(&quot;value&quot;).as(&quot;value&quot;));&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;in result I get dataset like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+------+------+-----+-------------------+&#xA;|userID|itemID|value|          relevance|&#xA;+------+------+-----+-------------------+&#xA;|     3|     1|  5.0| 0.2132007163556104|&#xA;|     3|     1|  5.0| 0.2132007163556104|&#xA;|     3|     2|  5.0| 0.1111111111111111|&#xA;|     3|     2|  5.0| 0.2222222222222222|&#xA;|     3|     3|  5.0| 0.3434343434343434|&#xA;|     3|     3|  1.0| 0.3434343434343434|&#xA;|     3|     4|  1.0| 0.5555555555555555|&#xA;|     3|     4|  5.0| 0.4999999999999994|&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;but I need to improve my query to remove duplicates, considering that MAX &quot;relevance&quot; must be selected. But in case when &quot;relevance&quot; is equal, MAX &quot;value&quot; must be selected.&#xA;Finally i should get dataset like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+------+------+-----+-------------------+&#xA;|userID|itemID|value|          relevance|&#xA;+------+------+-----+-------------------+&#xA;|     3|     1|  5.0| 0.2132007163556104|&#xA;|     3|     2|  5.0| 0.2222222222222222|&#xA;|     3|     3|  5.0| 0.3434343434343434|&#xA;|     3|     4|  1.0| 0.5555555555555555|&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Sorry for a stupid and boring question, but could somebody help me to solve this task?&lt;/p&gt;&#xA;" OwnerUserId="8897158" LastEditorUserId="8897158" LastEditDate="2018-03-05T01:17:35.530" LastActivityDate="2018-03-05T21:02:52.240" Title="Apache Spark SQL dataset groupBy with max function and different values in one more column" Tags="&lt;java&gt;&lt;apache-spark-sql&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49102292" PostTypeId="1" CreationDate="2018-03-05T01:28:55.333" Score="0" ViewCount="37" Body="&lt;p&gt;On EMR Spark, writing an &lt;code&gt;RDD[String]&lt;/code&gt; to S3 via a dataframe. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;rddString&#xA;  .toDF()&#xA;  .coalesce(16)&#xA;  .write&#xA;  .option(&quot;compression&quot;, &quot;gzip&quot;)&#xA;  .mode(SaveMode.Overwrite)&#xA;  .json(s&quot;s3n://my-bucket/some/new/path&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Save mode is &lt;code&gt;Overwrite&lt;/code&gt; and &lt;code&gt;s3n://my-bucket/some/new/path&lt;/code&gt; does not yet exist.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I consistently get an &lt;code&gt;IOException: File already exists&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;org.apache.spark.SparkException: Job aborted due to stage failure: Task 15 in stage 55.0 failed 4 times, most recent failure: Lost task 15.3 in stage 55.0 (TID 8441, ip-172-31-17-30.us-west-2.compute.internal, executor 3): org.apache.spark.SparkException: Task failed while writing rows&#xA;    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:270)&#xA;    at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:189)&#xA;    at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:188)&#xA;    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)&#xA;    at org.apache.spark.scheduler.Task.run(Task.scala:108)&#xA;    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;    at java.lang.Thread.run(Thread.java:748)&#xA;Caused by: java.io.IOException: File already exists:s3n://my-bucket/some/new/path/part-00015-03a0c001-fc99-4055-9be5-68a1fb0cf6d3-c000.json.gz&#xA;    at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.create(S3NativeFileSystem.java:625)&#xA;    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:932)&#xA;    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:913)&#xA;    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:810)&#xA;    at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.create(EmrFileSystem.java:176)&#xA;    at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)&#xA;    at org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)&#xA;    at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.&amp;lt;init&amp;gt;(JsonFileFormat.scala:140)&#xA;    at org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:80)&#xA;    at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:303)&#xA;    at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:312)&#xA;    at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:256)&#xA;    at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:254)&#xA;    at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1371)&#xA;    at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:259)&#xA;    ... 8 more&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Spark v2.2.1, EMR v5.12.0&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Prior to the exception being thrown, files are written to the destination. However, I cannot tell if they are complete.&lt;/p&gt;&#xA;" OwnerUserId="45525" LastActivityDate="2018-03-05T02:01:00.967" Title="File already exists error writing new files from dataframe" Tags="&lt;apache-spark&gt;&lt;emr&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49102611" PostTypeId="1" CreationDate="2018-03-05T02:24:25.573" Score="1" ViewCount="15" Body="&lt;p&gt;How can i enable the display of the log messages of a loaded JAR library in Apache Zeppeline's Spark interpreter output? For example, all the println() statements in the JAR are shown while the log4j statement are not shown. Please note that the JAR also has its own log4j.properties file included in the package. The log messages appear if invoked from the command line.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried executing the following before calling functions that invoke the log statements:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import org.apache.log4j.{Level, Logger}&#xA;val rootLogger = Logger.getRootLogger()&#xA;rootLogger.setLevel(Level.INFO)&#xA;spark.sparkContext.setLogLevel(&quot;INFO&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However it did not work.&lt;/p&gt;&#xA;" OwnerUserId="4072972" LastActivityDate="2018-03-05T02:24:25.573" Title="Apache Zeppeline Spark interpreter not showing log messages of loaded JAR" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;apache-zeppelin&gt;&lt;log4&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49102802" PostTypeId="1" CreationDate="2018-03-05T02:49:32.420" Score="0" ViewCount="21" Body="&lt;p&gt;When I use JanusGraph 0.2.0 and submit any Spark jobs, e.g.,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;gremlin&amp;gt; graph = GraphFactory.open('conf/hadoop-graph/read-cassandra.properties')&#xA;gremlin&amp;gt; g = graph.traversal().withComputer(SparkGraphComputer)&#xA;gremlin&amp;gt; g.V().count()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;it throws an exception, such as: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;java.lang.ClassCastException: &#xA;  org.apache.hadoop.yarn.&#xA;  proto.YarnServiceProtos$GetNewApplicationRequestProto &#xA;  cannot be cast to com.google.protobuf.Message&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Can anyone help me solve this problem?&lt;/p&gt;&#xA;" OwnerUserId="4669845" LastEditorUserId="3618671" LastEditDate="2018-03-05T17:57:39.743" LastActivityDate="2018-03-05T17:57:39.743" Title="JanusGraph throws exception when submitting Spark job" Tags="&lt;apache-spark&gt;&lt;gremlin&gt;&lt;tinkerpop&gt;&lt;janusgraph&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49103077" PostTypeId="1" CreationDate="2018-03-05T03:28:22.347" Score="0" ViewCount="35" Body="&lt;p&gt;I have configured &lt;code&gt;hadoop&lt;/code&gt; and &lt;code&gt;spark&lt;/code&gt; on cluster and use &lt;code&gt;10.1.10.101&lt;/code&gt; as master node, &lt;code&gt;10.1.10.102 ~ 110&lt;/code&gt; as slave nodes. Here I write a simple python script&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark.conf import SparkConf&#xA;from pyspark.context import SparkContext&#xA;&#xA;sc = SparkContext(conf=SparkConf().setAppName(&quot;test&quot;))&#xA;&#xA;def inside(p):&#xA;    x, y = random.random(), random.random()&#xA;    return x*x + y*y &amp;lt; 1&#xA;&#xA;count = sc.parallelize(xrange(0, NUM_SAMPLES)) \&#xA;             .filter(inside).count()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and submitting it use &lt;code&gt;spark-submit&lt;/code&gt; as follows&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark-submit --master yarn /home/test/pyspark_script.py&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;where &lt;code&gt;pyspark_scirpt.py&lt;/code&gt; is the python file given above. Then I got the following lengthy output, in which several &lt;code&gt;ERROR&lt;/code&gt;s and &lt;code&gt;WARN&lt;/code&gt;s appear.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;18/03/05 21:01:17 INFO spark.SparkContext: Running Spark version 2.1.0&#xA;18/03/05 21:01:17 INFO spark.SecurityManager: Changing view acls to: hadoop&#xA;18/03/05 21:01:17 INFO spark.SecurityManager: Changing modify acls to: hadoop&#xA;18/03/05 21:01:17 INFO spark.SecurityManager: Changing view acls groups to: &#xA;18/03/05 21:01:17 INFO spark.SecurityManager: Changing modify acls groups to: &#xA;18/03/05 21:01:17 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()&#xA;18/03/05 21:01:17 INFO util.Utils: Successfully started service 'sparkDriver' on port 35615.&#xA;18/03/05 21:01:17 INFO spark.SparkEnv: Registering MapOutputTracker&#xA;18/03/05 21:01:17 INFO spark.SparkEnv: Registering BlockManagerMaster&#xA;18/03/05 21:01:17 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information&#xA;18/03/05 21:01:17 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up&#xA;18/03/05 21:01:17 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-2299fc0c-6efe-47e1-9bb5-7e1ca7aa49a1&#xA;18/03/05 21:01:17 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB&#xA;18/03/05 21:01:18 INFO spark.SparkEnv: Registering OutputCommitCoordinator&#xA;18/03/05 21:01:18 INFO util.log: Logging initialized @2328ms&#xA;18/03/05 21:01:18 INFO server.Server: jetty-9.2.z-SNAPSHOT&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69e220f7{/jobs,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f754bab{/jobs/json,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b644d36{/jobs/job,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3c6a49f3{/jobs/job/json,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@44f73311{/stages,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@514a0037{/stages/json,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58ceead5{/stages/stage,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@368515ee{/stages/stage/json,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b17c794{/stages/pool,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4add2d79{/stages/pool/json,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@656afeb5{/storage,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7b5ebd93{/storage/json,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@e00fe0b{/storage/rdd,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d15ac57{/storage/rdd/json,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1fb87016{/environment,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1184f457{/environment/json,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@187da0ca{/executors,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@180ac086{/executors/json,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@446e3b51{/executors/threadDump,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@727e59c7{/executors/threadDump/json,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f0dcb2{/static,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49b0323a{/,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c312cee{/api,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7c01e2ce{/jobs/job/kill,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7d15b4b0{/stages/stage/kill,null,AVAILABLE}&#xA;18/03/05 21:01:18 INFO server.ServerConnector: Started ServerConnector@1307dfca{HTTP/1.1}{0.0.0.0:4040}&#xA;18/03/05 21:01:18 INFO server.Server: Started @2433ms&#xA;18/03/05 21:01:18 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.&#xA;18/03/05 21:01:18 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.1.10.101:4040&#xA;18/03/05 21:01:18 INFO client.RMProxy: Connecting to ResourceManager at hadoop-datanode101.zipeiyi.corp/10.1.10.101:8032&#xA;18/03/05 21:01:19 INFO yarn.Client: Requesting a new application from cluster with 10 NodeManagers&#xA;18/03/05 21:01:19 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (8192 MB per container)&#xA;18/03/05 21:01:19 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead&#xA;18/03/05 21:01:19 INFO yarn.Client: Setting up container launch context for our AM&#xA;18/03/05 21:01:19 INFO yarn.Client: Setting up the launch environment for our AM container&#xA;18/03/05 21:01:19 INFO yarn.Client: Preparing resources for our AM container&#xA;18/03/05 21:01:20 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.&#xA;18/03/05 21:01:21 INFO yarn.Client: Uploading resource file:/tmp/spark-a93238f0-37a0-4e2f-a9b3-0857b87af379/__spark_libs__7062584398451840567.zip -&amp;gt; hdfs://hadoop-datanode101.zipeiyi.corp:8020/user/hadoop/.sparkStaging/application_1520253963057_0003/__spark_libs__7062584398451840567.zip&#xA;18/03/05 21:01:23 INFO yarn.Client: Uploading resource file:/app/zpy/spark/python/lib/pyspark.zip -&amp;gt; hdfs://hadoop-datanode101.zipeiyi.corp:8020/user/hadoop/.sparkStaging/application_1520253963057_0003/pyspark.zip&#xA;18/03/05 21:01:23 INFO yarn.Client: Uploading resource file:/app/zpy/spark/python/lib/py4j-0.10.4-src.zip -&amp;gt; hdfs://hadoop-datanode101.zipeiyi.corp:8020/user/hadoop/.sparkStaging/application_1520253963057_0003/py4j-0.10.4-src.zip&#xA;18/03/05 21:01:23 INFO yarn.Client: Uploading resource file:/tmp/spark-a93238f0-37a0-4e2f-a9b3-0857b87af379/__spark_conf__4059920525961811317.zip -&amp;gt; hdfs://hadoop-datanode101.zipeiyi.corp:8020/user/hadoop/.sparkStaging/application_1520253963057_0003/__spark_conf__.zip&#xA;18/03/05 21:01:23 INFO spark.SecurityManager: Changing view acls to: hadoop&#xA;18/03/05 21:01:23 INFO spark.SecurityManager: Changing modify acls to: hadoop&#xA;18/03/05 21:01:23 INFO spark.SecurityManager: Changing view acls groups to: &#xA;18/03/05 21:01:23 INFO spark.SecurityManager: Changing modify acls groups to: &#xA;18/03/05 21:01:23 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()&#xA;18/03/05 21:01:23 INFO yarn.Client: Submitting application application_1520253963057_0003 to ResourceManager&#xA;18/03/05 21:01:23 INFO impl.YarnClientImpl: Submitted application application_1520253963057_0003&#xA;18/03/05 21:01:23 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1520253963057_0003 and attemptId None&#xA;18/03/05 21:01:24 INFO yarn.Client: Application report for application_1520253963057_0003 (state: ACCEPTED)&#xA;18/03/05 21:01:24 INFO yarn.Client: &#xA;     client token: N/A&#xA;     diagnostics: N/A&#xA;     ApplicationMaster host: N/A&#xA;     ApplicationMaster RPC port: -1&#xA;     queue: default&#xA;     start time: 1520254883772&#xA;     final status: UNDEFINED&#xA;     tracking URL: http://hadoop-datanode101.zipeiyi.corp:8088/proxy/application_1520253963057_0003/&#xA;     user: hadoop&#xA;18/03/05 21:01:25 INFO yarn.Client: Application report for application_1520253963057_0003 (state: ACCEPTED)&#xA;18/03/05 21:01:26 INFO yarn.Client: Application report for application_1520253963057_0003 (state: ACCEPTED)&#xA;18/03/05 21:01:27 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(null)&#xA;18/03/05 21:01:27 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&amp;gt; hadoop-datanode101.zipeiyi.corp, PROXY_URI_BASES -&amp;gt; http://hadoop-datanode101.zipeiyi.corp:8088/proxy/application_1520253963057_0003), /proxy/application_1520253963057_0003&#xA;18/03/05 21:01:27 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter&#xA;18/03/05 21:01:27 INFO yarn.Client: Application report for application_1520253963057_0003 (state: ACCEPTED)&#xA;18/03/05 21:01:28 INFO yarn.Client: Application report for application_1520253963057_0003 (state: RUNNING)&#xA;18/03/05 21:01:28 INFO yarn.Client: &#xA;     client token: N/A&#xA;     diagnostics: N/A&#xA;     ApplicationMaster host: 10.1.10.101&#xA;     ApplicationMaster RPC port: 0&#xA;     queue: default&#xA;     start time: 1520254883772&#xA;     final status: UNDEFINED&#xA;     tracking URL: http://hadoop-datanode101.zipeiyi.corp:8088/proxy/application_1520253963057_0003/&#xA;     user: hadoop&#xA;18/03/05 21:01:28 INFO cluster.YarnClientSchedulerBackend: Application application_1520253963057_0003 has started running.&#xA;18/03/05 21:01:28 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44883.&#xA;18/03/05 21:01:28 INFO netty.NettyBlockTransferService: Server created on 10.1.10.101:44883&#xA;18/03/05 21:01:28 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy&#xA;18/03/05 21:01:28 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.1.10.101, 44883, None)&#xA;18/03/05 21:01:28 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.1.10.101:44883 with 366.3 MB RAM, BlockManagerId(driver, 10.1.10.101, 44883, None)&#xA;18/03/05 21:01:28 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.1.10.101, 44883, None)&#xA;18/03/05 21:01:28 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.1.10.101, 44883, None)&#xA;18/03/05 21:01:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1893d602{/metrics/json,null,AVAILABLE}&#xA;18/03/05 21:01:32 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(null)&#xA;18/03/05 21:01:32 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&amp;gt; hadoop-datanode101.zipeiyi.corp, PROXY_URI_BASES -&amp;gt; http://hadoop-datanode101.zipeiyi.corp:8088/proxy/application_1520253963057_0003), /proxy/application_1520253963057_0003&#xA;18/03/05 21:01:32 INFO ui.JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter&#xA;18/03/05 21:01:34 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (10.1.10.101:55212) with ID 1&#xA;18/03/05 21:01:34 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode101.zipeiyi.corp:39278 with 366.3 MB RAM, BlockManagerId(1, hadoop-datanode101.zipeiyi.corp, 39278, None)&#xA;18/03/05 21:01:35 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (10.1.10.101:55216) with ID 2&#xA;18/03/05 21:01:35 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8&#xA;18/03/05 21:01:35 INFO storage.BlockManagerMasterEndpoint: Registering block manager hadoop-datanode101.zipeiyi.corp:41472 with 366.3 MB RAM, BlockManagerId(2, hadoop-datanode101.zipeiyi.corp, 41472, None)&#xA;Traceback (most recent call last):&#xA;  File &quot;/app/zpy/test/test.py&quot;, line 14, in &amp;lt;module&amp;gt;&#xA;    count = sc.parallelize(xrange(0, NUM_SAMPLES)) \&#xA;NameError: name 'xrange' is not defined&#xA;18/03/05 21:01:35 INFO spark.SparkContext: Invoking stop() from shutdown hook&#xA;18/03/05 21:01:35 INFO server.ServerConnector: Stopped ServerConnector@1307dfca{HTTP/1.1}{0.0.0.0:4040}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@7d15b4b0{/stages/stage/kill,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@7c01e2ce{/jobs/job/kill,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@7c312cee{/api,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@49b0323a{/,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@7f0dcb2{/static,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@727e59c7{/executors/threadDump/json,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@446e3b51{/executors/threadDump,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@180ac086{/executors/json,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@187da0ca{/executors,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1184f457{/environment/json,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@1fb87016{/environment,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2d15ac57{/storage/rdd/json,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@e00fe0b{/storage/rdd,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@7b5ebd93{/storage/json,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@656afeb5{/storage,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4add2d79{/stages/pool/json,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@4b17c794{/stages/pool,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@368515ee{/stages/stage/json,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@58ceead5{/stages/stage,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@514a0037{/stages/json,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@44f73311{/stages,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@3c6a49f3{/jobs/job/json,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@2b644d36{/jobs/job,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@3f754bab{/jobs/json,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@69e220f7{/jobs,null,UNAVAILABLE}&#xA;18/03/05 21:01:35 INFO ui.SparkUI: Stopped Spark web UI at http://10.1.10.101:4040&#xA;18/03/05 21:01:35 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread&#xA;18/03/05 21:01:35 ERROR client.TransportClient: Failed to send RPC 4708722597800291410 to /10.1.10.101:55202: java.nio.channels.ClosedChannelException&#xA;java.nio.channels.ClosedChannelException&#xA;    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)&#xA;18/03/05 21:01:35 ERROR cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(0,0,Map()) to AM was unsuccessful&#xA;java.io.IOException: Failed to send RPC 4708722597800291410 to /10.1.10.101:55202: java.nio.channels.ClosedChannelException&#xA;    at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:249)&#xA;    at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:233)&#xA;    at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:514)&#xA;    at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:488)&#xA;    at io.netty.util.concurrent.DefaultPromise.access$000(DefaultPromise.java:34)&#xA;    at io.netty.util.concurrent.DefaultPromise$1.run(DefaultPromise.java:438)&#xA;    at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:408)&#xA;    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:455)&#xA;    at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)&#xA;    at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)&#xA;    at java.lang.Thread.run(Thread.java:745)&#xA;Caused by: java.nio.channels.ClosedChannelException&#xA;    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)&#xA;18/03/05 21:01:35 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices&#xA;(serviceOption=None,&#xA; services=List(),&#xA; started=false)&#xA;18/03/05 21:01:35 ERROR util.Utils: Uncaught exception in thread Thread-5&#xA;org.apache.spark.SparkException: Exception thrown in awaitResult&#xA;    at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77)&#xA;    at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:75)&#xA;    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)&#xA;    at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)&#xA;    at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)&#xA;    at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)&#xA;    at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)&#xA;    at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.requestTotalExecutors(CoarseGrainedSchedulerBackend.scala:512)&#xA;    at org.apache.spark.scheduler.cluster.YarnSchedulerBackend.stop(YarnSchedulerBackend.scala:93)&#xA;    at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.stop(YarnClientSchedulerBackend.scala:151)&#xA;    at org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:467)&#xA;    at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1588)&#xA;    at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1826)&#xA;    at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)&#xA;    at org.apache.spark.SparkContext.stop(SparkContext.scala:1825)&#xA;    at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)&#xA;    at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)&#xA;    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)&#xA;    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)&#xA;    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)&#xA;    at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)&#xA;    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)&#xA;    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)&#xA;    at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)&#xA;    at scala.util.Try$.apply(Try.scala:192)&#xA;    at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)&#xA;    at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)&#xA;    at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)&#xA;Caused by: java.io.IOException: Failed to send RPC 4708722597800291410 to /10.1.10.101:55202: java.nio.channels.ClosedChannelException&#xA;    at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:249)&#xA;    at org.apache.spark.network.client.TransportClient$3.operationComplete(TransportClient.java:233)&#xA;    at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:514)&#xA;    at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:488)&#xA;    at io.netty.util.concurrent.DefaultPromise.access$000(DefaultPromise.java:34)&#xA;    at io.netty.util.concurrent.DefaultPromise$1.run(DefaultPromise.java:438)&#xA;    at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:408)&#xA;    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:455)&#xA;    at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)&#xA;    at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)&#xA;    at java.lang.Thread.run(Thread.java:745)&#xA;Caused by: java.nio.channels.ClosedChannelException&#xA;    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(...)(Unknown Source)&#xA;18/03/05 21:01:35 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!&#xA;18/03/05 21:01:35 INFO memory.MemoryStore: MemoryStore cleared&#xA;18/03/05 21:01:35 INFO storage.BlockManager: BlockManager stopped&#xA;18/03/05 21:01:35 INFO storage.BlockManagerMaster: BlockManagerMaster stopped&#xA;18/03/05 21:01:35 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!&#xA;18/03/05 21:01:35 INFO spark.SparkContext: Successfully stopped SparkContext&#xA;18/03/05 21:01:35 INFO util.ShutdownHookManager: Shutdown hook called&#xA;18/03/05 21:01:35 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-a93238f0-37a0-4e2f-a9b3-0857b87af379&#xA;18/03/05 21:01:35 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-a93238f0-37a0-4e2f-a9b3-0857b87af379/pyspark-15d826a4-2617-4f3f-a93c-cdab487bbe56&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What do these errors mean and how can I fix it ? Some suggestions to track this problem will be very helpful.       &lt;/p&gt;&#xA;" OwnerUserId="8601361" LastEditorUserId="8601361" LastEditDate="2018-03-05T13:04:08.347" LastActivityDate="2018-03-05T13:04:08.347" Title="Error when submitting a python script on YARN using spark-submit" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;yarn&gt;&lt;spark-submit&gt;" AnswerCount="0" CommentCount="4" />
  <row Id="49103177" PostTypeId="1" CreationDate="2018-03-05T03:41:36.807" Score="1" ViewCount="40" Body="&lt;p&gt;I'm trying to use TFOCS to solve linear programming problems. In the documentation, the &quot;Constraint matrix&quot; is an array of vectors:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val A = sc.parallelize(Array(&#xA;    Vectors.sparse(3, Seq((0, 0.88))),&#xA;    Vectors.sparse(3, Seq((1, 0.63))),&#xA;    Vectors.sparse(3, Seq((0, 0.29), (2, 0.18)))), 2)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Are &lt;code&gt;Vectors&lt;/code&gt; equivalent to &lt;code&gt;Arrays&lt;/code&gt; in Scala? I want to be able to transpose &lt;code&gt;A&lt;/code&gt; and merge it with another vector (like the &lt;code&gt;.r_&lt;/code&gt; function in &lt;code&gt;numpy&lt;/code&gt;). I know that multi-dimensional arrays can be transposed, but is it also possible to transpose an array of vectors? Additionally, is there a &lt;code&gt;.r_&lt;/code&gt; equivalent in Scala?&lt;/p&gt;&#xA;" OwnerUserId="6092420" LastActivityDate="2018-03-05T03:41:36.807" Title="Arrays, vectors and matrices in Scala" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;linear-algebra&gt;&lt;linear-programming&gt;" AnswerCount="0" CommentCount="2" FavoriteCount="0" />
  <row Id="49103223" PostTypeId="1" CreationDate="2018-03-05T03:49:24.587" Score="3" ViewCount="34" Body="&lt;p&gt;I wonder why scala compiler couldn't infer a type of my function parameter when i using PairDStreamFunctions.reduceByKey,here is code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val ssc = new StreamingContext(conf, Seconds(10))&#xA;ssc.checkpoint(&quot;.checkpoint&quot;)&#xA;val lines = ssc.socketTextStream(&quot;localhost&quot;, 9999)&#xA;val words = lines.flatMap(_.split(&quot; &quot;))&#xA;val wordCounts = words&#xA;  .map((_, 1))&#xA;  .reduceByKey((x: Int, y: Int) =&amp;gt; x + y, 4)  //here i must specify the type Int,and this format can't work : reduceByKey((x, y) =&amp;gt; x + y, 4)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;here &lt;strong&gt;i must specify the type Int&lt;/strong&gt; of my function parameter like &lt;strong&gt;reduceByKey((x: Int, y: Int) =&gt; x + y, 4)&lt;/strong&gt; when i use &lt;strong&gt;PairDStreamFunctions&lt;/strong&gt;.reduceByKey ,and this format &lt;strong&gt;couldn't work&lt;/strong&gt; : &lt;strong&gt;reduceByKey((x, y) =&gt; x + y, 4)&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the other hand,when i use PairRDDFunctions.reduceByKey api,it can infer the type,here is the code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val conf = new SparkConf()&#xA;val sc = new SparkContext(conf)&#xA;val rdd = sc.parallelize(List(&#xA;  &quot;hi what&quot;&#xA;  , &quot;show you&quot;&#xA;  , &quot;matter how&quot;&#xA;))&#xA;rdd.flatMap(_.split(&quot; &quot;))&#xA;  .map((_, 1))&#xA;  .reduceByKey((x, y) =&amp;gt; x + y, 4)//in this code,scala compiler could infer the type of my function parameter (x,y) =&amp;gt; x+y&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When i use  &lt;strong&gt;PairRDDFunctions&lt;/strong&gt;.reduceByKey, &lt;strong&gt;reduceByKey((x, y) =&gt; x + y, 4) could work.&lt;/strong&gt;&#xA;I really don't understand what makes it different?&lt;/p&gt;&#xA;" OwnerUserId="9443688" LastEditorUserId="62576" LastEditDate="2018-03-05T03:50:40.763" LastActivityDate="2018-03-05T06:35:42.930" Title="why scala compiler failed to infer type when i write PairDStreamFunctions.reduceByKey" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49104093" PostTypeId="1" AcceptedAnswerId="49104130" CreationDate="2018-03-05T05:38:58.627" Score="1" ViewCount="59" Body="&lt;p&gt;I have data in two dataframes:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;selectedPersonDF:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ID    key  Name&#xA;1          lak&#xA;2          Mouny   &#xA;3          kkk&#xA;4          nnn&#xA;5          hhh&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;selectedDetailsDF:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;first  second third  key&#xA;--------------------------&#xA;1       9       9    777&#xA;9       8       8    878&#xA;8       10      10   765&#xA;10      12      19   909&#xA;11      2       20   708&#xA;&#xA;val columnsToCheck = selectedDetailsDF.columns.toSet - &quot;key&quot; toList&#xA;&#xA;import org.apache.spark.sql.functions._&#xA;val tempSelectedDetailsDF = selectedDetailsDF.select(array(columnsToCheck.map(col): _*).as(&quot;array&quot;), col(&quot;key&quot;).as(&quot;key2&quot;))&#xA;&#xA;&#xA;&#xA;val arrayContains = udf((array: collection.mutable.WrappedArray[String], value: String) =&amp;gt; array.contains(value))&#xA;&#xA;val finalDF = selectedPersonDF.join(tempSelectedDetailsDF, arrayContains($&quot;array&quot;, $&quot;ID&quot;), &quot;left&quot;)&#xA;  .select($&quot;ID&quot;, $&quot;key2&quot;.as(&quot;key&quot;))&#xA;  .na.fill(&quot;&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Getting output as below,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+---+---+&#xA;|ID |key|&#xA;+---+---+&#xA;|1  |777|&#xA;|2  |708|&#xA;|3  |   |&#xA;|4  |   |&#xA;|5  |   |&#xA;+---+---+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;expecting: &#xA;i want to show all my columns from selectedPersonDF I have to match the selectedPersonDF id column with selectedDetailsDF all the columns(First, Second, Third) if any of the column data matches with persons id then we have to take the key value from selectedDetailsDF and have to update in selectedPersonDF key column&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ID    key  Name&#xA;1     777  lak&#xA;2     708  Mouny   &#xA;3          kkk&#xA;4          nnn&#xA;5          hhh&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Please help me anyone.&lt;/p&gt;&#xA;" OwnerUserId="8919697" LastEditorUserId="8919697" LastEditDate="2018-03-05T05:51:40.933" LastActivityDate="2018-03-05T07:43:21.280" Title="Scala how to match two dfs if mathes then update the key in first df and select all columns from required df" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49104682" PostTypeId="1" AcceptedAnswerId="49104754" CreationDate="2018-03-05T06:32:56.820" Score="0" ViewCount="35" Body="&lt;p&gt;I need to update value of dataframe column based on a string that isn't part of any other column in the dataframe. How do I do this? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For e.g. Let's say my dataframe has column A, B, C. I want to update value of column C based on combination of value in column A &amp;amp; a static string. I tried to do the following.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val df = originalDF.withColumn(&quot;C&quot;, Helper.dudf(df(&quot;A&quot;), lit(&quot;str&quot;)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My helper class as following&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val addDummyColumn :(String, String)=&amp;gt;String=(input:String, recordType: String)=&amp;gt;{input}&#xA;&#xA;val dummyUDF = udf(addDummyColumn)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My UDF that takes in variable A &amp;amp; recordType: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;if(recordType.equals(&quot;TRANSACTION&quot;) {&#xA; if(A &amp;gt; 0 ) return &quot;CHARGE&quot;;&#xA;   else return &quot;REFUND&quot;&#xA;} else if (recordType.equals(&quot;CHARGEBACK&quot;) {&#xA;    return &quot;CHARGEBACK&quot;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Example Input &amp;amp; Output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Sample Input:&#xA;A=10, recordType=TRANSACTION&#xA;Output: C = CHARGE&#xA;A=-10, recordType=TRANSACTION&#xA;C = REFUND&#xA;&#xA;A=10, recordType=CHARGEBACK&#xA;C = CHARGEBACK&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My problem is that withColumn only accepts Column so I did lit(&quot;str&quot;) but I don't know how to extract value of that column in my UDF. Ideas?&lt;/p&gt;&#xA;" OwnerUserId="2337490" LastEditorUserId="2337490" LastEditDate="2018-03-05T06:49:32.607" LastActivityDate="2018-03-05T07:08:06.567" Title="Updating dataframe in spark/scala" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="2" CommentCount="5" />
  <row Id="49104868" PostTypeId="1" CreationDate="2018-03-05T06:47:12.433" Score="4" ViewCount="30" Body="&lt;p&gt;When creating a spark based java application, SparkConf is created using &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sparkConf = new SparkConf().setAppName(&quot;SparkTests&quot;)&#xA;                           .setMaster(&quot;local[*]&quot;).set(&quot;spark.executor.memory&quot;, &quot;2g&quot;)&#xA;                           .set(&quot;spark.driver.memory&quot;, &quot;2g&quot;)&#xA;                           .set(&quot;spark.driver.maxResultSize&quot;, &quot;2g&quot;);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But in the documentation &lt;a href=&quot;https://spark.apache.org/docs/latest/configuration.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;, it says that &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Any values specified as flags or in the properties file will be passed on to the application and merged with those specified through SparkConf. Properties set directly on the SparkConf take highest precedence, then flags passed to spark-submit or spark-shell, then options in the spark-defaults.conf file. A few configuration keys have been renamed since earlier versions of Spark; in such cases, the older key names are still accepted, but take lower precedence than any instance of the newer key.&#xA;  Spark properties mainly can be divided into two kinds: one is related to deploy, like “spark.driver.memory”, “spark.executor.instances”, this kind of properties may not be affected when setting programmatically through SparkConf in runtime, or the behavior is depending on which cluster manager and deploy mode you choose, so it would be suggested to set through configuration file or spark-submit command line options; another is mainly related to Spark runtime control, like “spark.task.maxFailures”, this kind of properties can be set in either way.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So is there a list of these deploy related properties which I can only give as command line arguments in spark-submit?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is given &lt;code&gt;local[*]&lt;/code&gt; here, but in run-time we are deploying through a yarn cluster.&lt;/p&gt;&#xA;" OwnerUserId="7353875" LastEditorUserId="7224597" LastEditDate="2018-03-05T06:48:36.457" LastActivityDate="2018-03-07T10:17:51.737" Title="Spark deploy-related properties in spark-submite" Tags="&lt;java&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" />
  <row Id="49105079" PostTypeId="1" CreationDate="2018-03-05T07:05:26.807" Score="0" ViewCount="28" Body="&lt;p&gt;I have the following mappings:&#xA;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;webSocket(&quot;/test/&quot;,Tester.class);&#xA;get(&quot;/home&quot;,Controller.getHome,templateEngine);&#xA;get(&quot;/:param/&quot;,Controller.getRoot,templateEngine);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I get the following error when connecting to the websocket at test&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Error during WebSocket handshake: Unexpected response code: 200&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;as the request gets handled by the get /:param route map. However, the get request to /home is not handled by /:param, and is handled by /home as expected. How can I enforce that the route map to work correctly for web sockets too?&lt;/p&gt;&#xA;" OwnerUserId="6482897" LastEditorUserId="6482897" LastEditDate="2018-03-05T17:46:12.993" LastActivityDate="2018-03-05T17:46:12.993" Title="Web socket connection request handled by get route in Spark framework" Tags="&lt;websocket&gt;&lt;jetty&gt;&lt;spark-java&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49105090" PostTypeId="1" CreationDate="2018-03-05T07:06:17.723" Score="1" ViewCount="28" Body="&lt;p&gt;Our production is 5 node Cassandra cluster and trying to access cassandra table in Spark scala code through datastax connector.&#xA;Our spark jobs takes longer than usual time and found that one of cassandra node is down which causing the delay.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Spark 2.1.0&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;&quot;com.datastax.spark&quot; %% &quot;spark-cassandra-connector&quot; % &quot;2.0.0-M3&quot;&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;scala 2.11.8&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; SparkSession.builder().&#xA;  appName(jobName).&#xA;  config(&quot;spark.cassandra.connection.host&quot;, &quot;27.0.5.126&quot;).&#xA;  config(&quot;spark.cassandra.connection.port&quot;, &quot;9042&quot;).&#xA;  config(&quot;spark.cassandra.input.consistency.level&quot;, &quot;ONE&quot;).&#xA;  config(&quot;spark.cassandra.output.consistency.level&quot;, &quot;ONE&quot;).&#xA;  config(&quot;hive.exec.dynamic.partition&quot;, &quot;true&quot;).&#xA;  config(&quot;hive.exec.dynamic.partition.mode&quot;, &quot;nonstrict&quot;).&#xA;  config(&quot;hive.enforce.bucketing&quot;,&quot;true&quot;).&#xA;  enableHiveSupport().getOrCreate()&#xA;&#xA; val alerts = spark.read.format(&quot;org.apache.spark.sql.cassandra&quot;).&#xA;  options(Map(&quot;table&quot;-&amp;gt;tableName, &quot;keyspace&quot; -&amp;gt; keyspace)).&#xA;  load().collect()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When we specify all the nodes in spark config, spark connector wait for 2 minutes before timing out to create the connection pool.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;config(&quot;spark.cassandra.connection.host&quot;, &quot;27.0.5.126,27.0.4.223,27.0.6.85,27.0.6.59&quot;).&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;18/03/02 19:10:40 INFO NettyUtil: Found Netty's native epoll transport in the classpath, using it&#xA;18/03/02 19:10:40 INFO Cluster: New Cassandra host /27.0.5.126:9042 added&#xA;18/03/02 19:10:40 INFO Cluster: New Cassandra host /27.0.4.223:9042 added&#xA;18/03/02 19:10:40 INFO Cluster: New Cassandra host /27.0.6.85:9042 added&#xA;18/03/02 19:10:40 INFO Cluster: New Cassandra host /27.0.6.59:9042 added&#xA;18/03/02 19:10:40 INFO Cluster: New Cassandra host /27.0.6.187:9042 added&#xA;18/03/02 19:10:40 INFO CassandraConnector: Connected to Cassandra cluster: Test Cluster&#xA;18/03/02 19:12:40 WARN Session: Error creating pool to /27.0.6.187:9042&#xA;com.datastax.driver.core.exceptions.ConnectionException: [/27.0.6.187] Pool was closed during initialization&#xA;at com.datastax.driver.core.HostConnectionPool$2.onSuccess(HostConnectionPool.java:149)&#xA;at com.datastax.driver.core.HostConnectionPool$2.onSuccess(HostConnectionPool.java:135)&#xA;at shade.com.datastax.spark.connector.google.common.util.concurrent.Futures$6.run(Futures.java:1319)&#xA;at &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;When I pass a single cassandra host for cassandra connection (a working one), due to load balancing policy, it attempts to connect to all nodes in the datacenter1 and timed out on the bad node which took almost 2 minutes.&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;18/03/02 21:17:37 INFO NettyUtil: Found Netty's native epoll transport in the classpath, using it&#xA;18/03/02 21:17:37 INFO Cluster: New Cassandra host /27.0.5.126:9042 added&#xA;18/03/02 21:17:37 INFO LocalNodeFirstLoadBalancingPolicy: Added host 27.0.5.126 (datacenter1)&#xA;18/03/02 21:17:37 INFO Cluster: New Cassandra host /27.0.4.223:9042 added&#xA;18/03/02 21:17:37 INFO LocalNodeFirstLoadBalancingPolicy: Added host 27.0.4.223 (datacenter1)&#xA;18/03/02 21:17:37 INFO Cluster: New Cassandra host /27.0.6.85:9042 added&#xA;18/03/02 21:17:37 INFO Cluster: New Cassandra host /27.0.6.59:9042 added&#xA;18/03/02 21:17:37 INFO LocalNodeFirstLoadBalancingPolicy: Added host 27.0.6.59 (datacenter1)&#xA;18/03/02 21:17:37 INFO Cluster: New Cassandra host /27.0.6.187:9042 added&#xA;18/03/02 21:17:37 INFO LocalNodeFirstLoadBalancingPolicy: Added host 27.0.6.187 (datacenter1)&#xA;18/03/02 21:17:37 INFO CassandraConnector: Connected to Cassandra cluster: Test Cluster&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;how can I avoid this delay if any one of node is down in Casssandra cluster&lt;/em&gt;&lt;/p&gt;&#xA;" OwnerUserId="9231454" LastActivityDate="2018-03-05T07:06:17.723" Title="Cassandra spark connector takes long delays to access table" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;cassandra&gt;&lt;spark-cassandra-connector&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49105404" PostTypeId="1" CreationDate="2018-03-05T07:29:47.027" Score="0" ViewCount="14" Body="&lt;p&gt;when i am executing simple sparkr script in Zeppelin :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;code &lt;code&gt;%spark.r 2 + 2&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am finding following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;org.apache.zeppelin.interpreter.InterpreterException: sparkr is not  responding &#xA;R version 3.0.2 (2013-09-25) -- &quot;Frisbee Sailing&quot;&#xA;Copyright (C) 2013 The R Foundation for Statistical Computing&#xA;Platform: x86_64-pc-linux-gnu (64-bit)&#xA;&#xA;R is free software and comes with ABSOLUTELY NO WARRANTY.&#xA;You are welcome to redistribute it under certain conditions.&#xA;Type 'license()' or 'licence()' for distribution details.&#xA;&#xA;  Natural language support but running in an English locale&#xA;&#xA;R is a collaborative project with many contributors.&#xA;Type 'contributors()' for more information and&#xA;'citation()' on how to cite R or R packages in publications.&#xA;&#xA;Type 'demo()' for some demos, 'help()' for on-line help, or&#xA;'help.start()' for an HTML browser interface to help.&#xA;Type 'q()' to quit R.&#xA;&#xA;&#xA;&#xA;at org.apache.zeppelin.spark.ZeppelinR.open(ZeppelinR.java:165)&#xA;at org.apache.zeppelin.spark.SparkRInterpreter.open(SparkRInterpreter.java:91)&#xA;at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)&#xA;at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:618)&#xA;at org.apache.zeppelin.scheduler.Job.run(Job.java:186)&#xA;at org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)&#xA;at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)&#xA;at java.util.concurrent.FutureTask.run(FutureTask.java:266)&#xA;at &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;even i have set SPARK_HOME and JAVA_HOME in zeppline.env.sh&lt;/p&gt;&#xA;" OwnerUserId="7527189" LastActivityDate="2018-03-05T07:29:47.027" Title="org.apache.zeppelin.interpreter.InterpreterException: sparkr is not responding" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;apache-zeppelin&gt;&lt;sparkr&gt;&lt;zeppelin&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49105485" PostTypeId="1" AcceptedAnswerId="49108122" CreationDate="2018-03-05T07:36:47.273" Score="1" ViewCount="37" Body="&lt;p&gt;I load a file into a dataframe and before saving the dataframe in my table, I want to check if any rows in the dataframe already exists in the table. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;e.g. &#xA;My table has following schema&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;A  B  C &#xA;1  2  3 &#xA;2  4  5&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My dataframe has following&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;A  B  C&#xA;1  2  5 &#xA;2  3  5&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Definition of unique is combination of A &amp;amp; B column. So in the above example, the duplicate entry is ( 1, 2, 5) since both dataframe and the table has (1, 2) as a value for A &amp;amp; B. And Unique record is (2,3,5) since table doesn't have (2,3) as value for A &amp;amp; B. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the output I want is two dataframe. One that contains unique i.e. (2,3,5) and one that contains duplicate i.e (1,2,5).  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Let's call my current dataframe as df1 &amp;amp; the dataframe after loading the table as df2.&lt;/p&gt;&#xA;" OwnerUserId="9444482" LastActivityDate="2018-03-05T18:29:46.283" Title="Separating unique and duplicate entries between two dataframes" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="49105568" PostTypeId="1" AcceptedAnswerId="49105666" CreationDate="2018-03-05T07:42:25.660" Score="-2" ViewCount="53" Body="&lt;p&gt;For below Dataset, to get Total Summary values of &lt;strong&gt;Col1&lt;/strong&gt; , I did &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import org.apache.spark.sql.functions._&#xA;val totaldf = df.groupBy(&quot;Col1&quot;).agg(lit(&quot;Total&quot;).as(&quot;Col2&quot;), sum(&quot;price&quot;).as(&quot;price&quot;), sum(&quot;displayPrice&quot;).as(&quot;displayPrice&quot;))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and then merged with &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df.union(totaldf).orderBy(col(&quot;Col1&quot;), col(&quot;Col2&quot;).desc).show(false)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;df.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+-----------+-------+--------+--------------+&#xA;|   Col1    | Col2  | price  | displayPrice |&#xA;+-----------+-------+--------+--------------+&#xA;| Category1 | item1 |     15 |           14 |&#xA;| Category1 | item2 |     11 |           10 |&#xA;| Category1 | item3 |     18 |           16 |&#xA;| Category2 | item1 |     15 |           14 |&#xA;| Category2 | item2 |     11 |           10 |&#xA;| Category2 | item3 |     18 |           16 |&#xA;+-----------+-------+--------+--------------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;After merging.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+-----------+-------+-------+--------------+&#xA;|   Col1    | Col2  | price | displayPrice |&#xA;+-----------+-------+-------+--------------+&#xA;| Category1 | Total |    44 |           40 |&#xA;| Category1 | item1 |    15 |           14 |&#xA;| Category1 | item2 |    11 |           10 |&#xA;| Category1 | item3 |    18 |           16 |&#xA;| Category2 | Total |    46 |           44 |&#xA;| Category2 | item1 |    16 |           15 |&#xA;| Category2 | item2 |    11 |           10 |&#xA;| Category2 | item3 |    19 |           17 |&#xA;+-----------+-------+-------+--------------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now I want summary of Whole Dataset as Below , which will have Col1 Summary as Total and has the Data of All Col1 and Col2.&#xA;&lt;strong&gt;Required.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    +-----------+-------+-------+--------------+&#xA;    |   Col1    | Col2  | price | displayPrice |&#xA;    +-----------+-------+-------+--------------+&#xA;    | Total     | Total |    90 |           84 |&#xA;    | Category1 | Total |    44 |           40 |&#xA;    | Category1 | item1 |    15 |           14 |&#xA;    | Category1 | item2 |    11 |           10 |&#xA;    | Category1 | item3 |    18 |           16 |&#xA;    | Category2 | Total |    46 |           44 |&#xA;    | Category2 | item1 |    16 |           15 |&#xA;    | Category2 | item2 |    11 |           10 |&#xA;    | Category2 | item3 |    19 |           17 |&#xA;    +-----------+-------+-------+--------------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How Can I be able to achieve the above result?&lt;/p&gt;&#xA;" OwnerUserId="9444445" LastActivityDate="2018-03-06T11:05:29.480" Title="Getting the Summary of Whole Dataset or Only Columns in Apache Spark Java" Tags="&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;apache-spark-dataset&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49105743" PostTypeId="1" CreationDate="2018-03-05T07:55:04.330" Score="-4" ViewCount="28" Body="&lt;p&gt;I have 2 different pyspark dataframes having a column inside which each row present as a sparse matrix. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sample - &#xA;&lt;a href=&quot;https://i.stack.imgur.com/cL6r3.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/cL6r3.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/iE7b7.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/iE7b7.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How to get max cosine similarity between the 2 different dataframes for each row of dataframe 1 column when compared with dataframe 2?&lt;/p&gt;&#xA;" OwnerUserId="6038067" LastActivityDate="2018-03-05T07:55:04.330" Title="PySpark cosine similarity" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49106011" PostTypeId="1" CreationDate="2018-03-05T08:14:17.877" Score="0" ViewCount="10" Body="&lt;p&gt;I have a dataset and want to get the 1st and 2nd item in the list.&#xA;I have a very strange problem occuring only after a very long chain of compuation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That is to say I am calculating a dataset of MyTest.&#xA;And calling  ds.mapPartitions(topNGLCPoints)&#xA;I get bad results. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;But if I create a list of same object and make it a dataset myself&#xA;I do not get same error. As you see hashCode for objects are calculated differently.&#xA;The strange thing about code is it is generating an output as below:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;9 arrived @-1109701733 GLC 475163&#xA;&#xA;first before @1416367481 GLC -1&#xA;second before @1416367482 GLC -2&#xA;path3 &#xA;&#xA;&#xA;first end @-1109701733 GLC 475163&#xA;second end @1416367481 GLC -1&#xA;&#xA;9 arrived @-1109701732 GLC 475164&#xA;&#xA;first before @-1109701732 GLC 475164&#xA;second before @1416367481 GLC -1&#xA;path4 &#xA;&#xA;&#xA;first end @-1109701732 GLC 475164&#xA;second end @-1109701732 GLC 475164&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When second 9 arrives it also seems to be the value of  first. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also before doing anything first is also assigned to new element without doing anything.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Summary:&#xA;Somehow first and second are pointing to iterator next , and at every change of iterator first and second are automatically assigned to it.(Before ifelse blocks)&#xA;but how?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def dumpVars(sbf:mutable.StringBuilder,first : MyTest ,second : MyTest,info:String): Unit ={&#xA;val firstVal = if(first ==null )&quot;&quot; else first.hashCode +&quot; GLC &quot;+ first.glcId&#xA;val secondVal = if(second ==null )&quot;&quot; else second.hashCode +&quot; GLC &quot;+ second.glcId&#xA;sbf.append(&quot;\n  first &quot;+info+&quot; @&quot; + firstVal)&#xA;sbf.append(&quot;\n  second &quot;+info+&quot; @&quot; + secondVal)&#xA;}&#xA;&#xA;def topNGLCPoints(iter: Iterator[MyTest]): Iterator[MyTest] = {&#xA;&#xA;val i = 0&#xA;//Inited to Dummy objects&#xA;var first : MyTest = MyTest(i + &quot;&quot;, 2, i + &quot;&quot;, i + &quot;&quot;,glcId = -1)&#xA;var second : MyTest = MyTest(i + &quot;&quot;, 2, i + &quot;&quot;, i + &quot;&quot;,glcId = -2)&#xA;&#xA;val sbf = new mutable.StringBuilder()&#xA;&#xA;sbf.append(distinctHash + &quot;\n&quot;)&#xA;&#xA;while (iter.hasNext) {&#xA;  val el = iter.next()&#xA;&#xA;  val point = el.testParams.addressPoint&#xA;&#xA;    sbf.append(&quot;\n\n&quot; + point + &quot; arrived @&quot; +el.hashCode() + &quot; GLC &quot;+ el.glcId + &quot;\n&quot;)&#xA;&#xA;    dumpVars(sbf,first,second,&quot;begin&quot;)&#xA;&#xA;    if( first == null){&#xA;      first = el&#xA;      sbf.append(&quot;\npath1 \n&quot; )&#xA;    }else if( second == null){&#xA;      second = el&#xA;      sbf.append(&quot;\npath2 \n&quot; )&#xA;    }else if( point &amp;gt; first.testParams.addressPoint){&#xA;      second = first&#xA;      first = el&#xA;      sbf.append(&quot;\npath3 \n&quot; )&#xA;    }else if( point &amp;gt; second.testParams.addressPoint){&#xA;      second = el&#xA;      sbf.append(&quot;\npath4 \n&quot; )&#xA;    }&#xA;&#xA;    sbf.append(&quot;\n  --------&quot; )&#xA;    dumpVars(sbf,first,second,&quot;end&quot;)&#xA;&#xA;&#xA;}&#xA;&#xA;sbf.append(&quot;\n  iteration end @&quot; )&#xA;dumpVars(sbf,first,second,&quot;final&quot;)&#xA;&#xA;var resultBuffer = Array(first,second).filter( _  != null)&#xA;val it = resultBuffer.iterator&#xA;it&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="5156154" LastActivityDate="2018-03-05T08:14:17.877" Title="Spark MapPartition Strange Behavior" Tags="&lt;apache-spark&gt;&lt;dataset&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49106063" PostTypeId="1" CreationDate="2018-03-05T08:18:22.793" Score="0" ViewCount="17" Body="&lt;p&gt;I am trying to use spark-hive lib inside a spring boot project, I can create the table, but then on methods like show(), or count()the following error occurs. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Caused by: java.lang.IllegalAccessError: tried to access method com.google.common.base.Stopwatch.&amp;lt;init&amp;gt;()V from class org.apache.hadoop.mapred.FileInputFormat&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;if i remove the spark-hive dependency, then i can use the show() and count() methods without any problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;moreover, i try the code with the same dependencies in a normal java project (without spring) and it works fine. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think there is a problem in the dependencies between hive and spring, because if i use spark 2.0.0 after the error (after creating the table) it works fine for inserting new values into the table. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;any thoughts? thanks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;my code&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; spark.sql(&quot;CREATE TABLE IF NOT EXISTS &quot;+tableName+&quot; (firstName STRING,middleName STRING,lastName STRING) USING hive&quot;);&#xA;    spark.sql(&quot;INSERT INTO &quot;+tableName+&quot; VALUES ('test','test','test') &quot;);&#xA;    Dataset&amp;lt;Row&amp;gt; sqlDF = spark.sql(&quot;SELECT * FROM &quot;+tableName);&#xA;    count = sqlDF.count();&#xA;    sqlDF.show();&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;my dependencies &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;dependencies {&#xA;compile('org.springframework.boot:spring-boot-starter-actuator')&#xA;        {&#xA;            exclude module: &quot;spring-boot-starter-logging&quot;&#xA;            exclude module: &quot;logback-classic&quot;&#xA;        }&#xA;compile('org.springframework.boot:spring-boot-starter-web')&#xA;        {&#xA;            exclude module: &quot;logback-classic&quot;&#xA;        }&#xA;compile 'org.apache.spark:spark-core_2.11:2.3.0'&#xA;compile 'org.apache.spark:spark-sql_2.11:2.3.0'&#xA;compile 'org.apache.spark:spark-hive_2.11:2.3.0'&#xA;runtime('org.springframework.boot:spring-boot-devtools')&#xA;testCompile('org.springframework.boot:spring-boot-starter-test')&#xA;testCompile('org.springframework.security:spring-security-test')&#xA;compile group: 'io.springfox', name: 'springfox-swagger-ui', version: '2.8.0'&#xA;compile group: 'io.springfox', name: 'springfox-swagger2', version: '2.8.0'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;}&lt;/p&gt;&#xA;" OwnerUserId="7924934" LastActivityDate="2018-03-05T08:18:22.793" Title="spark-hive with Spring IllegalAccessError" Tags="&lt;java&gt;&lt;spring&gt;&lt;apache-spark&gt;&lt;hive&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49106151" PostTypeId="1" AcceptedAnswerId="49107112" CreationDate="2018-03-05T08:25:09.620" Score="0" ViewCount="43" Body="&lt;p&gt;I have dataset which like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+-------+------+-------+&#xA;|groupid|rownum|column2|&#xA;+-------+------+-------+&#xA;|   1   |  1   |   7   |&#xA;|   1   |  2   |   9   |&#xA;|   1   |  3   |   8   |&#xA;|   1   |  4   |   5   |&#xA;|   1   |  5   |   1   |&#xA;|   1   |  6   |   0   |&#xA;|   1   |  7   |   15  |&#xA;|   1   |  8   |   1   |&#xA;|   1   |  9   |   13  |&#xA;|   1   |  10  |   20  |&#xA;|   2   |  1   |   8   |&#xA;|   2   |  2   |   1   |&#xA;|   2   |  3   |   4   |&#xA;|   2   |  4   |   2   |&#xA;|   2   |  5   |   19  |&#xA;|   2   |  6   |   11  |&#xA;|   2   |  7   |   5   |&#xA;|   2   |  8   |   6   |&#xA;|   2   |  9   |   15  |&#xA;|   2   |  10  |   8   |&#xA; still have more rows......&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to add a new column &quot;column3&quot; , which if the continuous column2 values are less than 10,then they will be arranged a same number such as 1. if their appear a value larger than 10 in column2, this row will be dropped ，then the following column3 row’s value will increase 1. For example， when groupid = 1，the column3's value from rownum 1 to 6 will be 1 and the rownum7 will be dropped, the column3's value of rownum 8 will be 2 and the rownum9,10 will be dropped.After the procedure, the table will like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+-------+------+-------+-------+&#xA;|groupid|rownum|column2|column3|&#xA;+-------+------+-------+-------+&#xA;|   1   |  1   |   7   |   1   |&#xA;|   1   |  2   |   9   |   1   | &#xA;|   1   |  3   |   8   |   1   |&#xA;|   1   |  4   |   5   |   1   |&#xA;|   1   |  5   |   1   |   1   |&#xA;|   1   |  6   |   0   |   1   |&#xA;|   1   |  7   |   15  |  drop | this row will be dropped, in fact not exist  &#xA;|   1   |  8   |   1   |   2   |&#xA;|   1   |  9   |   13  |  drop |  like above&#xA;|   1   |  10  |   20  |  drop |  like above&#xA;|   2   |  1   |   8   |   1   |&#xA;|   2   |  2   |   1   |   1   |&#xA;|   2   |  3   |   4   |   1   |&#xA;|   2   |  4   |   2   |   1   |&#xA;|   2   |  5   |   19  |  drop |   ...&#xA;|   2   |  6   |   11  |  drop |   ...&#xA;|   2   |  7   |   5   |   2   |&#xA;|   2   |  8   |   6   |   2   |&#xA;|   2   |  9   |   15  |  drop |   ...&#xA;|   2   |  10  |   8   |   3   |&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In our project, the dataset is expressed as dataframe in spark sql&#xA;I try to solve this problem by udf in this way:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;var last_rowNum: Int = 1&#xA;var column3_Num: Int = 1    &#xA;def assign_column3_Num(rowNum:Int): Int = {&#xA;    if (rowNum == 1){   //do nothing, just arrange 1&#xA;      column3_Num = 1&#xA;      last_rowNum = 1&#xA;      return column3_Num&#xA;    }&#xA;    /*** if the difference between rownum is 1, they have the same column3 &#xA;     * value, if not, column3_Num++, so they are different&#xA;     */ &#xA;    if(rowNum - last_rowNum == 1){  &#xA;      last_rowNum = rowNum&#xA;      return column3_Num&#xA;    }else{&#xA;      column3_Num += 1&#xA;      last_rowNum = rowNum&#xA;      return column3_Num&#xA;    }&#xA;}&#xA;spark.sqlContext.udf.register(&quot;assign_column3_Num&quot;,assign_column3_Num _)&#xA;df.filter(&quot;column2&amp;gt;10&quot;)   //drop the larger rows&#xA;  .withColumn(&quot;column3&quot;,assign_column3_Num(col(&quot;column2&quot;))) //add column3&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;as you can see, i use global variable. However, it's only effective in spark local[1] model. if i use local[8] or yarn-client, the result will totally wrong! this is because spark's running mechanism，they operate the global variable without distinguishing groupid and order!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the question is how can i arrange right number when spark running on cluster?&#xA;use udf or udaf or RDD or other ?&#xA;thank you!&lt;/p&gt;&#xA;" OwnerUserId="8740367" LastEditorUserId="8740367" LastEditDate="2018-03-05T13:17:23.920" LastActivityDate="2018-03-05T13:17:23.920" Title="how to operate global variable in spark sql dataframe row by row sequentially on spark cluster?" Tags="&lt;apache-spark&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="49106333" PostTypeId="1" CreationDate="2018-03-05T08:36:57.077" Score="0" ViewCount="32" Body="&lt;p&gt;In H2O KMeans Cluster. is there a way to calculate the actual distances from the cluster centroids for each point in the data set?&#xA;Currently H2o Gives the predicted Cluster for the data passed but what the best way of getting the distance of a point from its cluster centroid.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I intend to this for anomaly detection where points found far from the centroid are seen as anomalies. I have dont this using Apache Spark but Intend to try this using Sparking Water but the H2o Api does not seem to show the best way to get distances for each point from the cluster centroid.&lt;/p&gt;&#xA;" OwnerUserId="7606428" LastActivityDate="2018-03-06T21:42:15.023" Title="Get Distance of Point From Cluster Centroid on H2o KMEANS Clustering" Tags="&lt;python&gt;&lt;k-means&gt;&lt;h2o&gt;&lt;anomaly-detection&gt;&lt;sparkling-water&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49106500" PostTypeId="1" CreationDate="2018-03-05T08:49:12.090" Score="-3" ViewCount="25" Body="&lt;p&gt;getting error  &lt;code&gt;unclosed string literal&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;code :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val relianceDF = relianceRDD.map(rel =&amp;gt; {&#xA; (rel.split(&quot;,&quot;)(0).toInt,&#xA;  rel.split(&quot;,&quot;)(1),&#xA;  rel.split(&quot;,')(2),&#xA;  rel.split(&quot;,&quot;)(3).toInt,&#xA;  rel.split(&quot;,&quot;)(4))&#xA; }).toDF&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="9444721" LastEditorUserId="5880706" LastEditDate="2018-03-05T09:31:23.010" LastActivityDate="2018-03-05T09:31:23.010" Title="error in scala while creating dataframe" Tags="&lt;scala&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="49106611" PostTypeId="1" CreationDate="2018-03-05T08:56:26.277" Score="1" ViewCount="20" Body="&lt;p&gt;Im trying to execute some of the sample Python - Scikit scripts into Spark in Single node (my desktop - Mac - 8 GB). Here is my configuration&#xA;spark-env.sh file.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SPARK_MASTER_HOST='IP'&#xA;SPARK_WORKER_INSTANCES=3&#xA;SPARK_WORKER_CORES=2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Im starting my slaves&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;./sbin/start-slave.sh spark://IP&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Workers Table in (&lt;a href=&quot;http://localhost:8080/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://localhost:8080/&lt;/a&gt;) shows there are 3 workers running with each 2 cores&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My script file which I took it from (&lt;a href=&quot;https://databricks.com/blog/2016/02/08/auto-scaling-scikit-learn-with-apache-spark.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://databricks.com/blog/2016/02/08/auto-scaling-scikit-learn-with-apache-spark.html&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn import svm, grid_search, datasets&#xA;from sklearn.ensemble import RandomForestClassifier&#xA;from spark_sklearn.util import createLocalSparkSession&#xA;from pyspark import SparkContext&#xA;&#xA;sc=SparkContext.getOrCreate()&#xA;digits = datasets.load_digits()&#xA;X, y = digits.data, digits.target&#xA;&#xA;param_grid = {&quot;max_depth&quot;: [3, None],&#xA;              &quot;max_features&quot;: [1, 3, 10],&#xA;              &quot;min_samples_split&quot;: [2, 3, 10],&#xA;              &quot;min_samples_leaf&quot;: [1, 3, 10],&#xA;              &quot;bootstrap&quot;: [True, False],&#xA;              &quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;],&#xA;              &quot;n_estimators&quot;: [10, 20, 40, 80]}&#xA;gs = GridSearchCV(sc,RandomForestClassifier(), param_grid=param_grid)&#xA;gs.fit(X, y)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Submitting the script&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark-submit --master spark://IP traingrid.py&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However I do not see any significant improvement in the execution time. &#xA;Is there any other configurations required to make it more parallel? Or I should add another node to improve it?&lt;/p&gt;&#xA;" OwnerUserId="903521" LastActivityDate="2018-03-05T08:56:26.277" Title="Spark performance in single node" Tags="&lt;apache-spark&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="49106672" PostTypeId="1" CreationDate="2018-03-05T09:00:10.693" Score="0" ViewCount="36" Body="&lt;p&gt;My OS is windows 10&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark.conf import SparkConf&#xA;sc = SparkContext.getOrCreate()&#xA;spark = SparkSession.builder.enableHiveSupport().getOrCreate()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This code gives me below error&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Py4JJavaError                             Traceback (most recent call&#xA;  last)&#xA;  ~\Documents\spark\spark-2.1.0-bin-hadoop2.7\python\pyspark\sql\utils.py&#xA;  in deco(*a, **kw)&#xA;       62         try:&#xA;  ---&gt; 63             return f(*a, **kw)&#xA;       64         except py4j.protocol.Py4JJavaError as e:&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;~\Documents\spark\spark-2.1.0-bin-hadoop2.7\python\lib\py4j-0.10.4-src.zip\py4j\protocol.py&#xA;  in get_return_value(answer, gateway_client, target_id, name)&#xA;      318                     &quot;An error occurred while calling {0}{1}{2}.\n&quot;.&#xA;  --&gt; 319                     format(target_id, &quot;.&quot;, name), value)&#xA;      320             else:&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Py4JJavaError: An error occurred while calling o22.sessionState. :&#xA;  java.lang.IllegalArgumentException: Error while instantiating&#xA;  'org.apache.spark.sql.hive.HiveSessionState':     at&#xA;  org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$reflect(SparkSession.scala:981)&#xA;    at&#xA;  org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:110)&#xA;    at&#xA;  org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:109)&#xA;    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at&#xA;  sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&#xA;    at&#xA;  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#xA;    at java.lang.reflect.Method.invoke(Method.java:606)     at&#xA;  py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)  at&#xA;  py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)    at&#xA;  py4j.Gateway.invoke(Gateway.java:280)     at&#xA;  py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#xA;    at py4j.commands.CallCommand.execute(CallCommand.java:79)   at&#xA;  py4j.GatewayConnection.run(GatewayConnection.java:214)    at&#xA;  java.lang.Thread.run(Thread.java:745) Caused by:&#xA;  java.lang.reflect.InvocationTargetException   at&#xA;  sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&#xA;    at&#xA;  sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)&#xA;    at&#xA;  sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#xA;    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)&#xA;    at&#xA;  org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$reflect(SparkSession.scala:978)&#xA;    ... 13 more Caused by: java.lang.IllegalArgumentException: Error&#xA;  while instantiating 'org.apache.spark.sql.hive.HiveExternalCatalog':&#xA;    at&#xA;  org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$reflect(SharedState.scala:169)&#xA;    at&#xA;  org.apache.spark.sql.internal.SharedState.(SharedState.scala:86)&#xA;    at&#xA;  org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)&#xA;    at&#xA;  org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:101)&#xA;    at scala.Option.getOrElse(Option.scala:121)     at&#xA;  org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:101)&#xA;    at&#xA;  org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:100)&#xA;    at&#xA;  org.apache.spark.sql.internal.SessionState.(SessionState.scala:157)&#xA;    at&#xA;  org.apache.spark.sql.hive.HiveSessionState.(HiveSessionState.scala:32)&#xA;    ... 18 more Caused by: java.lang.reflect.InvocationTargetException&#xA;    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native&#xA;  Method)   at&#xA;  sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)&#xA;    at&#xA;  sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#xA;    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)&#xA;    at&#xA;  org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$reflect(SharedState.scala:166)&#xA;    ... 26 more Caused by: java.lang.reflect.InvocationTargetException&#xA;    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native&#xA;  Method)   at&#xA;  sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)&#xA;    at&#xA;  sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&#xA;    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)&#xA;    at&#xA;  org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:264)&#xA;    at&#xA;  org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:366)&#xA;    at&#xA;  org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:270)&#xA;    at&#xA;  org.apache.spark.sql.hive.HiveExternalCatalog.(HiveExternalCatalog.scala:65)&#xA;    ... 31 more Caused by: java.lang.RuntimeException:&#xA;  java.lang.RuntimeException: Unable to instantiate&#xA;  org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient     at&#xA;  org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)&#xA;    at&#xA;  org.apache.spark.sql.hive.client.HiveClientImpl.(HiveClientImpl.scala:192)&#xA;    ... 39 more Caused by: java.lang.RuntimeException: Unable to&#xA;  instantiate&#xA;  org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;My full code is here &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark.sql import SQLContext &#xA;from pyspark.sql import SparkSession &#xA;import findspark &#xA;findspark.init('C:/Users/asus/Documents/spark/spark-2.1.0-bin-hadoop2.7') &#xA;import pyspark from pyspark.conf &#xA;import SparkConf sc = SparkContext.getOrCreate() &#xA;spark = SparkSession.builder.enableHiveSupport().getOrCreate()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="9444768" LastEditorUserId="9444768" LastEditDate="2018-03-05T10:22:59.613" LastActivityDate="2018-03-05T13:00:40.963" Title="Sparksession error is about hive" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;hive&gt;&lt;pyspark&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="49107104" PostTypeId="1" CreationDate="2018-03-05T09:28:09.460" Score="1" ViewCount="44" Body="&lt;p&gt;When use spark sql to read jdbc data, spark will start only 1 partition in default.  But when table is too big, spark will read very slow.&lt;br&gt;&#xA;I know there are two ways to make partitions :&lt;br&gt;&#xA;1. set partitionColumn,lowerBound，upperBound and numPartitions in option;&lt;br&gt;&#xA;2. set an array of offsets in option;&lt;br&gt;&#xA;But my situation is :&lt;br&gt;&#xA;  My jdbc table have no INT column or string column can easily separated by offsets for these two ways.&lt;br&gt;&#xA;With these 2 ways won't work in my situation, is there any others ways to manage spark read jdbc data partitionally?   &lt;/p&gt;&#xA;" OwnerUserId="9444881" LastEditorUserId="9444881" LastEditDate="2018-03-06T02:09:58.943" LastActivityDate="2018-03-06T02:09:58.943" Title="Other ways to make spark read jdbc partitionly" Tags="&lt;apache-spark&gt;&lt;jdbc&gt;" AnswerCount="0" CommentCount="7" />
  <row Id="49107459" PostTypeId="1" CreationDate="2018-03-05T09:47:56.953" Score="0" ViewCount="8" Body="&lt;p&gt;I trained a spark rdd based gbdt mllib model, but according to spark document, PMML model export - RDD-based API&#xA;&lt;a href=&quot;https://spark.apache.org/docs/2.1.1/mllib-pmml-model-export.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://spark.apache.org/docs/2.1.1/mllib-pmml-model-export.html&lt;/a&gt;,&#xA;only KMeansModel, LinearRegressionModel,&#xA;RidgeRegressionModel, LassoModel, SVMModel, Binary LogisticRegressionModel can be converted to pmml model.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also tried JPMML-SparkML, but it can only convert Spark ML pipelines to PMML.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So What can I do?&lt;/p&gt;&#xA;" OwnerUserId="9444883" LastEditorUserId="9444994" LastEditDate="2018-03-05T10:01:39.337" LastActivityDate="2018-03-05T10:01:39.337" Title="How to convert spark rdd based gbdt mllib model to pmml model?" Tags="&lt;apache-spark-mllib&gt;&lt;pmml&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49107731" PostTypeId="1" CreationDate="2018-03-05T10:02:12.330" Score="0" ViewCount="15" Body="&lt;p&gt;I am learning spark sql and noticed that this is possible: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SELECT   a, b, &#xA;         Row_number() OVER (partition BY a, b ORDER BY start_time DESC ) AS r ,&#xA;         Count(*) OVER (partition BY a, b)                       AS count&#xA;FROM     tbl &#xA;WHERE    ...  &#xA;**HAVING r &amp;lt;= 10**&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As far as I know, having clause is something that can be applied only to an aggregation in a group-by clause. Impala does not recognise this syntax, nor is it documented in the only &lt;a href=&quot;https://stackoverflow.com/questions/30887571/spark-sql-syntax-reference#&quot;&gt;reference I was able to find&lt;/a&gt; for spark sql.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What's up with that? Are the semantics the same as putting the same condition in a where clause in an outer query (like I normally would?)&lt;/p&gt;&#xA;" OwnerUserId="180650" LastActivityDate="2018-03-05T21:19:25.967" Title="What are the semantics of using a having clause on a window function in spark sql?" Tags="&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;having&gt;&lt;having-clause&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="49108050" PostTypeId="1" CreationDate="2018-03-05T10:16:40.110" Score="0" ViewCount="29" Body="&lt;p&gt;I have a a shapefile containing multiple features and I wish to read it into an RDD format. So far, I've got a &lt;code&gt;Seq[MultiPolygonFeature[Map[String,Object]]]&lt;/code&gt; using &lt;code&gt;ShapefileReader.readMultiPolygonFeatures&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to convert it into something like a &lt;code&gt;RDD[(SpatialKey, Iterable[MultiPolygonFeature[UUID]])]&lt;/code&gt; or even just a &lt;code&gt;RDD[Feature[Polygon, UUID]]&lt;/code&gt; type and work from there. I've been trying to look for examples, but they always seem to start with a pre-populated RDD of polygons beforehand.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def readShapefileToRDD(filepath: String): RDD[(SpatialKey, Iterable[MultiPolygonFeature[UUID]])] = {&#xA;  val features : Seq[MultiPolygonFeature[Map[String,Object]]] = ShapeFileReader.readMultiPolygonFeatures(filepath)&#xA;&#xA;  val groupedPolys: RDD[(SpatialKey, Iterable[MultiPolygonFeature[UUID]])] = ???&#xA;&#xA;  return groupedPolys&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="4518355" LastEditorUserId="4518355" LastEditDate="2018-03-06T03:24:22.957" LastActivityDate="2018-03-06T03:24:22.957" Title="Geotrellis: Reading a Multi-Feature Shapefile into a Spark RDD" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;gis&gt;&lt;geotrellis&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49108114" PostTypeId="1" CreationDate="2018-03-05T10:19:41.153" Score="0" ViewCount="46" Body="&lt;p&gt;There're billions of intervals in format &lt;code&gt;[a, b]&lt;/code&gt;, and all of them will cut the number space into multiple single pieces. I intend to output all single pieces with the number of overlapped intervals within this piece. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance: there are 3 intervals, namely: [1,7], [2,3], [6, 8]. It should output result as below:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;[-∞, 1]: 0&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;[1, 2]: 1&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;[2, 3]: 2&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;[3, 6]: 1&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;[6, 7]: 2&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;[7, 8]: 1&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;[8, +∞]: 0&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;If for a single machine (not a distributed solution as in MapReduce), I know the solution could be break down the interval instance into &lt;code&gt;start_n&lt;/code&gt;, &lt;code&gt;end_n&lt;/code&gt;, sort by the number and iterate from left to right and use a counter to count the amount in current piece and output. But I'm not sure how this algorithm could be splitted into a distributed way. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any suggestions? Thanks.&lt;/p&gt;&#xA;" OwnerUserId="6594488" LastEditorUserId="6594488" LastEditDate="2018-03-06T01:51:49.150" LastActivityDate="2018-03-06T06:53:49.387" Title="Best MapReduce Algorithm to calculate the number of every single overlapped intervals" Tags="&lt;java&gt;&lt;hadoop&gt;&lt;apache-spark&gt;&lt;mapreduce&gt;&lt;distributed-computing&gt;" AnswerCount="2" CommentCount="3" FavoriteCount="1" />
  <row Id="49108243" PostTypeId="1" CreationDate="2018-03-05T10:25:14.920" Score="0" ViewCount="60" Body="&lt;p&gt;I am connected via &lt;code&gt;jdbc&lt;/code&gt; to a DB having 500'000'000 of rows and 14 columns.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the code used:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark.sql import SparkSession&#xA;&#xA;spark = SparkSession.builder.getOrCreate()&#xA;&#xA;properties = {'jdbcurl': 'jdbc:db:XXXXXXXXX','user': 'XXXXXXXXX', 'password': 'XXXXXXXXX'}&#xA;&#xA;data = spark.read.jdbc(properties['jdbcurl'], table='XXXXXXXXX', properties=properties)&#xA;&#xA;data.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The code above took &lt;strong&gt;9 seconds&lt;/strong&gt; to display the first 20 rows of the DB.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Later I created a SQL temporary view via&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;data[['XXX','YYY']].createOrReplaceTempView(&quot;ZZZ&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and I ran the following query:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sqlContext.sql('SELECT AVG(XXX) FROM ZZZ').show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The code above took &lt;strong&gt;1355.79 seconds&lt;/strong&gt; (circa 23 minutes). Is this ok? It seems to be a large amount of time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In the end I tried to count the number of rows of the DB&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sqlContext.sql('SELECT COUNT(*) FROM ZZZ').show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It took &lt;strong&gt;2848.95 seconds&lt;/strong&gt; (circa 48 minutes).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Am I doing something wrong or are these amounts standard?&lt;/p&gt;&#xA;" OwnerUserId="9015878" LastEditorUserId="9441142" LastEditDate="2018-03-05T11:48:40.297" LastActivityDate="2018-03-05T19:09:48.370" Title="Spark (pyspark) speed test" Tags="&lt;apache-spark&gt;&lt;parallel-processing&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="7" />
  <row Id="49108386" PostTypeId="1" CreationDate="2018-03-05T10:32:46.390" Score="0" ViewCount="29" Body="&lt;p&gt;Trying to understand how to use the Spark Global Temporary Views. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In one spark-shell session I've created a view &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark = SparkSession.builder.appName('spark_sql').getOrCreate()&#xA;&#xA;df = (&#xA;spark.read.option(&quot;header&quot;, &quot;true&quot;)&#xA;    .option(&quot;delimiter&quot;, &quot;,&quot;)&#xA;    .option(&quot;inferSchema&quot;, &quot;true&quot;)&#xA;    .csv(&quot;/user/root/data/cars.csv&quot;))&#xA;&#xA;df.createGlobalTempView(&quot;my_cars&quot;)&#xA;&#xA;# works without any problem&#xA;spark.sql(&quot;SELECT * FROM global_temp.my_cars&quot;).show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And on another I tried to access it, without success (table or view not found). &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; #second Spark Shell &#xA; spark = SparkSession.builder.appName('spark_sql').getOrCreate()&#xA; spark.sql(&quot;SELECT * FROM global_temp.my_cars&quot;).show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;That's the error I receive : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; pyspark.sql.utils.AnalysisException: u&quot;Table or view not found: `global_temp`.`my_cars`; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation `global_temp`.`my_cars`\n&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I've read that each spark-shell has its own context, and that's why one spark-shell cannot see the other. So I don't understand, what's the usage of the GTV, where will it be useful ? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;" OwnerUserId="6089016" LastEditorUserId="1305344" LastEditDate="2018-03-05T19:05:42.733" LastActivityDate="2018-03-05T19:05:42.733" Title="What is the purpose of global temporary views?" Tags="&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;pyspark-sql&gt;" AnswerCount="0" CommentCount="6" />
  <row Id="49108446" PostTypeId="1" CreationDate="2018-03-05T10:36:04.173" Score="0" ViewCount="7" Body="&lt;p&gt;I am using &quot;cloudera-quickstart-vm-5.7.0-0-virtualbox&quot;. It provides big data related framework. I am using spark environment in that vm. &quot;pyspark&quot; is working fine. I want to use pandas  and some other anaconda modules inside pyspark environment. As pandas module is not available in that  vm, &quot;import pandas as pd&quot; statement is raising an exception. So i want to install anaconda in my vm. I am looking for support in this issue.&#xA;Thanks in advance&lt;/p&gt;&#xA;" OwnerUserId="3836120" LastActivityDate="2018-03-05T10:36:04.173" Title="how to setup anaconda parcel on cloudera quickstart vm" Tags="&lt;pandas&gt;&lt;pyspark&gt;&lt;anaconda&gt;&lt;cloudera-quickstart-vm&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49108541" PostTypeId="1" CreationDate="2018-03-05T10:40:27.463" Score="0" ViewCount="29" Body="&lt;p&gt;I am loading a csv file having 1 million records using pyspark, but getting the error. TextParsingException: Length of parsed input (1000001) exceeds the maximum number of characters defined in your parser settings (1000000)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I checked if any of my record in the file has data greater than 1000000 characters, but none of the record is like that. maximum record length in my file is 850.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please help.... &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;CODE SNIPPET:&#xA;        input_df =  spark.read.format('com.databricks.spark.csv').option(&quot;delimiter&quot;,&quot;\001&quot;).option(&quot;quote&quot;,u&quot;\u0000&quot;).load(INPUT_PATH)&#xA;        input_df.write.mode('overwrite').format('orc').save(TARGET_LOC)&#xA;&#xA;    SAMPLE DATA&#xA;&#xA;        A    B     C&#xA;        --   --    --&#xA;        a   xyz&quot;a  123&#xA;        b   pqr    456&#xA;        c   ABC&quot;z  789&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="8030630" LastEditorUserId="8030630" LastEditDate="2018-03-05T11:13:11.713" LastActivityDate="2018-03-05T12:12:19.217" Title="Pyspark TextParsingException while loading a file" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="6" FavoriteCount="1" />
  <row Id="49109152" PostTypeId="1" AcceptedAnswerId="49109576" CreationDate="2018-03-05T11:11:42.700" Score="1" ViewCount="35" Body="&lt;p&gt;In &lt;a href=&quot;https://stackoverflow.com/questions/33655920/when-to-use-mapparitions-and-mappartitionswithindex&quot;&gt;SO 33655920&lt;/a&gt; I come across the below, fine.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;rdd = sc.parallelize([1, 2, 3, 4], 2)&#xA;def f(iterator): yield sum(iterator)&#xA;rdd.mapPartitions(f).collect()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In Scala, I cannot seem to get the the def in the same shorthand way. The equivalent is? I have searched and tried but to no avail.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance.&lt;/p&gt;&#xA;" OwnerUserId="6933993" LastEditorUserId="86485" LastEditDate="2018-03-05T23:11:23.517" LastActivityDate="2018-03-05T23:11:23.517" Title="Spark Scala def with yield" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49109476" PostTypeId="1" CreationDate="2018-03-05T11:29:50.187" Score="0" ViewCount="26" Body="&lt;p&gt;Did a fresh installation of Laravel Spark 6.0 &#xA;Not able to see the billing block, did I miss something in configuration. Please see the image below:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/fvA0u.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/fvA0u.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="6808774" LastEditorUserId="294867" LastEditDate="2018-03-05T11:41:50.673" LastActivityDate="2018-03-05T15:29:33.587" Title="Laravel billing block not showing" Tags="&lt;laravel&gt;&lt;laravel-spark&gt;" AnswerCount="1" CommentCount="9" />
  <row Id="49109521" PostTypeId="1" AcceptedAnswerId="49110268" CreationDate="2018-03-05T11:32:00.547" Score="0" ViewCount="36" Body="&lt;p&gt;Given that the following code is well understood:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val rdd = sc.parallelize(List((&quot;A&quot;, List(1, 1)), &#xA;                              (&quot;B&quot;, List(2, 2, 2, 200)), &#xA;                              (&quot;C&quot;, List(3, 3)),&#xA;                              (&quot;D&quot;, List(2, 2)),&#xA;                              (&quot;A&quot;, List(1, 1, 1)),&#xA;                              (&quot;B&quot;, List(1, 1, 1)),&#xA;                              (&quot;P&quot;, List(1, 1, 1))                             &#xA;                         ),3)&#xA;rdd.flatMap(_._2).sum&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As well as: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val mapped =   rdd.mapPartitionsWithIndex{&#xA;                    (index, iterator) =&amp;gt; {&#xA;                       println(&quot;Called in Partition -&amp;gt; &quot; + index)&#xA;                       val myList = iterator.toList&#xA;                       // In a normal user case, we will do the&#xA;                       // the initialization(ex : initializing database)&#xA;                       // before iterating through each element&#xA;                       myList.map(x =&amp;gt; x + &quot; -&amp;gt; &quot; + index).iterator&#xA;&#xA;                    }&#xA;                 }&#xA;  mapped.collect()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then, for argument's sake - i.e. may be a bad example, but none-the-less, how can I apply the &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;rdd.flatMap(_._2).sum&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;similarly in conjunction with mapPartitions or mapPartitionsWithIndex? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I get an error every time, due to Iterator I think. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;This may well bring tie a few things together for me. I think it is simply not possible, but would like to confirm that. &lt;/p&gt;&#xA;" OwnerUserId="6933993" LastActivityDate="2018-03-05T12:13:20.293" Title="SPARK mapPartitions summing within Partition" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="49109867" PostTypeId="1" AcceptedAnswerId="49123318" CreationDate="2018-03-05T11:50:26.440" Score="0" ViewCount="49" Body="&lt;p&gt;I am using Ubuntu and I am trying to connect spark with Cassandra I used the following steps.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/datastax/spark-cassandra-connector.git&#xA;cd spark-cassandra-connector&#xA;./sbt/sbt assembly&#xA;./spark-shell --jars ~/spark/jars/spark-cassandra-connector-assembly-1.4.0-SNAPSHOT.jar&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And After this I tried this &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Scala&amp;gt; sc.stop&#xA;Scala&amp;gt; import com.datastax.spark.connector._&#xA;Scala&amp;gt; org.apache.spark.SparkContext&#xA;Scala&amp;gt; import org.apache.spark.SparkContext._&#xA;Scala import org.apache.spark.SparkConf&#xA;Scala&amp;gt; val conf = new SparkConf(true).set(&quot;spark.cassandra.connection.host&quot;, &quot;localhost&quot;)&#xA;Scala&amp;gt; val sc = new SparkContext(conf)&#xA;Scala&amp;gt; val test_spark_rdd = sc.cassandraTable(&quot;keyspace&quot;, &quot;table&quot;) &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am using spark 2.2.1  and my Cassandra is apache-cassandra-2.2.12&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I enter this command &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Scala&amp;gt; val test_spark_rdd = sc.cassandraTable(&quot;keyspace&quot;, &quot;table&quot;) &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;it gives me this error.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;error: missing or invalid dependency detected while loading class file 'CassandraConnector.class'.&#xA;  Could not access type Logging in package org apache spark,&#xA;  because it (or its dependencies) are missing. Check your build definition for&#xA;  missing or conflicting dependencies. (Re-run with Ylog classpath to see the problematic classpath.)&#xA;  A full rebuild may help if 'CassandraConnector class' was compiled against an incompatible version of org apache spark.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I Find different tutorial but I am not able to solve my issue, is someone will give me suggestion. Thanks &lt;/p&gt;&#xA;" OwnerUserId="6450118" LastEditorUserId="6450118" LastEditDate="2018-03-05T11:56:00.970" LastActivityDate="2018-03-06T04:18:46.393" Title="How to connect spark with Cassandra" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;cassandra&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="49109967" PostTypeId="1" CreationDate="2018-03-05T11:54:48.140" Score="2" ViewCount="11" Body="&lt;p&gt;I want to run Spark with Kryo serialisation. Therefore I set &lt;code&gt;spark.serializer=org.apache.spark.serializer.KryoSerializer&lt;/code&gt; and &lt;code&gt;spark.kryo.registrationRequired=true&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I then run my code I get the error:  &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Class is not registered: org.apache.spark.sql.catalyst.InternalRow[]&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;According to &lt;a href=&quot;https://stackoverflow.com/questions/22027451/kryo-serialization-refuses-to-register-class&quot; title=&quot;This post&quot;&gt;this post&lt;/a&gt; I used &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sc.getConf.registerKryoClasses(Array( classOf[ org.apache.spark.sql.catalyst.InternalRow[_] ] ))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But then the error is:  &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;org.apache.spark.sql.catalyst.InternalRow does not take type parameters&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="6162321" LastEditorUserId="6162321" LastEditDate="2018-03-05T12:00:55.677" LastActivityDate="2018-03-05T12:00:55.677" Title="How to register InternalRow with Kryo in Spark" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;hadoop2&gt;&lt;kryo&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49110054" PostTypeId="1" CreationDate="2018-03-05T11:59:28.577" Score="1" ViewCount="29" Body="&lt;p&gt;I am interfacing Elasticsearch with Spark, using the Elasticsearch-Hadoop plugin and I am having difficulty writing a dataframe with a &lt;code&gt;timestamp&lt;/code&gt; type column to Elasticsearch. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is when I try to write using dynamic/multi resource formatting to create a daily index. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;From the &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html#cfg-multi-writes-format&quot; rel=&quot;nofollow noreferrer&quot;&gt;relevant documentation&lt;/a&gt; I get the impression that this is possible, however, the python example below fails to run unless I change my dataframe type to &lt;code&gt;date&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pyspark&#xA;conf = pyspark.SparkConf()&#xA;conf.set('spark.jars', 'elasticsearch-spark-20_2.11-6.1.2.jar')&#xA;conf.set('es.nodes', '127.0.0.1:9200')&#xA;conf.set('es.read.metadata', 'true')&#xA;conf.set('es.nodes.wan.only', 'true')&#xA;from datetime import datetime, timedelta&#xA;now = datetime.now()&#xA;before = now - timedelta(days=1)&#xA;after = now + timedelta(days=1)&#xA;cols = ['idz', 'name', 'time']&#xA;vals = [(0,'maria', before), (1, 'lolis', after)]  &#xA;time_df = spark.createDataFrame(vals, cols)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I try to write, I use the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;time_df.write.mode('append').format(&#xA;    'org.elasticsearch.spark.sql'&#xA;).options(&#xA;    **{'es.write.operation': 'index' }&#xA;).save('xxx-{time|yyyy.MM.dd}/1')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Unfortunatelly this renders an error:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;.... Caused by: java.lang.IllegalArgumentException: Invalid format:&#xA;  &quot;2018-03-04 12:36:12.949897&quot; is malformed at &quot; 12:36:12.949897&quot;   at&#xA;  org.joda.time.format.DateTimeFormatter.parseDateTime(DateTimeFormatter.java:945)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;On the other hand this works perfectly fine if I use dates when I create my dataframe:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cols = ['idz', 'name', 'time']&#xA;vals = [(0,'maria', before.date()), (1, 'lolis', after.date())]  &#xA;time_df = spark.createDataFrame(vals, cols)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is it possible to format a dataframe &lt;code&gt;timestamp&lt;/code&gt; to be written to daily indexes with this method, without also keeping a &lt;code&gt;date&lt;/code&gt; column around? How about monthly indexes?&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Pyspark version:&#xA;  spark version 2.2.1&#xA;  Using Scala version 2.11.8, OpenJDK 64-Bit Server VM, 1.8.0_151&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;ElasticSearch version &#xA;  number    &quot;6.2.2&quot; build_hash  &quot;10b1edd&quot;&#xA;  build_date    &quot;2018-02-16T19:01:30.685723Z&quot; build_snapshot    false&#xA;  lucene_version    &quot;7.2.1&quot; minimum_wire_compatibility_version  &quot;5.6.0&quot;&#xA;  minimum_index_compatibility_version   &quot;5.0.0&quot;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="3607939" LastActivityDate="2018-03-05T11:59:28.577" Title="Elasticsearch-Hadoop formatting multi resouce writes issue" Tags="&lt;python-2.7&gt;&lt;elasticsearch&gt;&lt;pyspark&gt;&lt;apache-spark-sql&gt;&lt;elasticsearch-hadoop&gt;" AnswerCount="0" CommentCount="0" FavoriteCount="2" />
  <row Id="49110384" PostTypeId="1" CreationDate="2018-03-05T12:19:27.343" Score="4" ViewCount="107" Body="&lt;p&gt;I have code along the lines of:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val lines: RDD[String] = sparkSession.sparkContext.textFile(&quot;s3://mybucket/file.gz&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The URL ends in &lt;code&gt;.gz&lt;/code&gt; but this is a result of legacy code. The file is plain text with no compression involved. However spark insists on reading it as a GZIP file which obviously fails. How can I make it ignore the extension and simply read the file as text?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Based on &lt;a href=&quot;https://medium.com/@samuelboyd/spark-reading-gzipped-files-from-s3-without-the-gz-extension-58cc78e6a09d&quot; rel=&quot;nofollow noreferrer&quot;&gt;this article&lt;/a&gt; I've tried setting configuration in various places that doesn't include the GZIP codec, e.g.:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sparkContext.getConf.set(&quot;spark.hadoop.io.compression.codecs&quot;, classOf[DefaultCodec].getCanonicalName)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This doesn't seem to have any effect.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since the files are on S3, I can't simply rename them without copying the entire file.&lt;/p&gt;&#xA;" OwnerUserId="2482744" LastActivityDate="2018-03-05T19:14:53.080" Title="How can I force spark/hadoop to ignore the .gz extension on a file and read it as uncompressed plain text?" Tags="&lt;scala&gt;&lt;hadoop&gt;&lt;apache-spark&gt;&lt;gzip&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="2" />
  <row Id="49110505" PostTypeId="1" CreationDate="2018-03-05T12:26:31.787" Score="0" ViewCount="31" Body="&lt;p&gt;&lt;strong&gt;Problem Statement:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have 2 RDDs A and B.&#xA;A has two timestamp fields A_Start and A_end.&#xA;B has two timestamp fields B_Start and B_end.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now how do i join A and B such that A.A_Start &gt; B.B_start and A.A_end &amp;lt;= B.B_end.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;A and B are large datasets.  &lt;/li&gt;&#xA;&lt;li&gt;A and B do not have a common column.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;I can think of one solution where cartesian product can be taken and filter applied, but this may not be perform efficiently for large datasets.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;DataSet A&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/TtVrA.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/TtVrA.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;DataSet B&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/xiJVp.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/xiJVp.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Result&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/TIevE.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/TIevE.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="1605937" LastEditorUserId="1605937" LastEditDate="2018-03-05T13:46:39.400" LastActivityDate="2018-03-05T13:46:39.400" Title="Spark RDD join based on timestamps" Tags="&lt;apache-spark&gt;&lt;join&gt;&lt;date-range&gt;" AnswerCount="0" CommentCount="1" FavoriteCount="1" />
  <row Id="49110803" PostTypeId="1" AcceptedAnswerId="49113823" CreationDate="2018-03-05T12:43:52.857" Score="0" ViewCount="59" Body="&lt;p&gt;I have the following csv file.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Index,Arrival_Time,Creation_Time,x,y,z,User,Model,Device,gt&#xA;0,1424696633908,1424696631913248572,-5.958191,0.6880646,8.135345,a,nexus4,nexus4_1,stand&#xA;1,1424696633909,1424696631918283972,-5.95224,0.6702118,8.136536,a,nexus4,nexus4_1,stand&#xA;2,1424696633918,1424696631923288855,-5.9950867,0.6535491999999999,8.204376,a,nexus4,nexus4_1,stand&#xA;3,1424696633919,1424696631928385290,-5.9427185,0.6761626999999999,8.128204,a,nexus4,nexus4_1,stand&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have to create a RDD where USER MODEL AND GT are PRIMARY KEY, I don't know if I have to do it using them as a tuple.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then when I have the primary key field I have to calculate AVG, MAX and MIN from 'x','y' and 'z'.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is an output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;User,Model,gt,media(x,y,z),desviacion(x,y,z),max(x,y,z),min(x,y,z)&#xA;a, nexus4,stand,-3.0,0.7,8.2,2.8,0.14,0.0,-1.0,0.8,8.2,-5.0,0.6,8.2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any idea about how to group them and for example get the media values from &quot;x&quot; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;With my current code I get the following.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# Data loading&#xA;&#xA;    lectura = sc.textFile(&quot;Phones_accelerometer.csv&quot;)&#xA;&#xA;    datos = lectura.map(lambda x: ((x.split(&quot;,&quot;)[6], x.split(&quot;,&quot;)[7], x.split(&quot;,&quot;)[9]),(x.split(&quot;,&quot;)[3], x.split(&quot;,&quot;)[4], x.split(&quot;,&quot;)[5])))&#xA;&#xA;    sumCount = datos.combineByKey(lambda value: (value, 1), lambda x, value: (x[0] + value, x[1] + 1), lambda x, y: (x[0] + y[0], x[1] + y[1]))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;An example of my tuples: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;   [(('a', 'nexus4', 'stand'), ('-5.958191', '0.6880646', '8.135345'))]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="9326486" LastEditorUserId="9326486" LastEditDate="2018-03-05T12:51:37.373" LastActivityDate="2018-03-05T15:26:37.563" Title="pyspark - Grouping and calculating data" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;rdd&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="49110972" PostTypeId="1" CreationDate="2018-03-05T12:54:29.313" Score="0" ViewCount="27" Body="&lt;p&gt;FileA has data like this with start and end time stamps as the last two columns&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;dataa, data1, 9:10, 9:15&#xA;datab, data2, 10:00, 10:10&#xA;datac, data3, 11:20, 11:30&#xA;datad, data4, 12:30, 12:40&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;FileB has data like this with start and end time stamps as the last two columns&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;dataaa, data11, 9:13, 9:17&#xA;databb, data22, 10:02, 10:08&#xA;datacc, data33, 6:20, 6:30&#xA;datadd, data44, 12:31, 12:35&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Perform a join between this two file, which should result the following from FileB,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;databb, data22, 10:02, 10:08&#xA;datadd, data44, 12:31, 12:35&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The criteria for join is the start time of FileB should be greater than start time of FileA whereas end time of FileB should be less than start time of FileA.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;how to to write the code it in spark-sql.?&lt;/p&gt;&#xA;" OwnerUserId="6842300" LastEditorUserId="5880706" LastEditDate="2018-03-05T17:34:55.413" LastActivityDate="2018-03-05T17:35:07.077" Title="Writing query to fetch records from two file in spark based on timestamps criteria" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="49111073" PostTypeId="1" CreationDate="2018-03-05T12:59:52.697" Score="0" ViewCount="53" Body="&lt;p&gt;I would like to use spark structured streaming to watch a drop location that exists on the driver only. I do this with&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val trackerData = spark.readStream.text(sourcePath)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;After that I would like to parse, filter, and map incoming data and write it out to elastic.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This works well except that it does only work when &lt;em&gt;spark.master&lt;/em&gt; is set to e.g. &lt;em&gt;local[&lt;/em&gt;]*. When set to &lt;em&gt;yarn&lt;/em&gt;, no files get found even when deployment mode is set to &lt;em&gt;client&lt;/em&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I thought that reading data in from local driver node is achieved by setting deployment to client and doing the actual processing and writing within the spark cluster.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How could I improve my code to use driver for reading in and cluster for processing and writing?&lt;/p&gt;&#xA;" OwnerUserId="7804338" LastEditorUserId="1305344" LastEditDate="2018-03-05T19:02:43.773" LastActivityDate="2018-03-06T07:52:56.600" Title="How to use driver to load data and executors for processing and writing?" Tags="&lt;apache-spark&gt;&lt;structured-streaming&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="49111512" PostTypeId="1" CreationDate="2018-03-05T13:25:06.790" Score="0" ViewCount="9" Body="&lt;p&gt;I have a data-set of size 10 Petabytes. my Current data is in Hbase where I am using Spark HbaseContext but it is not performing well. Will it be useful to move data from HbaseContext to HiveContext on Spark?&lt;/p&gt;&#xA;" OwnerUserId="4333248" LastActivityDate="2018-03-05T13:25:06.790" Title="Spark HiveContext vs HbaseContext?" Tags="&lt;apache-spark&gt;&lt;hivecontext&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49111793" PostTypeId="1" CreationDate="2018-03-05T13:40:33.560" Score="0" ViewCount="33" Body="&lt;p&gt;I would like to know if there is a way to stop a job (and have it in a FAILED or KILLED state) when I detect something wrong within a map or a reduce task without Hadoop retries the task.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If possible I would like to keep the fact that on &quot;normal&quot; fails Yarn restart the task.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently I am throwing an exception but Hadoop tries again.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is Scala/Spark code but It may be useful in Java/Hadoop too.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;" OwnerUserId="5877122" LastEditorUserId="5877122" LastEditDate="2018-03-05T15:14:20.213" LastActivityDate="2018-03-05T15:14:20.213" Title="Hadoop / Spark: kill a task and do not retry" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;yarn&gt;" AnswerCount="0" CommentCount="6" FavoriteCount="1" />
  <row Id="49112252" PostTypeId="1" CreationDate="2018-03-05T14:06:06.787" Score="0" ViewCount="18" Body="&lt;p&gt;I am trying to create Apache Spark job to consume Kafka messages submitted in to a topic. To submit messages to the topic using kafka-console-producer as below.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;./kafka-console-producer.sh --broker-list kafka1:9092 --topic my-own-topic&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;To read messages I am using spark-streaming-kafka-0-10_2.11 library. With the library manage to to read the total counts of the messages received to the topic. But I can not read ConsumerRecord object in the stream and when I try to read it entire application get blocked and can not print it in to the console. Note I am running Kafka, Zookeeper and Spark in docker containers. Help would be greatly appreciated.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-java prettyprint-override&quot;&gt;&lt;code&gt;import java.util.Arrays;&#xA;import java.util.Collection;&#xA;import java.util.HashMap;&#xA;import java.util.Map;&#xA;import org.apache.kafka.clients.consumer.ConsumerRecord;&#xA;import org.apache.kafka.common.serialization.StringDeserializer;&#xA;import org.apache.spark.SparkConf;&#xA;import org.apache.spark.TaskContext;&#xA;import org.apache.spark.api.java.JavaRDD;&#xA;import org.apache.spark.streaming.Durations;&#xA;import org.apache.spark.streaming.api.java.JavaInputDStream;&#xA;import org.apache.spark.streaming.api.java.JavaStreamingContext;&#xA;import org.apache.spark.streaming.kafka010.ConsumerStrategies;&#xA;import org.apache.spark.streaming.kafka010.HasOffsetRanges;&#xA;import org.apache.spark.streaming.kafka010.KafkaUtils;&#xA;import org.apache.spark.streaming.kafka010.LocationStrategies;&#xA;import org.apache.spark.streaming.kafka010.OffsetRange;&#xA;&#xA;public class SparkKafkaStreamingJDBCExample {&#xA;&#xA;  public static void main(String[] args) {&#xA;&#xA;    // Start a spark instance and get a context&#xA;    SparkConf conf =&#xA;        new SparkConf().setAppName(&quot;Study Spark&quot;).setMaster(&quot;spark://spark-master:7077&quot;);&#xA;&#xA;    // Setup a streaming context.&#xA;    JavaStreamingContext streamingContext = new JavaStreamingContext(conf, Durations.seconds(3));&#xA;&#xA;    // Create a map of Kafka params&#xA;    Map&amp;lt;String, Object&amp;gt; kafkaParams = new HashMap&amp;lt;String, Object&amp;gt;();&#xA;    // List of Kafka brokers to listen to.&#xA;    kafkaParams.put(&quot;bootstrap.servers&quot;, &quot;kafka1:9092&quot;);&#xA;    kafkaParams.put(&quot;key.deserializer&quot;, StringDeserializer.class);&#xA;    kafkaParams.put(&quot;value.deserializer&quot;, StringDeserializer.class);&#xA;    kafkaParams.put(&quot;group.id&quot;, &quot;use_a_separate_group_id_for_each_stream&quot;);&#xA;    // Do you want to start from the earliest record or the latest?&#xA;    kafkaParams.put(&quot;auto.offset.reset&quot;, &quot;earliest&quot;);&#xA;    kafkaParams.put(&quot;enable.auto.commit&quot;, true);&#xA;&#xA;    // List of topics to listen to.&#xA;    Collection&amp;lt;String&amp;gt; topics = Arrays.asList(&quot;my-own-topic&quot;);&#xA;&#xA;    // Create a Spark DStream with the kafka topics.&#xA;    final JavaInputDStream&amp;lt;ConsumerRecord&amp;lt;String, String&amp;gt;&amp;gt; stream =&#xA;        KafkaUtils.createDirectStream(streamingContext, LocationStrategies.PreferConsistent(),&#xA;            ConsumerStrategies.&amp;lt;String, String&amp;gt;Subscribe(topics, kafkaParams));&#xA;&#xA;    System.out.println(&quot;Study Spark Example Starting ....&quot;);&#xA;&#xA;    stream.foreachRDD(rdd -&amp;gt; {&#xA;&#xA;      if (rdd.isEmpty()) {&#xA;        System.out.println(&quot;RDD Empty &quot; + rdd.count());&#xA;        return;&#xA;      } else {&#xA;        System.out.println(&quot;RDD not empty &quot; + rdd.count());&#xA;&#xA;        OffsetRange[] offsetRanges = ((HasOffsetRanges) rdd.rdd()).offsetRanges();&#xA;        System.out.println(&quot;Partition Id &quot; + TaskContext.getPartitionId());&#xA;        OffsetRange o = offsetRanges[TaskContext.getPartitionId()];&#xA;        System.out.println(&quot;Topic &quot; + o.topic());&#xA;&#xA;        System.out.println(&quot;Creating RDD !!!&quot;);&#xA;        JavaRDD&amp;lt;ConsumerRecord&amp;lt;String, String&amp;gt;&amp;gt; r =&#xA;            KafkaUtils.createRDD(streamingContext.sparkContext(), kafkaParams, offsetRanges,&#xA;                LocationStrategies.PreferConsistent());&#xA;        System.out.println(&quot;Count &quot; + r.count());&#xA;    //Application stuck from here onwards ...&#xA;    ConsumerRecord&amp;lt;String, String&amp;gt; first = r.first();&#xA;    System.out.println(&quot;First taken&quot;);&#xA;    System.out.println(&quot;First value &quot; + first.value());&#xA;&#xA;&#xA;      }&#xA;    });&#xA;&#xA;    System.out.println(&quot;Stream context starting ...&quot;);&#xA;    // Start streaming.&#xA;    streamingContext.start();&#xA;    System.out.println(&quot;Stream context started ...&quot;);&#xA;&#xA;    try {&#xA;      System.out.println(&quot;Stream context await termination ...&quot;);&#xA;      streamingContext.awaitTermination();&#xA;    } catch (InterruptedException e) {&#xA;      e.printStackTrace();&#xA;    }&#xA;&#xA;  }&#xA;&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Sample output given below also.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Study Spark Example Starting ....&#xA;Stream context starting ...&#xA;Stream context started ...&#xA;Stream context await termination ...&#xA;RDD Empty 0&#xA;RDD Empty 0&#xA;RDD Empty 0&#xA;RDD Empty 0&#xA;RDD not empty 3&#xA;Partition Id 0&#xA;Topic my-own-topic&#xA;Creating RDD !!!&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="3859188" LastEditorUserId="1765189" LastEditDate="2018-03-05T14:19:04.193" LastActivityDate="2018-03-05T14:19:04.193" Title="Apache Spark can not read Kafka message Content" Tags="&lt;apache-spark&gt;&lt;apache-kafka&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49112437" PostTypeId="1" CreationDate="2018-03-05T14:15:36.340" Score="0" ViewCount="74" Body="&lt;p&gt;I am trying to parse a json file as csv file. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The structure is a little bit complex and I wrote a spark program in scala to accomplish this task.&#xA;Like the document does not contain a json object per line I decided to use the &lt;strong&gt;wholeTextFiles&lt;/strong&gt; method as suggested in some answers and posts I’ve found. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val jsonRDD  = spark.sparkContext.wholeTextFiles(fileInPath).map(x =&amp;gt; x._2)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then I read the json content in a dataframe&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val dwdJson = spark.read.json(jsonRDD)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then I would like to navigate the json and flatten out the data.&#xA;This is the schema from dwdJson&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;root&#xA; |-- meta: struct (nullable = true)&#xA; |    |-- dimensions: struct (nullable = true)&#xA; |    |    |-- lat: long (nullable = true)&#xA; |    |    |-- lon: long (nullable = true)&#xA; |    |-- directory: string (nullable = true)&#xA; |    |-- filename: string (nullable = true)&#xA; |-- records: array (nullable = true)&#xA; |    |-- element: struct (containsNull = true)&#xA; |    |    |-- grids: array (nullable = true)&#xA; |    |    |    |-- element: struct (containsNull = true)&#xA; |    |    |    |    |-- gPt: array (nullable = true)&#xA; |    |    |    |    |    |-- element: double (containsNull = true)&#xA; |    |    |-- time: string (nullable = true)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is my best approach:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val dwdJson_e1 = dwdJson.select($&quot;meta.filename&quot;, explode($&quot;records&quot;).as(&quot;records_flat&quot;))&#xA;val dwdJson_e2 = dwdJson_e1.select($&quot;filename&quot;, $&quot;records_flat.time&quot;,explode($&quot;records_flat.grids&quot;).as(&quot;gPt&quot;))&#xA;val dwdJson_e3 = dwdJson_e2.select($&quot;filename&quot;, $&quot;time&quot;, $&quot;gPt.gPt&quot;)&#xA;val dwdJson_flat = dwdJson_e3.select($&quot;filename&quot;&#xA;      ,$&quot;time&quot;&#xA;      ,$&quot;gPt&quot;.getItem(0).as(&quot;lat1&quot;)&#xA;      ,$&quot;gPt&quot;.getItem(1).as(&quot;long1&quot;)&#xA;      ,$&quot;gPt&quot;.getItem(2).as(&quot;lat2&quot;)&#xA;      ,$&quot;gPt&quot;.getItem(3).as(&quot;long2&quot;)&#xA;      ,$&quot;gPt&quot;.getItem(4).as(&quot;value&quot;))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am a scala rookie and I am wondering if I can avoid create the intermediate dataframes (dwdJson_e1, dwdJson_e2, dwdJson_e3) that seems to be inefficient and the program runs very slowly (compare with a java parser running in a laptop). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the other side I could not find I way how to unbind these nested arrays.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;spark version:&lt;/strong&gt; 2.0.0&#xA;&lt;strong&gt;scala:&lt;/strong&gt; 2.11.8&#xA;&lt;strong&gt;java:&lt;/strong&gt; 1.8&lt;/p&gt;&#xA;&#xA;&lt;p&gt;**&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Edit 1: Sample Json file and csv output&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;**&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is a sample Json file I want to convert:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{&#xA;  &quot;meta&quot; : {&#xA;    &quot;directory&quot; : &quot;weather/cosmo/de/grib/12/aswdir_s&quot;,&#xA;    &quot;filename&quot; : &quot;COSMODE_single_level_elements_ASWDIR_S_2018022312_000.grib2.bz2&quot;,&#xA;    &quot;dimensions&quot; : {&#xA;      &quot;lon&quot; : 589,&#xA;      &quot;time&quot; : 3,&#xA;      &quot;lat&quot; : 441&#xA;    }&#xA;   },&#xA;  &quot;records&quot; : [ {&#xA;    &quot;grids&quot; : [ {&#xA;      &quot;gPt&quot; : [ 45.175, 13.55, 45.2, 13.575, 3.366295E-7 ]&#xA;    }, {&#xA;      &quot;gPt&quot; : [ 45.175, 13.575, 45.2, 13.6, 3.366295E-7 ]&#xA;    }, {&#xA;      &quot;gPt&quot; : [ 45.175, 13.6, 45.2, 13.625, 3.366295E-7 ]&#xA;    } ],&#xA;    &quot;time&quot; : &quot;2018-02-23T12:15:00Z&quot;&#xA;  }, {&#xA;    &quot;grids&quot; : [ {&#xA;      &quot;gPt&quot; : [ 45.175, 13.55, 45.2, 13.575, 4.545918E-7 ]&#xA;    }, {&#xA;      &quot;gPt&quot; : [ 45.175, 13.575, 45.2, 13.6, 4.545918E-7 ]&#xA;    }, {&#xA;      &quot;gPt&quot; : [ 45.175, 13.6, 45.2, 13.625, 4.545918E-7 ]&#xA;    }&#xA;    ],&#xA;    &quot;time&quot; : &quot;2018-02-23T12:30:00Z&quot;&#xA;    }&#xA;    ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is a sample output from the json above:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;filename, time, lat1, long1, lat2, long2, value&#xA;ASWDIR_S_...,2018-02-23T12:15:00Z,45.175,13.55, 45.2, 13.575,3.366295E-7&#xA;ASWDIR_S_...,2018-02-23T12:15:00Z,45.175,13.575, 45.2, 13.6,3.366295E-7&#xA;ASWDIR_S_...,2018-02-23T12:15:00Z,45.175,13.6, 45.2, 13.625,3.366295E-7&#xA;ASWDIR_S_...,2018-02-23T12:30:00Z,45.175,45.175, 13.55, 45.2,13.575,4.545918E-7&#xA;ASWDIR_S_...,2018-02-23T12:30:00Z,45.175,45.175,13.575,45.2,13.6,4.545918E-7&#xA;ASWDIR_S_...,2018-02-23T12:30:00Z,45.175,45.175,13.6,45.2,13.625,4.545918E-7&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any help will be appreciated. &#xA;Kind regards, &lt;/p&gt;&#xA;" OwnerUserId="3506535" LastEditorUserId="3506535" LastEditDate="2018-03-06T07:56:18.177" LastActivityDate="2018-03-06T09:15:15.860" Title="Flatten out nested Json Document in Spark2 with scala" Tags="&lt;json&gt;&lt;scala&gt;&lt;csv&gt;&lt;apache-spark&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="49112701" PostTypeId="1" CreationDate="2018-03-05T14:29:11.690" Score="-1" ViewCount="16" Body="&lt;p&gt;This may seem trivial but I want to be certain I understand how/where the code is executed.  Let's say I have a spark program that uses pyspark calls and regular python functionality.  I know when pyspark code is called it gets executed across the spark cluster on the worker/task nodes...but what about regular python code that does not require Spark?  Does that regular python code get executed on the master node?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This example might help illustrate what I'm trying to understand.  Say I have pulled data into a Spark Dataframe.  Then I transfer that data into a Pandas Dataframe.  Am I correct to assume the Pandas Dataframe exists in memory on the Master Node?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;" OwnerUserId="9446228" LastActivityDate="2018-03-05T14:29:11.690" Title="Spark Cluster - Regular Python Execution versus Pyspark Execution" Tags="&lt;python&gt;&lt;pandas&gt;&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49113021" PostTypeId="1" AcceptedAnswerId="49115058" CreationDate="2018-03-05T14:45:59.487" Score="4" ViewCount="31" Body="&lt;p&gt;&lt;code&gt;filter&lt;/code&gt; on basic scala collections containing &lt;code&gt;null&lt;/code&gt; values has the following (and quite intuitive) behaviour:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;scala&amp;gt; List(&quot;a&quot;, &quot;b&quot;, null).filter(_ != &quot;a&quot;)&#xA;res0: List[String] = List(b, null)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, I was very surprised to figure out that the following filter removes nulls in spark dataframe:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;scala&amp;gt; val df = List((&quot;a&quot;, null), ( &quot;c&quot;, &quot;d&quot;)).toDF(&quot;A&quot;, &quot;B&quot;)&#xA;scala&amp;gt; df.show&#xA;+---+----+&#xA;|  A|   B|&#xA;+---+----+&#xA;|  a|null|&#xA;|  c|   d|&#xA;+---+----+&#xA;scala&amp;gt; df.filter('B =!= &quot;d&quot;).show&#xA;+---+---+&#xA;|  A|  B|&#xA;+---+---+&#xA;+---+---+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If I want to keep &lt;code&gt;null&lt;/code&gt; values, I should add&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df.filter('B =!= &quot;d&quot; || 'B.isNull).show&#xA;+---+----+&#xA;|  A|   B|&#xA;+---+----+&#xA;|  a|null|&#xA;+---+----+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Personally, I think that removing nulls by default is very error prone. &#xA;Why this choice? and why is not explicitely stated in the api documentation? Am I missing something? &lt;/p&gt;&#xA;" OwnerUserId="5872695" LastEditorUserId="3297229" LastEditDate="2018-03-06T09:52:22.263" LastActivityDate="2018-03-06T11:13:11.590" Title="why does filter remove null value by default on spark dataframe?" Tags="&lt;sql&gt;&lt;apache-spark&gt;&lt;null&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49113498" PostTypeId="1" CreationDate="2018-03-05T15:10:19.007" Score="0" ViewCount="12" Body="&lt;p&gt;I am trying to convert categorical to numerical values using &lt;code&gt;StringIndexer&lt;/code&gt;, &lt;code&gt;OneHotEncoder&lt;/code&gt; and &lt;code&gt;VectorAssembler&lt;/code&gt; in order to apply K-means clustering in PySpark. Here's my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;indexers = [&#xA;    StringIndexer(inputCol=c, outputCol=&quot;{0}_indexed&quot;.format(c))&#xA;    for c in columnList&#xA;]&#xA;&#xA;encoders = [OneHotEncoder(dropLast=False, inputCol=indexer.getOutputCol(),&#xA;                          outputCol=&quot;{0}_encoded&quot;.format(indexer.getOutputCol()))&#xA;            for indexer in indexers&#xA;            ]&#xA;&#xA;assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders], outputCol=&quot;features&quot;)&#xA;&#xA;&#xA;pipeline = Pipeline(stages=indexers + encoders + [assembler])&#xA;model = pipeline.fit(df)&#xA;transformed = model.transform(df)&#xA;&#xA;kmeans = KMeans().setK(2).setFeaturesCol(&quot;features&quot;).setPredictionCol(&quot;prediction&quot;)&#xA;kMeansPredictionModel = kmeans.fit(transformed)&#xA;&#xA;predictionResult = kMeansPredictionModel.transform(transformed)&#xA;predictionResult.show(5)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am getting &lt;code&gt;Exception in thread &quot;dag-scheduler-event-loop&quot; java.lang.OutOfMemoryError: Java heap space&lt;/code&gt;. How can I allocate more heap space in the code or better? Is it smart to allocate more space? Can I restrict my program to the available number of threads and heap space?&lt;/p&gt;&#xA;" OwnerUserId="1818286" LastActivityDate="2018-03-05T15:10:19.007" Title="PySpark: Exception in thread &quot;dag-scheduler-event-loop&quot; java.lang.OutOfMemoryError: Java heap space" Tags="&lt;python&gt;&lt;pyspark&gt;&lt;k-means&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49113732" PostTypeId="1" CreationDate="2018-03-05T15:22:13.790" Score="1" ViewCount="28" Body="&lt;p&gt;Considering a Apache Spark 2.2.0 Structured Stream as:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;jsonStream.printSchema()&#xA;root&#xA; |-- body: binary (nullable = true)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The data inside body is of type Protocol Buffers v2 and a nested JSON. It looks like&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;syntax = &quot;proto2&quot;;&#xA;&#xA;message Data {&#xA;  required string data = 1;&#xA;}&#xA;&#xA;message List {&#xA;  repeated Data entry = 1;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How can I extract the data inside Spark to &quot;further&quot; process it?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I looked into &lt;a href=&quot;https://scalapb.github.io/sparksql.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;ScalaPB&lt;/a&gt;, but as I run my code in Jupyter couldn't get the &quot;.proto&quot; code to be included inline. I also do not know how to convert a DataFrame to an RDD on a stream. Trying &lt;code&gt;.rdd&lt;/code&gt; failed because of a streaming source.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Update 1&lt;/em&gt;: I figured out how to generate Scala files from protobuf specifications, using the console tool of ScalaPB. Still I'm not able to import them as of a &quot;type mismatch&quot;.&lt;/p&gt;&#xA;" OwnerUserId="227821" LastEditorUserId="1305344" LastEditDate="2018-03-05T18:38:32.430" LastActivityDate="2018-03-05T18:38:32.430" Title="How to extract JSON from a binary protobuf?" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;protocol-buffers&gt;&lt;spark-structured-streaming&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49113742" PostTypeId="1" CreationDate="2018-03-05T15:22:38.760" Score="0" ViewCount="8" Body="&lt;p&gt;I am running standalone Spark 2.3 and I get the following warnings:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&lt;/p&gt;&#xA;&#xA;&lt;p&gt;WARN  MetricsConfig:125 - Cannot locate configuration: tried hadoop-metrics2-azure-file-system.properties,hadoop-metrics2.properties&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone know how to address them? Thanks&lt;/p&gt;&#xA;" OwnerUserId="9388056" LastActivityDate="2018-03-05T15:22:38.760" Title="Spark 2.3 NativeCodeLoader and MetricsConfig warnings" Tags="&lt;apache-spark&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49113875" PostTypeId="1" AcceptedAnswerId="49114142" CreationDate="2018-03-05T15:28:46.867" Score="1" ViewCount="22" Body="&lt;p&gt;The data like&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;mdn day flag&#xA;c 20180302 0&#xA;c 20180303 1&#xA;b 20180303 0&#xA;a 20180301 1&#xA;b 20180301 0&#xA;a 20180302 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I get the whole by select distinct mdn from data, and left join every day, how to realize it by using hive? As following, it's only one day sample:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;with temp as (select distinct mdn from data)&#xA;select * from  temp b&#xA;left outer join&#xA;(select * from data where day=20180302) a&#xA;on a.mdn=b.mdn                            &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The result of one day like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;c c 20180302 0&#xA;a a 20180302 1&#xA;b null null null&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Exactly, it is just one day, and I want to get 'b null 20180302 null'                                                                                                                                                    &lt;/p&gt;&#xA;" OwnerUserId="6929529" LastEditorUserId="1144035" LastEditDate="2018-03-06T02:46:48.843" LastActivityDate="2018-03-06T02:46:48.843" Title="Group by day and left outer join the whole" Tags="&lt;sql&gt;&lt;apache-spark&gt;&lt;hive&gt;&lt;hiveql&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49114025" PostTypeId="1" AcceptedAnswerId="49114521" CreationDate="2018-03-05T15:36:05.097" Score="1" ViewCount="35" Body="&lt;p&gt;I’ve got a pipe-delimited textfile without a header, and the rows have different numbers of columns (some rows are type &lt;code&gt;A&lt;/code&gt; with 400 columns, others type &lt;code&gt;B&lt;/code&gt; with 200, so I need to separate them first):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val textFileRaw = sc.textFile(&quot;./data.txt&quot;)&#xA;val textFile = textFileRaw.map(line =&amp;gt; line.split(&quot;\\|&quot;, -1))&#xA;val dataA = textFile.filter(line =&amp;gt; line(0) == &quot;A&quot;)&#xA;val dataB = textFile.filter(line =&amp;gt; line(0) == &quot;B&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now I'd like to convert these RDD's into Spark DataFrames, but the split is returning a single array, rather than 400 or 200 distinct values. This results in the following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;# ANames are my column names, length=400&#xA;val ANames = Array(&quot;Row ID&quot;, &quot;City&quot;, &quot;State&quot;, ...)&#xA;val dataADF = dataA.toDF(ANames: _*)&#xA;&#xA;Name: java.lang.IllegalArgumentException&#xA;Message: requirement failed: The number of columns doesn't match.&#xA;Old column names (1): value&#xA;New column names (400): Row ID, City, State ...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/43461342/spark-adding-column-name-to-csv-file-fails&quot;&gt;This question&lt;/a&gt; faces the same problem, but all the answers suggest manually specifying a mapping from array to Tuple, which isn't great in the case with hundreds of columns.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think I could get it to work if I used &lt;a href=&quot;https://docs.databricks.com/spark/latest/data-sources/read-csv.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;Spark's csv loader&lt;/a&gt;, but this doesn't work for my data because the rows have different number of fields (it's not a true csv file). A work-around would be to first split the files, write new files that are well-formed csv's, and then use the csv loader, but  I'd like to avoid this if possible. How can I convert these RDD's into DataFrames with named columns?&lt;/p&gt;&#xA;" OwnerUserId="1748679" LastEditorUserId="1748679" LastEditDate="2018-03-05T15:57:54.410" LastActivityDate="2018-03-05T16:17:04.387" Title="Programmatically add column names to Spark DataFrame built from an RDD" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="49114357" PostTypeId="1" CreationDate="2018-03-05T15:52:31.710" Score="0" ViewCount="36" Body="&lt;p&gt;I am having issues with saving my dataframe to a hive table using the following API code.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df.write.mode(SaveMode.Append).format(&quot;parquet&quot;).partitionBy(&quot;ord_deal_year&quot;, &quot;ord_deal_month&quot;, &quot;ord_deal_day&quot;).insertInto(tableName)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My Dataframe has around 48 Columns. Where the Hive table has 90 Columns.&#xA;When I attempt to save the Dataframe I receive the following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;12:56:11 Executor task launch worker-0 ERROR Executor:96  Exception in task 0.0 in stage 3.0 (TID 3)&#xA;java.lang.ArrayIndexOutOfBoundsException: 51&#xA;    at org.apache.spark.sql.catalyst.expressions.GenericMutableRow.genericGet(rows.scala:253)&#xA;    at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow$class.getAs(rows.scala:34)&#xA;    at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow$class.isNullAt(rows.scala:35)&#xA;    at org.apache.spark.sql.catalyst.expressions.GenericMutableRow.isNullAt(rows.scala:247)&#xA;    at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1$1.apply(InsertIntoHiveTable.scala:107)&#xA;    at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1$1.apply(InsertIntoHiveTable.scala:104)&#xA;    at scala.collection.Iterator$class.foreach(Iterator.scala:727)&#xA;    at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)&#xA;    at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1(InsertIntoHiveTable.scala:104)&#xA;    at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:85)&#xA;    at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:85)&#xA;    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)&#xA;    at org.apache.spark.scheduler.Task.run(Task.scala:88)&#xA;    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&#xA;    at java.lang.Thread.run(Thread.java:745)&#xA;12:56:11 task-result-getter-3 WARN  TaskSetManager:71  Lost task 0.0 in stage 3.0 (TID 3, localhost): java.lang.ArrayIndexOutOfBoundsException: 51&#xA;    at org.apache.spark.sql.catalyst.expressions.GenericMutableRow.genericGet(rows.scala:253)&#xA;    at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow$class.getAs(rows.scala:34)&#xA;    at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow$class.isNullAt(rows.scala:35)&#xA;    at org.apache.spark.sql.catalyst.expressions.GenericMutableRow.isNullAt(rows.scala:247)&#xA;    at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1$1.apply(InsertIntoHiveTable.scala:107)&#xA;    at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1$1.apply(InsertIntoHiveTable.scala:104)&#xA;    at scala.collection.Iterator$class.foreach(Iterator.scala:727)&#xA;    at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)&#xA;    at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1(InsertIntoHiveTable.scala:104)&#xA;    at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:85)&#xA;    at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:85)&#xA;    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)&#xA;    at org.apache.spark.scheduler.Task.run(Task.scala:88)&#xA;    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&#xA;    at java.lang.Thread.run(Thread.java:745)&#xA;&#xA;12:56:11 task-result-getter-3 ERROR TaskSetManager:75  Task 0 in stage 3.0 failed 1 times; aborting job&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have attempted to add the missing columns using the following snippet of code.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val columnsAdded = columns.foldLeft(df) { case (d, c) =&amp;gt;&#xA;  if (d.columns.contains(c._1)) {&#xA;    // column exists; skip it&#xA;    d&#xA;  } else {&#xA;    // column is not available so add it&#xA;    d.withColumn(c._1, lit(null).cast(c._2))&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But the same issue still remains.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have checked the following question: &lt;a href=&quot;https://stackoverflow.com/questions/44657693/error-while-trying-to-save-the-data-to-hive-tables-from-dataframe&quot;&gt;Error while trying to save the data to Hive tables from Dataframe&lt;/a&gt; and the solution, which was determined to be an incorrect schema in the dataframe in comparison to the Hive table.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;newDF.schema.map{i =&amp;gt;&#xA;     s&quot;Column ${i.name},${i.dataType}&quot;+&#xA;     s&quot; Column exists in hive ${hiveSchema.get(i.name).isDefined}&quot; +&#xA;     s&quot; Hive Table has the correct datatype ${i.dataType == hiveSchema(i.name)}&quot;&#xA;}.foreach(i =&amp;gt; println(i))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Has anyone seen this issue or have any advice as to how to resolve this?&lt;/p&gt;&#xA;" OwnerUserId="185572" LastActivityDate="2018-03-06T18:25:23.990" Title="ArrayIndexOutOfBoundsException While saving Dataframe save to Hive" Tags="&lt;apache-spark&gt;&lt;hive&gt;&lt;apache-spark-sql&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49114381" PostTypeId="1" CreationDate="2018-03-05T15:53:24.003" Score="0" ViewCount="14" Body="&lt;p&gt;I have installed Elasticsearch 6.2.2 on local. And I have Elastcisearch 5.6 on cluster.&#xA;I want import a data on Elasticsearch in local via Spark of my cluster.&#xA;I changed the config file of my cluster, I puted the IP address of my machine, and I did telnet local_@IP&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Trying local_@IP...&#xA;telnet: connect to address local_@IP: Connection timed out&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want when I use the function &lt;code&gt;saveToEs&lt;/code&gt;, spark write in Elasticsearch of my local.&#xA;How can I do it please ?&#xA;Thank you&lt;/p&gt;&#xA;" OwnerUserId="8170477" LastActivityDate="2018-03-05T15:53:24.003" Title="Import Data from Spark on cluster to Elastcisearch on local" Tags="&lt;apache-spark&gt;&lt;elasticsearch&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="49114395" PostTypeId="1" CreationDate="2018-03-05T15:54:05.800" Score="0" ViewCount="38" Body="&lt;p&gt;I would like to count the distinct values of a column (NumberOfItems) over a rolling window:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;rollingWindow = Window.partitionBy('orderID').orderBy(&quot;ROWNUMBER&quot;).rowsBetween(-3, 0)&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am able to calculate the sum, avg and other functions, &lt;strong&gt;but I am having trouble counting the distinct values of the column 'NumberOfItems' over the window.&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;ds.withColumn( &quot;AVG_NUMBEROFITEMS&quot;,func.avg(ds[&quot;NumberOfItems&quot;]).over(rollingWindow)).orderBy(&quot;ROWNUMBER&quot;).show()&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am using pyspark (Spark 1.6.1).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The following solution has not worked for me: &#xA;&lt;a href=&quot;https://stackoverflow.com/questions/45869186/pyspark-count-distinct-over-a-window(I&quot;&gt;pyspark: count distinct over a window&lt;/a&gt; obtain the following error message: ' java.lang.UnsupportedOperationException: 'collect_set(NumberOfItem) is not supported in a window operation.').&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any feedback is really appreciated. Thanks!&lt;/p&gt;&#xA;" OwnerUserId="6939295" LastEditorUserId="6939295" LastEditDate="2018-03-05T20:05:08.037" LastActivityDate="2018-03-05T20:05:08.037" Title="PySpark - Count distinct values over a Window" Tags="&lt;count&gt;&lt;pyspark&gt;&lt;spark-dataframe&gt;&lt;window-functions&gt;&lt;distinct-values&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49114758" PostTypeId="1" CreationDate="2018-03-05T16:11:44.340" Score="0" ViewCount="23" Body="&lt;p&gt;I'm trying to convert my Pandas project to PySpark. When using Pandas and Scipy, I was able to interpolate points quite easily. Now I believe I need to convert the scipy griddata function into a PySpark UDF function to be able to replicate the behaviour in PySpark. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;At the moment, I have:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def interpolateToList(points, values, xi, method):&#xA;    data = scipy.interpolate.griddata(points,  values, xi, method)&#xA;    return data.tolist()&#xA;&#xA;sqlContext.udf.register(&quot;interpol&quot;, lambda a, b, c, d: interpolateToList(a, b, c, d), ArrayType(DoubleType()))&#xA;&#xA;znew = sqlContext.sql(&quot;SELECT interpolateToList({0}, {1}, {2}, {3})&quot;.format(points, np_IVs, (grid_x, grid_y), d)).collect()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Where &lt;code&gt;points&lt;/code&gt; is an RDD containing:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[(1.0, 261.125), (0.917184265010352, 0.16666666666666666), (0.7971014492753623, 142.125), (0.0, 107.125)]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;np_IVs&lt;/code&gt; also an rdd containing:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[0.11615423649543664, 0.10177386211985723, 0.03760565497504577, 0.01746264328602701]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;grid_x&lt;/code&gt; and &lt;code&gt;grid_y&lt;/code&gt; are also RDDs, &lt;code&gt;grid_x&lt;/code&gt; contains:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[array([0., 0., 0., 0.]), array([0.33333333, 0.33333333, 0.33333333, 0.33333333]), array([0.66666667, 0.66666667, 0.66666667, 0.66666667]), array([1., 1., 1., 1.])]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;grid_y&lt;/code&gt; contains:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[array([1.66666667e-01, 8.71527778e+01, 1.74138889e+02, 2.61125000e+02]), array([1.66666667e-01, 8.71527778e+01, 1.74138889e+02, 2.61125000e+02]), array([1.66666667e-01, 8.71527778e+01, 1.74138889e+02, 2.61125000e+02]), array([1.66666667e-01, 8.71527778e+01, 1.74138889e+02, 2.61125000e+02])]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;d = 'nearest'&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The error I receive:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ParseException: u&quot;\nextraneous input 'at' expecting {')', ',', '.', '[', 'OR', 'AND', 'IN', NOT, 'BETWEEN', 'LIKE', RLIKE, 'IS', EQ, '&amp;lt;=&amp;gt;', '&amp;lt;&amp;gt;', '!=', '&amp;lt;', LTE, '&amp;gt;', GTE, '+', '-', '*', '/', '%', 'DIV', '&amp;amp;', '|', '^'}(line 1, pos 53)\n\n== SQL ==\nSELECT interpolateToList(ParallelCollectionRDD[9702] at parallelize at PythonRDD.scala:475, ParallelCollectionRDD[9709] at parallelize at PythonRDD.scala:475, (ParallelCollectionRDD[9714] at parallelize at PythonRDD.scala:475, ParallelCollectionRDD[9715] at parallelize at PythonRDD.scala:475), nearest)\n-----------------------------------------------------^^^\n&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Has anyone worked with interpolation in PySpark before and has any ideas on how I can do this successfully?&lt;/p&gt;&#xA;" OwnerUserId="9402556" LastEditorUserId="7832176" LastEditDate="2018-03-05T17:07:56.960" LastActivityDate="2018-03-05T17:07:56.960" Title="Scipy Griddata PySpark - UDF function" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;scipy&gt;&lt;pyspark&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="49114841" PostTypeId="1" AcceptedAnswerId="49127123" CreationDate="2018-03-05T16:16:58.440" Score="0" ViewCount="34" Body="&lt;p&gt;I have a pyspark dataframe which has event column as 0's and 1's for every month on a user_id. I need to select the event as 1's which must have exactly pervious rows as 5 0's. If this condition satisfies then only get the first 1's ? actually identifying the patterns of 5 0's and next 1's&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried is by using rank but its not working on whole dataset for me. Any inputs will be helpful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;for eg. if you have dataframe as:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; df:&#xA; user_id   event&#xA;   1         0   &#xA;   1         0&#xA;   1         0&#xA;   1         0&#xA;   1         0&#xA;   1         1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now i need to find out this kind of pattern in my dataset it can be available at place for my records of data set having specific user id. As for as user id i can have max 48 records i need to find such groups of pattern out it. Between this if 1 occurs then i am not interested.&lt;/p&gt;&#xA;" OwnerUserId="4245148" LastActivityDate="2018-03-06T09:11:30.777" Title="Get the first '1's on condition if it followed by consecutive 5 '0's in pyspark dataframe" Tags="&lt;python&gt;&lt;pandas&gt;&lt;pyspark&gt;&lt;spark-dataframe&gt;&lt;pyspark-sql&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49114901" PostTypeId="1" CreationDate="2018-03-05T16:19:31.303" Score="1" ViewCount="14" Body="&lt;p&gt;Often I am encountering a pattern of dividing Big processing steps in batches when these steps can't be processed entirely in our Big Data Spark cluster.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For instance, we have a large cross join or some calculus that fails when done with all the input data and then we usually are dividing these spark task in chunks so the spark mini-tasks can complete.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Particularly I doubt this is the right way to do it in Spark.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a recipe to solve this issue? Or even with Spark we are again in the old-way of chunking/batching the work so to the work can be completed in a small cluster?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this a mere question of re-partitioning the input data so that Spark can do more sequential processing instead of parallel processing?&lt;/p&gt;&#xA;" OwnerUserId="835697" LastActivityDate="2018-03-05T16:19:31.303" Title="Avoid chunk / batch processing in Spark" Tags="&lt;apache-spark&gt;&lt;partitioning&gt;&lt;large-data&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49115508" PostTypeId="1" CreationDate="2018-03-05T16:50:52.760" Score="0" ViewCount="24" Body="&lt;p&gt;I using the following code to write a stream to elasticsearch from python (pyspark) application.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#Streaming code&#xA;query = df.writeStream \&#xA;.outputMode(&quot;append&quot;) \&#xA;.format(&quot;org.elasticsearch.spark.sql&quot;) \&#xA;.option(&quot;checkpointLocation&quot;, &quot;/tmp/&quot;) \&#xA;.option(&quot;es.resource&quot;, &quot;logs/raw&quot;) \&#xA;.option(&quot;es.nodes&quot;, &quot;localhost&quot;) \&#xA;.start()&#xA;&#xA;query.awaitTermination()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If I write the results to the console it works fine, also, if I write to ES - not in streaming mode, it works ok. This is the code I used to write to ES:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#Not streaming&#xA;df.write.format(&quot;org.elasticsearch.spark.sql&quot;) \&#xA;.mode('append') \&#xA;.option(&quot;es.resource&quot;, &quot;log/raw&quot;) \&#xA;.option(&quot;es.nodes&quot;, &quot;localhost&quot;).save(&quot;log/raw&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The thing is, I can't debug it, the code is running, but nothing is written to ES (in streaming mode).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks,&lt;/p&gt;&#xA;" OwnerUserId="1178689" LastActivityDate="2018-03-05T16:50:52.760" Title="Spark Streaming: Write dataframe to ElasticSearch" Tags="&lt;apache-spark&gt;&lt;elasticsearch&gt;&lt;pyspark&gt;&lt;spark-streaming&gt;&lt;pyspark-sql&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49116359" PostTypeId="1" AcceptedAnswerId="49120549" CreationDate="2018-03-05T17:38:18.367" Score="-3" ViewCount="51" Body="&lt;p&gt;I would like to do union of data frame in the recursive method.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am doing some calculations in the recursive method and filtering the data and storing in one variable. In 2nd iteration i will do some calculation and again i will store the data in same variable.when i am calling the method second time my first result is getting vanished.Ideally i have to store the result in one temp variable and i need to do union of all the result till the recursive method gets completed its execution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Iteration1 output in df:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Col1   &#xA;    14      &#xA;    35    &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Iteration2 output in df:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Col1&#xA;    18      &#xA;    20&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now i need the final output as,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Col1&#xA;    14&#xA;    35&#xA;    18&#xA;    20&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def myRecursiveMethod(first: List[List[String]],&#xA;                        Inputcolumnsdummy: List[List[String]],&#xA;                        secondInputcolumns: List[List[String]] = {&#xA;&#xA;  val ongoingResult = doSomeCalculation(first,Inputcolumnsdummy, secondInputcolumns)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want my code should be something like below,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def myRecursiveMethod(first: List[List[String]],&#xA;                        Inputcolumnsdummy: List[List[String]],&#xA;                        secondInputcolumns: List[List[String]]) = {&#xA;&#xA;    val ongoingResult = doSomeCalculation(first, Inputcolumnsdummy, secondInputcolumns)&#xA;    Val temp = temp.union(ongoingResult)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="8919697" LastEditorUserId="7115301" LastEditDate="2018-03-06T07:50:14.737" LastActivityDate="2018-03-06T07:50:14.737" Title="Scala: How to do union of data frames in the loop" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="8" />
  <row Id="49116437" PostTypeId="1" AcceptedAnswerId="49122893" CreationDate="2018-03-05T17:43:36.740" Score="1" ViewCount="55" Body="&lt;p&gt;I want to optimize my script doing a &lt;strong&gt;hush partitioning&lt;/strong&gt; and using persist() in spark 2.1 but running my code I have an error that I don't understand.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The code is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;rdd = sc.textFile(&quot;path&quot;).map(lambdal:l.split(&quot;;&quot;))&#xA;&#xA;rdd_pair=rdd.map(lambda a: (a[0], a)).PartitionBy(920).persist()&#xA;&#xA;rdd_pair=rdd_pair.combineByKey(lambda v:[v],lambda x,y:x+[y],lambda x,y:x+y)&#xA;&#xA;def fn(input_bag):&#xA;    output=[]&#xA;    loc0 = input_bag[0]&#xA;    for loc in input_bag[1:]:&#xA;        output.append((loc0[2],loc[2]))&#xA;        loc0 = loc&#xA;    return output&#xA;&#xA;data=rdd1.map(lambda k: (k[0], fn(k[1]))).persist()&#xA;&#xA;data=data.flatMap(lambda x: map(lambda e: (e, x[0]), x[1])).map(lambda x: (x[0],1)).reduceByKey(lambda p,q: p+q)&#xA;&#xA;data.map(lambda x:&quot;,&quot;.join(map(str,x))).saveAsTextFile(Path)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The &lt;strong&gt;error&lt;/strong&gt; is the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;&#xA;  File &quot;/home/wrjx9579/test1.py&quot;, line 12, in &amp;lt;module&amp;gt;&#xA;sc = SparkContext(conf=conf)&#xA;&#xA;  File &quot;/opt/application/Spark2/current/python/lib/pyspark.zip/pyspark&#xA;/context.py&quot;, line 118, in __ init __&#xA;&#xA;  File &quot;/opt/application/Spark2/current/python/lib/pyspark.zip/pyspark&#xA;/context.py&quot;, line 179, in _do_init&#xA;&#xA;  File &quot;/opt/application/Spark2/current/python/lib/pyspark.zip/pyspark/context.py&quot;, line 246, in _initialize_context&#xA;&#xA;  File &quot;/opt/application/Spark2/current/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py&quot;, line 1401, in __ call__&#xA;&#xA;  File &quot;/opt/application/Spark2/current/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py&quot;, line 319, in get_return_value&#xA;&#xA;py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.&#xA;&#xA;: java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; &#xA;Host Details : local host is: &quot;...&quot;; destination host is: &quot;...&quot;:8032; &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="9252261" LastEditorUserId="7115301" LastEditDate="2018-03-05T23:57:32.510" LastActivityDate="2018-03-06T06:04:03.053" Title="Using partitionBy() and persist() in pyspark" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49116557" PostTypeId="1" CreationDate="2018-03-05T17:50:19.677" Score="2" ViewCount="29" Body="&lt;p&gt;This dummy data represents a device with measurement cycles.&#xA;One measurement clycle goes from &quot;Type&quot; Init to Init.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I want to find out is the f.e. last error (the condition will get way more complicated) within each measurement cylce.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I already figured out a solution for this. What I really want to know is if there is an easier / more efficient way to calculate this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Example Dataset&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val df_orig = spark.sparkContext.parallelize(Seq(&#xA;      (&quot;Init&quot;, 1, 17, &quot;I&quot;),&#xA;      (&quot;TypeA&quot;, 2, 17, &quot;W&quot;),&#xA;      (&quot;TypeA&quot;, 3, 17, &quot;E&quot;),&#xA;      (&quot;TypeA&quot;, 4, 17, &quot;W&quot;),&#xA;      (&quot;TypeA&quot;, 5, 17, &quot;E&quot;),&#xA;      (&quot;TypeA&quot;, 6, 17, &quot;W&quot;),&#xA;      (&quot;Init&quot;, 7, 12, &quot;I&quot;),&#xA;      (&quot;TypeB&quot;, 8, 12, &quot;W&quot;),&#xA;      (&quot;TypeB&quot;, 9, 12, &quot;E&quot;),&#xA;      (&quot;TypeB&quot;, 10, 12, &quot;W&quot;),&#xA;      (&quot;TypeB&quot;, 11, 12, &quot;W&quot;),&#xA;      (&quot;TypeB&quot;, 12, 12, &quot;E&quot;),&#xA;      (&quot;TypeB&quot;, 13, 12, &quot;E&quot;)&#xA;    )).toDF(&quot;Type&quot;, &quot;rn&quot;, &quot;X_ChannelC&quot;, &quot;Error_Type&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The following code represents my solution.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val fillWindow = Window.partitionBy().orderBy($&quot;rn&quot;).rowsBetween(Window.unboundedPreceding, 0)&#xA;&#xA;    //create window&#xA;    val df_with_window = df_orig.withColumn(&quot;window_flag&quot;, when($&quot;Type&quot;.contains(&quot;Init&quot;), 1).otherwise(null))&#xA;        .withColumn(&quot;window_filled&quot;, sum($&quot;window_flag&quot;).over(fillWindow))&#xA;&#xA;    val window = Window.partitionBy(&quot;window_filled&quot;).orderBy($&quot;rn&quot;).rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)&#xA;&#xA;    //calulate last entry&#xA;    val df_new = df_with_window.withColumn(&quot;is_relevant&quot;, when($&quot;Error_Type&quot;.contains(&quot;E&quot;), $&quot;rn&quot;).otherwise(null))&#xA;      .withColumn(&quot;last&quot;, last($&quot;is_relevant&quot;, true).over(window))&#xA;      .withColumn(&quot;pass&quot;, when($&quot;last&quot; === $&quot;is_relevant&quot;, &quot;Fail&quot;).otherwise(null))&#xA;&#xA;    df_new.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Result:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+-----+---+----------+----------+-----------+-------------+-----------+----+--------+&#xA;| Type| rn|X_ChannelC|Error_Type|window_flag|window_filled|is_relevant|last|    pass|&#xA;+-----+---+----------+----------+-----------+-------------+-----------+----+--------+&#xA;| Init|  1|        17|         I|          1|            1|       null|   5|    null|&#xA;|TypeA|  2|        17|         W|       null|            1|       null|   5|    null|&#xA;|TypeA|  3|        17|         E|       null|            1|          3|   5|    null|&#xA;|TypeA|  4|        17|         W|       null|            1|       null|   5|    null|&#xA;|TypeA|  5|        17|         E|       null|            1|          5|   5|This one|&#xA;|TypeA|  6|        17|         W|       null|            1|       null|   5|    null|&#xA;| Init|  7|        12|         I|          1|            2|       null|  13|    null|&#xA;|TypeB|  8|        12|         W|       null|            2|       null|  13|    null|&#xA;|TypeB|  9|        12|         E|       null|            2|          9|  13|    null|&#xA;|TypeB| 10|        12|         W|       null|            2|       null|  13|    null|&#xA;|TypeB| 11|        12|         W|       null|            2|       null|  13|    null|&#xA;|TypeB| 12|        12|         E|       null|            2|         12|  13|    null|&#xA;|TypeB| 13|        12|         E|       null|            2|         13|  13|This one|&#xA;+-----+---+----------+----------+-----------+-------------+-----------+----+--------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2811630" LastActivityDate="2018-03-05T20:27:30.573" Title="Last Entry that matches a condition per Window" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49116984" PostTypeId="1" CreationDate="2018-03-05T18:18:30.427" Score="0" ViewCount="20" Body="&lt;p&gt;I am trying to read a parquet file in HDFS using spark and push the data to a vertica table. Below is my code that i am running in Zeppelin.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import java.util.Properties&#xA;import java.sql&#xA;import org.apache.spark.sql.{DataFrame, Row, SQLContext, SaveMode}&#xA;import org.apache.spark.sql.types._&#xA;import com.vertica.spark.datasource&#xA;import org.apache.spark.{SparkConf, SparkContext}&#xA;val sqlContext = new org.apache.spark.sql.SQLContext(sc)&#xA;val res = sqlContext.read.parquet(&quot;hdfs:/xxx/xxx/xxxx/xxxx/xxxx/xxx//CALL_RSLT&quot;)&#xA;val url = &quot;jdbc:vertica://xcxcxcxc.xcxcx.xcxcxcx.com:5433/xcxcxc_xcxcxc_vertica&quot;&#xA;val prop = new java.util.Properties&#xA;prop.setProperty(&quot;user&quot;,&quot;xxxx&quot;)&#xA;prop.setProperty(&quot;password&quot;,&quot;xxxx&quot;)&#xA;res.write.mode(SaveMode.Append).jdbc(url, &quot;S_H9A0_AC.call_rslt&quot;, prop)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When i run this code in Zeppelin, i get the below error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; &amp;lt;console&amp;gt;:70: error: object spark is not a member of package com.vertica&#xA;   import com.vertica.spark.datasource&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have included the vertica-8.1.0_spark2.0_scala2.11.jar and vertica-jdbc8.1.0-3.jar files in my spark-shell command. The vertica version i am using is 8.1.0-3 and spark 2.1.1. This code is running on a cluster. When i run the above mentioned code in the spark shell, my code executes until the last line, where it gives the below error: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;java.sql.SQLException: No suitable driver&#xA;    at java.sql.DriverManager.getDriver(DriverManager.java:315)&#xA;    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$2.apply(JdbcUtils.scala:50)&#xA;    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$2.apply(JdbcUtils.scala:50)&#xA;    at scala.Option.getOrElse(Option.scala:120)&#xA;    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createConnectionFactory(JdbcUtils.scala:49)&#xA;    at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:278)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Please help me understand what i am missing or what i am not doing right. Thanks in advance.&lt;/p&gt;&#xA;" OwnerUserId="9420288" LastActivityDate="2018-03-05T18:18:30.427" Title="Error while connecting Vertica to Spark" Tags="&lt;apache-spark&gt;&lt;vertica&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49117252" PostTypeId="1" AcceptedAnswerId="49120853" CreationDate="2018-03-05T18:34:42.623" Score="0" ViewCount="18" Body="&lt;p&gt;Is it possible to allow JDBC/ODBC access in Spark standalone mode? I know it's possible in YARN mode, but I don't know how to turn it on in standalone mode.&lt;/p&gt;&#xA;" OwnerUserId="1763955" LastActivityDate="2018-03-05T22:56:44.330" Title="In Spark standalone mode, how do I enable JDBC/ODBC access?" Tags="&lt;apache-spark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49118110" PostTypeId="1" CreationDate="2018-03-05T19:33:41.997" Score="0" ViewCount="26" Body="&lt;p&gt;Spark stores on-going processed data in the &lt;code&gt;_temporary&lt;/code&gt; folder. Once the job finishes, the data is moved to its final destination. However, when there are tens of thousands of partitions, it takes quite some time to move the files from one place to the other. Question: how to speed up this move?&#xA;Running applications in yarn-cluster mode, on a bare-metal Hadoop, not on AWS (no S3, EMR, etc).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Update: my job takes around 1 hour to generate 2.3T of data in 25000 partitions, and another hour to move data out of _temporary.&lt;/p&gt;&#xA;" OwnerUserId="459888" LastEditorUserId="459888" LastEditDate="2018-03-05T20:05:16.883" LastActivityDate="2018-03-07T14:02:47.533" Title="Moving content of _temporary folder to final location" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;yarn&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="49118736" PostTypeId="1" CreationDate="2018-03-05T20:16:52.697" Score="-1" ViewCount="15" Body="&lt;p&gt;We have a five-node zookeeper cluster running fine. We're using it for Spark, Kakfa, Solr, etc. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I created a second, two-node Spark cluster and pointed it at the same zookeeper cluster and it wouldn't work. It seemed to make my two new Spark nodes and add them to the first cluster.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Anyone know how to run two spark clusters (or two solr, kafka, etc.,) clusters in a single zookeeper cluster?&lt;/p&gt;&#xA;" OwnerUserId="9447821" LastActivityDate="2018-03-05T20:16:52.697" Title="Multiple spark clusters share a single zookeeper cluster" Tags="&lt;apache-spark&gt;&lt;apache-zookeeper&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49119159" PostTypeId="1" CreationDate="2018-03-05T20:45:18.603" Score="-2" ViewCount="9" Body="&lt;p&gt;can I load this into the spark-shell/pyspark&#xA;&lt;a href=&quot;https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/all/online-retail-dataset.csv&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://raw.githubusercontent.com/databricks/Spark-The-Definitive-Guide/master/data/retail-data/all/online-retail-dataset.csv&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="9447945" LastActivityDate="2018-03-05T22:03:53.097" Title="How to load below link to spark-shell or pyspark" Tags="&lt;apache-spark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49119206" PostTypeId="1" CreationDate="2018-03-05T20:48:28.890" Score="0" ViewCount="14" Body="&lt;p&gt;How do I create a custom model in pyspark?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In scikit-learn it is easy (see &lt;a href=&quot;http://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator&quot; rel=&quot;nofollow noreferrer&quot;&gt;Rolling your own estimator&lt;/a&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, in pyspark I cannot find any similar documentation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I found out, from reading the &lt;a href=&quot;https://github.com/apache/spark/blob/master/python/pyspark/ml/base.py&quot; rel=&quot;nofollow noreferrer&quot;&gt;source code&lt;/a&gt;, that there are three relevant base interfaces: Model, Estimator and Transformer&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, it is not clear to me if I should inherit from Model or Estimator or both. Especially the inheritance from Parameters is complicated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've seen a similar answer &lt;a href=&quot;https://stackoverflow.com/questions/37270446/how-to-roll-a-custom-estimator-in-pyspark-mllib&quot;&gt;here&lt;/a&gt; but it is almost one year old and I guess things have changed since then. It also seems to refer to mllib and not ml version of spark.&lt;/p&gt;&#xA;" OwnerUserId="2424587" LastEditorUserId="2424587" LastEditDate="2018-03-07T14:52:13.733" LastActivityDate="2018-03-07T14:52:13.733" Title="How can one develop a custom model with pyspark ml?" Tags="&lt;pyspark&gt;&lt;apache-spark-ml&gt;" AnswerCount="0" CommentCount="4" />
  <row Id="49119256" PostTypeId="1" CreationDate="2018-03-05T20:51:15.780" Score="0" ViewCount="35" Body="&lt;p&gt;I tried to create a function which would get the data from relational database and insert them into Hive table. Since I use Spark 1.6, I need to register a temporary table, because writing dataframe directly as Hive table is not &lt;a href=&quot;https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_rn_spark_ki.html#ki_sparksql_dataframe_saveastable&quot; rel=&quot;nofollow noreferrer&quot;&gt;compatible with Hive&lt;/a&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark_conf = SparkConf()&#xA;sc = SparkContext(conf=spark_conf)&#xA;sqlContext = HiveContext(sc)&#xA;&#xA;query = &quot;(select * from employees where emp_no &amp;lt; 10008) as emp_alias&quot;&#xA;df = sqlContext.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, url) \&#xA;        .option(&quot;dbtable&quot;, query) \&#xA;        .option(&quot;user&quot;, user) \&#xA;        .option(&quot;password&quot;, pswd).load()&#xA;&#xA;df.registerTempTable('tempEmp')&#xA;&#xA;sqlContext.sql('insert into table employment_db.hive_employees select * from tempEmp')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;employees&lt;/code&gt; table in RDB contains few thousand records. After running my program I can see that two parquet files are created:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;a file, which is created after my code finishes&lt;/li&gt;&#xA;&lt;li&gt;a file, which is created after two hours&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;So when I try to select from the Hive table after the job is completed, there are missing records.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have multiple ideas, which could cause the problem:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Could it be caused by lazy evaluation of &lt;code&gt;registerTempTable&lt;/code&gt;? Does Spark think that I don't use those records? I am familiar with lazy evaluation in generators, but I can't imagine how exactly lazy evaluation works in  &lt;code&gt;registerTempTable&lt;/code&gt; function.&lt;/li&gt;&#xA;&lt;li&gt;Does it save the temporary tables in &lt;code&gt;tmp&lt;/code&gt; folder? Can it be caused because of not enough space? Should I use the &lt;code&gt;dropTempTable&lt;/code&gt; function?&lt;/li&gt;&#xA;&lt;li&gt;Is safer to use &lt;code&gt;createOrReplaceTempView&lt;/code&gt; (despite the fact that &lt;code&gt;registerTempTable&lt;/code&gt; is deprecated in Spark 2).&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;More info&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;using Spark 1.6 on Yarn (Hadoop 2.6.0-cdh5.8.0)&lt;/li&gt;&#xA;&lt;li&gt;running multiple jobs with different Hive Context, but I don't access the temporary tables across the context&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="2400986" LastActivityDate="2018-03-06T00:49:31.683" Title="registerTempTable() doesn't register all records" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;hive&gt;&lt;pyspark&gt;&lt;hivecontext&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49119521" PostTypeId="1" CreationDate="2018-03-05T21:10:14.203" Score="0" ViewCount="10" Body="&lt;pre&gt;&lt;code&gt;import org.apache.spark.SparkConf&#xA;import org.apache.spark.storage.StorageLevel&#xA;import org.apache.spark.streaming.Milliseconds&#xA;import org.apache.spark.streaming.StreamingContext&#xA;import org.apache.spark.streaming.dstream.DStream.toPairDStreamFunctions&#xA;&#xA;import com.amazonaws.auth.AWSCredentials&#xA;import com.amazonaws.auth.DefaultAWSCredentialsProviderChain&#xA;import com.amazonaws.auth.SystemPropertiesCredentialsProvider&#xA;import com.amazonaws.services.kinesis.AmazonKinesisClient&#xA;import com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream&#xA;import org.apache.spark.streaming.kinesis.KinesisInputDStream&#xA;import org.apache.spark.streaming.kinesis.KinesisInitialPositions.Latest&#xA;import org.apache.spark.streaming.kinesis.KinesisInitialPositions.TrimHorizon&#xA;import java.util.Date&#xA;&#xA;&#xA;&#xA;    val tStream = KinesisInputDStream.builder&#xA;            .streamingContext(ssc)&#xA;            .streamName(streamName)&#xA;            .endpointUrl(endpointUrl)&#xA;            .regionName(regionName)&#xA;            .initialPosition(new TrimHorizon())&#xA;            .checkpointAppName(appName)&#xA;            .checkpointInterval(kinesisCheckpointInterval)&#xA;            .storageLevel(StorageLevel.MEMORY_AND_DISK_2)&#xA;            .build()&#xA;    tStream.foreachRDD(rdd =&amp;gt; if (rdd.count() &amp;gt; 0) rdd.saveAsTextFile(&quot;/user/hdfs/test/&quot;) else println(&quot;No record to read&quot;))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here, even though I see data coming into the stream, my above spark job isn't getting any records. I am sure that I am connecting to right stream with all credentials.&#xA;Please help me out.&lt;/p&gt;&#xA;" OwnerUserId="3649072" LastActivityDate="2018-03-05T21:10:14.203" Title="Unable to read Kinesis stream from SparkStreaming" Tags="&lt;apache-spark&gt;&lt;spark-streaming&gt;&lt;amazon-kinesis&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49119718" PostTypeId="1" CreationDate="2018-03-05T21:24:49.243" Score="0" ViewCount="19" Body="&lt;p&gt;I use Anaconda on a Windows 10 laptop with Python 2.7 and Spark 2.1. Built a deep learning model using Sknn.mlp package. I have completed the model. When I try to predict using the predict function, it throws an error. I run the same code on my Mac and it works just fine. Wondering what is wrong with my windows packages.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;'NoneType' object is not callable&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I verified input data. It is numpy.array and it does not have null value. Its dimension is same as training one and all attributed are the same. Not sure what it can be.&lt;/p&gt;&#xA;" OwnerUserId="1918180" LastActivityDate="2018-03-07T05:41:02.273" Title="Error in prediction using sknn.mlp" Tags="&lt;python&gt;&lt;windows&gt;&lt;pyspark&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49120584" PostTypeId="1" CreationDate="2018-03-05T22:30:49.710" Score="0" ViewCount="16" Body="&lt;p&gt;My question is kinda &lt;a href=&quot;https://stackoverflow.com/questions/30851244/spark-read-file-from-s3-using-sc-textfile-s3n/33836325#33836325&quot;&gt;similar to this one&lt;/a&gt;, but the suggested solutions did not solve my problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a very simple spark job that I want to run locally, basically, it just reads a file from S3 and creates a data frame out of it.   &lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I run my code on Amazon &lt;code&gt;EC2&lt;/code&gt; cluster everything works fine, but when I want to run it locally I get this error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Caught exception while loading path, returning empty data frame: No FileSystem for scheme: s3a&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here are the differences when I run the code on EC2 cluster and locally :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I run it locally I commented all the provided tag for spark dependancy.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    &amp;lt;dependency&amp;gt;&#xA;        &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&#xA;        &amp;lt;artifactId&amp;gt;spark-core_2.11&amp;lt;/artifactId&amp;gt;&#xA;        &amp;lt;version&amp;gt;2.1.0&amp;lt;/version&amp;gt;&#xA;        &amp;lt;!--&amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;--&amp;gt;&#xA;    &amp;lt;/dependency&amp;gt;&#xA;    &amp;lt;dependency&amp;gt;&#xA;        &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&#xA;        &amp;lt;artifactId&amp;gt;spark-sql_2.11&amp;lt;/artifactId&amp;gt;&#xA;        &amp;lt;version&amp;gt;2.1.0&amp;lt;/version&amp;gt;&#xA;        &amp;lt;!--&amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;--&amp;gt;&#xA;    &amp;lt;/dependency&amp;gt;&#xA;    &amp;lt;dependency&amp;gt;&#xA;        &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&#xA;        &amp;lt;artifactId&amp;gt;spark-streaming_2.11&amp;lt;/artifactId&amp;gt;&#xA;        &amp;lt;version&amp;gt;2.1.0&amp;lt;/version&amp;gt;&#xA;        &amp;lt;!--&amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;--&amp;gt;&#xA;    &amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Also, I added this to the dependency &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;&#xA;  &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;&#xA;  &amp;lt;artifactId&amp;gt;hadoop-aws&amp;lt;/artifactId&amp;gt;&#xA;  &amp;lt;version&amp;gt;3.0.0&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and this the way I create spark session locally and on the cluster:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the cluster:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  def getSparkSession(config: BaseConfig): SparkSession = {&#xA;&#xA;    val conf = new SparkConf()&#xA;&#xA;    conf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;)&#xA;    conf.set(&quot;spark.kryoserializer.buffer.mb&quot;,&quot;24&quot;)&#xA;&#xA;    config.getSparkConfig.foreach(x =&amp;gt; conf.set(x._1, x._2))&#xA;&#xA;    val ss = SparkSession&#xA;      .builder()&#xA;      .config(conf)&#xA;      .getOrCreate()&#xA;&#xA;&#xA;    ss.sparkContext.hadoopConfiguration.set(&quot;fs.s3n.awsAccessKeyId&quot;, config.awsAccessKeyId)&#xA;    ss.sparkContext.hadoopConfiguration.set(&quot;fs.s3n.awsSecretAccessKey&quot;, config.awsSecretAccessKey)&#xA;&#xA;    ss&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I run it locally:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  def getLocalSparkSession(config: BaseConfig): SparkSession = {&#xA;    val warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePath&#xA;&#xA;    val ss = SparkSession.builder()&#xA;      .appName(this.getClass.getName)&#xA;      .master(&quot;local&quot;)&#xA;      .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation)&#xA;      .config(&quot;spark.sql.shuffle.partitions&quot;, &quot;4&quot;)&#xA;      .getOrCreate()&#xA;    ss.sparkContext.setLogLevel(&quot;WARN&quot;)&#xA;&#xA;&#xA;    ss.sparkContext.hadoopConfiguration.set(&quot;fs.s3n.awsAccessKeyId&quot;, config.awsAccessKeyId)&#xA;    ss.sparkContext.hadoopConfiguration.set(&quot;fs.s3n.awsSecretAccessKey&quot;, config.awsSecretAccessKey)&#xA;&#xA;    ss&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;P.S. My &lt;code&gt;spark-shell --version&lt;/code&gt; shows it uses spark version 2.2.1, but I think my spark on EC2 cluster is older version (should be 2.0 something)&lt;/p&gt;&#xA;" OwnerUserId="140903" LastActivityDate="2018-03-07T13:56:57.083" Title="local spark complains about s3a schema" Tags="&lt;apache-spark&gt;&lt;amazon-s3&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49120622" PostTypeId="1" CreationDate="2018-03-05T22:34:28.960" Score="-1" ViewCount="5" Body="&lt;p&gt;Does anyone know where I can find a tutorial or walk through on setting up a macOS app built using Visual Studio 2017 and the Sparkle open source updating system?&lt;/p&gt;&#xA;" OwnerUserId="856232" LastActivityDate="2018-03-05T22:34:28.960" Title="macOS Sparkle Updater in Visual Studio 2017" Tags="&lt;macos&gt;&lt;visual-studio-2017&gt;&lt;sparkle&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49120634" PostTypeId="1" AcceptedAnswerId="49120719" CreationDate="2018-03-05T22:35:49.923" Score="-1" ViewCount="25" Body="&lt;p&gt;I want to apologize for my really stupid question but I have a problem with my Linear Regression. I`m struggling with that a lot. Could you please help me.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is my main code. I`m currently using some external library to plot the data.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import com.fundtrml.config.ConfigSetUp&#xA;import org.apache.spark.ml.feature.LabeledPoint&#xA;import org.apache.spark.ml.linalg.Vectors&#xA;import org.apache.spark.ml.regression.LinearRegression&#xA;import org.apache.spark.sql.SparkSession&#xA;&#xA;object SimpleLinearRegression {&#xA;&#xA;  def main(args: Array[String]): Unit = {&#xA;    ConfigSetUp.HadoopBinariesConfig();&#xA;&#xA;    val ss = SparkSession.builder().appName(&quot;DataSet Test&quot;)&#xA;      .master(&quot;local[*]&quot;).getOrCreate()&#xA;&#xA;    import ss.implicits._&#xA;&#xA;    var listOfData = List(40, 41, 45, 43, 42, 60, 61, 59, 50, 49, 47, 39, 41, 37, 36, 34, 33, 37)&#xA;    val data =  listOfData  //(1 to 21 by 1)                      // create a collection of Doubles&#xA;      .map(n =&amp;gt; (n, n))                               // make it pairs&#xA;      .map { case (label, features) =&amp;gt;&#xA;      LabeledPoint(label, Vectors.dense(features)) } // create labeled points of dense vectors&#xA;      .toDF                                           // make it a DataFrame&#xA;    var splittedData = data.randomSplit(Array(0.6,0.4))&#xA;    var trainingData = splittedData(0)&#xA;    var testSetData = splittedData(1)&#xA;&#xA;    trainingData.show()&#xA;    val lr = new LinearRegression()&#xA;        .setMaxIter(10)&#xA;        .setRegParam(0.3)&#xA;        .setElasticNetParam(0.8)&#xA;&#xA;    //train&#xA;    val model = lr.fit(trainingData)&#xA;    println(s&quot;model.intercept: ${model.intercept}&quot;)&#xA;    println(s&quot;model.coefficients : ${model.coefficients}&quot;)&#xA;    // Summarize the model over the training set and print out some metrics&#xA;    val trainingSummary = model.summary&#xA;    println(s&quot;numIterations: ${trainingSummary.totalIterations}&quot;)&#xA;    println(s&quot;objectiveHistory: [${trainingSummary.objectiveHistory.mkString(&quot;,&quot;)}]&quot;)&#xA;    trainingSummary.residuals.show()&#xA;    println(s&quot;RMSE: ${trainingSummary.rootMeanSquaredError}&quot;)&#xA;    println(s&quot;r2: ${trainingSummary.r2}&quot;)&#xA;&#xA;    val predictions = model.transform(testSetData)&#xA;    predictions.show()&#xA;&#xA;    //Display the data&#xA;    import com.quantifind.charts.Highcharts._&#xA;    regression(listOfData) //using this external library with embeded functionality about regression&#xA;&#xA;    var currentPredictions = predictions.select(&quot;prediction&quot;).rdd.map(r =&amp;gt; r(0)).collect.toList&#xA;    println(currentPredictions)&#xA;//    regression(currentPredictions.map(_.toString.toDouble))&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My Training set is as follows, label collumn- value, which should be predicted, features- value, which should be used to make a prediction:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+-----+--------+&#xA;|label|features|&#xA;+-----+--------+&#xA;| 43.0|  [43.0]|&#xA;| 45.0|  [45.0]|&#xA;| 42.0|  [42.0]|&#xA;| 60.0|  [60.0]|&#xA;| 50.0|  [50.0]|&#xA;| 59.0|  [59.0]|&#xA;| 61.0|  [61.0]|&#xA;| 47.0|  [47.0]|&#xA;| 49.0|  [49.0]|&#xA;| 41.0|  [41.0]|&#xA;| 34.0|  [34.0]|&#xA;+-----+--------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Evaluating the regression model, I`m getting the following data: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;model.intercept: 1.7363839862169372&#xA;model.coefficients : [0.9640297102666925]&#xA;numIterations: 3&#xA;objectiveHistory: [0.5,0.406233822167566,0.031956224821402285]&#xA;RMSE: 0.29784178261548705&#xA;r2: 0.9987061382565019 --&amp;gt; Extremely High Close to 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;At the end I`m getting the following predictions:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+-----+--------+------------------+&#xA;|label|features|        prediction|&#xA;+-----+--------+------------------+&#xA;| 40.0|  [40.0]| 40.29757239688463|&#xA;| 41.0|  [41.0]|41.261602107151326|&#xA;| 39.0|  [39.0]|39.333542686617946|&#xA;| 36.0|  [36.0]|36.441453555817866|&#xA;| 37.0|  [37.0]| 37.40548326608456|&#xA;| 33.0|  [33.0]| 33.54936442501779|&#xA;| 37.0|  [37.0]| 37.40548326608456|&#xA;+-----+--------+------------------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It is really easy to see that the predictions are not on the same line. It`s impossible to be located on the straight line.&#xA;&lt;a href=&quot;https://i.stack.imgur.com/lqW9B.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;This is whole data set, plotted using the Scala Library- WISP&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/nMWWo.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;Predicted data&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/0YGND.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;Expected result, but done with the WISP&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="9359304" LastEditorUserId="9359304" LastEditDate="2018-03-05T22:58:35.837" LastActivityDate="2018-03-05T22:58:35.837" Title="Linear Regression Apache Spark with Scala not even straight line" Tags="&lt;apache&gt;&lt;scala&gt;&lt;apache-spark&gt;&lt;machine-learning&gt;&lt;apache-spark-mllib&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="49120708" PostTypeId="1" CreationDate="2018-03-05T22:43:09.880" Score="1" ViewCount="23" Body="&lt;p&gt;I have a long running Spark streaming job that runs on a kerberized Hadoop cluster. It fails every few days with the following error:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Diagnostics: token (token for XXXXXXX: HDFS_DELEGATION_TOKEN owner=XXXXXXXXX@XX.COM, renewer=yarn, realUser=, issueDate=XXXXXXXXXXXXXXX, maxDate=XXXXXXXXXX, sequenceNumber=XXXXXXXX, masterKeyId=XXX) can't be found in cache &lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I tried adding in --keytab and --principal options to spark-submit. But we already have the following options that do the same thing:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;For the second option, we already pass in the keytab and principal with the following:&#xA;  'spark.driver.extraJavaOptions=-Djava.security.auth.login.config=kafka_client_jaas.conf -Djava.security.krb5.conf=krb5.conf -XX:+UseCompressedOops -XX:+UseG1GC -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=12' \&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Same for spark.executor.extraJavaOptions. If we add the options --principal and --keytab it results in attempt to add file (keytab) multiple times to distributed cache&lt;/p&gt;&#xA;" OwnerUserId="9169829" LastEditorUserId="9169829" LastEditDate="2018-03-07T16:59:24.210" LastActivityDate="2018-03-07T16:59:24.210" Title="Kerberos ticket renewal on Spark streaming job that communicates to Kafka" Tags="&lt;apache-spark&gt;&lt;spark-streaming&gt;&lt;kerberos&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49120717" PostTypeId="1" CreationDate="2018-03-05T22:43:52.550" Score="0" ViewCount="19" Body="&lt;p&gt;I am new to spark and kafka. We have a requirement to integrate kafka+spark+Hbase(with Phoenix).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;ERROR: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Exception in thread &quot;main&quot; java.sql.SQLException: ERROR 2007 (INT09): Outdated jars. The following servers require an updated phoenix.jar to be put in the classpath of HBase:&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I ended up with the above ERROR. If anybody could you please help how to resolve this issue.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Below is error log:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;jdbc:phoenix:localhost.localdomain:2181:/hbase-unsecure&lt;br&gt;&#xA;      testlocalhost.localdomain:6667&lt;br&gt;&#xA;  18/03/05 16:18:52 INFO Metrics: Initializing metrics system: phoenix&lt;br&gt;&#xA;  18/03/05 16:18:52 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-phoenix.properties,hadoop-metrics2.properties&#xA;  18/03/05 16:18:52 INFO MetricsSystemImpl: Scheduled snapshot period at 10 second(s).&#xA;  18/03/05 16:18:52 INFO MetricsSystemImpl: phoenix metrics system started&#xA;  18/03/05 16:18:52 INFO ConnectionManager$HConnectionImplementation: Closing master protocol: MasterService&#xA;  18/03/05 16:18:52 INFO ConnectionManager$HConnectionImplementation: Closing zookeeper sessionid=0x161f6fc5e4800a3&#xA;  18/03/05 16:18:52 INFO ZooKeeper: Session: 0x161f6fc5e4800a3 closed&#xA;  18/03/05 16:18:52 INFO ClientCnxn: EventThread shut down&#xA;  Exception in thread &quot;main&quot; java.sql.SQLException: ERROR 2007 (INT09): Outdated jars. The following servers require an updated phoenix.jar to be put in the classpath of HBase: region=SYSTEM.CATALOG,,1519831518459.b16e566d706c68469922eba74844a444., hostname=localhost,16020,1520282812066, seqNum=59&#xA;      at org.apache.phoenix.exception.SQLExceptionCode$Factory$1.newException(SQLExceptionCode.java:476)&#xA;      at org.apache.phoenix.exception.SQLExceptionInfo.buildException(SQLExceptionInfo.java:150)&#xA;      at org.apache.phoenix.query.ConnectionQueryServicesImpl.checkClientServerCompatibility(ConnectionQueryServicesImpl.java:1272)&#xA;      at org.apache.phoenix.query.ConnectionQueryServicesImpl.ensureTableCreated(ConnectionQueryServicesImpl.java:1107)&#xA;      at org.apache.phoenix.query.ConnectionQueryServicesImpl.createTable(ConnectionQueryServicesImpl.java:1429)&#xA;      at org.apache.phoenix.schema.MetaDataClient.createTableInternal(MetaDataClient.java:2574)&#xA;      at org.apache.phoenix.schema.MetaDataClient.createTable(MetaDataClient.java:1024)&#xA;      at org.apache.phoenix.compile.CreateTableCompiler$2.execute(CreateTableCompiler.java:212)&#xA;      at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:358)&#xA;      at org.apache.phoenix.jdbc.PhoenixStatement$2.call(PhoenixStatement.java:341)&#xA;      at org.apache.phoenix.call.CallRunner.run(CallRunner.java:53)&#xA;      at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation(PhoenixStatement.java:339)&#xA;      at org.apache.phoenix.jdbc.PhoenixStatement.executeUpdate(PhoenixStatement.java:1492)&#xA;      at org.apache.phoenix.query.ConnectionQueryServicesImpl$12.call(ConnectionQueryServicesImpl.java:2437)&#xA;      at org.apache.phoenix.query.ConnectionQueryServicesImpl$12.call(ConnectionQueryServicesImpl.java:2382)&#xA;      at org.apache.phoenix.util.PhoenixContextExecutor.call(PhoenixContextExecutor.java:76)&#xA;      at org.apache.phoenix.query.ConnectionQueryServicesImpl.init(ConnectionQueryServicesImpl.java:2382)&#xA;      at org.apache.phoenix.jdbc.PhoenixDriver.getConnectionQueryServices(PhoenixDriver.java:255)&#xA;      at org.apache.phoenix.jdbc.PhoenixEmbeddedDriver.createConnection(PhoenixEmbeddedDriver.java:149)&#xA;      at org.apache.phoenix.jdbc.PhoenixDriver.connect(PhoenixDriver.java:221)&#xA;      at java.sql.DriverManager.getConnection(DriverManager.java:664)&#xA;      at java.sql.DriverManager.getConnection(DriverManager.java:270)&#xA;      at com.spark.kafka.PhoenixJdbcClient.getConnection(PhoenixJdbcClient.scala:41)&#xA;      at com.spark.kafka.PhoenixJdbcClient.currentTableSchema(PhoenixJdbcClient.scala:595)&#xA;      at com.spark.kafka.SparkHBaseClient$.main(SparkHBaseClient.scala:47)&#xA;      at com.spark.kafka.SparkHBaseClient.main(SparkHBaseClient.scala)&#xA;  18/03/05 16:18:52 INFO SparkContext: Invoking stop() from shutdown hook&#xA;  18/03/05 16:18:52 INFO SparkUI: Stopped Spark web UI at &lt;a href=&quot;http://192.168.1.103:4040&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://192.168.1.103:4040&lt;/a&gt;&#xA;  18/03/05 16:18:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!&#xA;  18/03/05 16:18:53 INFO MemoryStore: MemoryStore cleared&#xA;  18/03/05 16:18:53 INFO BlockManager: BlockManager stopped&#xA;  18/03/05 16:18:53 INFO BlockManagerMaster: BlockManagerMaster stopped&#xA;  18/03/05 16:18:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!&#xA;  18/03/05 16:18:53 INFO SparkContext: Successfully stopped SparkContext&#xA;  18/03/05 16:18:53 INFO ShutdownHookManager: Shutdown hook called&#xA;  18/03/05 16:18:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-c8dd26fc-74dd-40fb-a339-8c5dda36b973&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;We are using &lt;em&gt;Amabri Server 2.6.1.3&lt;/em&gt; with &lt;em&gt;HDP-2.6.3.0&lt;/em&gt; and below components:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Hbase-1.1.2&lt;/li&gt;&#xA;&lt;li&gt;kafka-0.10.1&lt;/li&gt;&#xA;&lt;li&gt;spark-2.2.0&lt;/li&gt;&#xA;&lt;li&gt;phoenix &lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Below are the POM artifact's I have added for HBase and Phoenix.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;org.apache.hbase&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;hbase-client&amp;lt;/artifactId&amp;gt;&#xA;            &amp;lt;version&amp;gt;1.3.1&amp;lt;/version&amp;gt;&#xA;        &amp;lt;/dependency&amp;gt;&#xA;        &amp;lt;dependency&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;org.apache.hbase&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;hbase-common&amp;lt;/artifactId&amp;gt;&#xA;            &amp;lt;version&amp;gt;1.3.1&amp;lt;/version&amp;gt;&#xA;        &amp;lt;/dependency&amp;gt;&#xA;        &amp;lt;dependency&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;org.apache.hbase&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;hbase-protocol&amp;lt;/artifactId&amp;gt;&#xA;            &amp;lt;version&amp;gt;1.3.1&amp;lt;/version&amp;gt;&#xA;        &amp;lt;/dependency&amp;gt;&#xA;        &amp;lt;dependency&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;org.apache.hbase&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;hbase-server&amp;lt;/artifactId&amp;gt;&#xA;            &amp;lt;version&amp;gt;1.3.1&amp;lt;/version&amp;gt;&#xA;        &amp;lt;/dependency&amp;gt;&#xA;&#xA;        &amp;lt;dependency&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;org.apache.phoenix&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;phoenix-spark&amp;lt;/artifactId&amp;gt;&#xA;            &amp;lt;version&amp;gt;4.10.0-HBase-1.2&amp;lt;/version&amp;gt;&#xA;        &amp;lt;/dependency&amp;gt;&#xA;        &amp;lt;dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="9381505" LastEditorUserId="9381505" LastEditDate="2018-03-06T13:27:41.000" LastActivityDate="2018-03-06T13:27:41.000" Title="java.sql.SQLException: ERROR 2007 (INT09): Outdated jars" Tags="&lt;apache-spark&gt;&lt;apache-kafka&gt;&lt;hbase&gt;&lt;phoenix&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="49120835" PostTypeId="1" AcceptedAnswerId="49128659" CreationDate="2018-03-05T22:55:13.957" Score="2" ViewCount="60" Body="&lt;p&gt;I have a &lt;code&gt;data frame&lt;/code&gt; in &lt;code&gt;pyspark&lt;/code&gt;. This data frame has say some columns with special characters.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cols = df.schema.names&#xA;&#xA;cols&#xA;['abc test', 'test*abc', 'eng)test', 'abc_&amp;amp;test']&#xA;&#xA;reps = ((' ', '_&amp;amp;'), ('(', '*_'), (')', '_*'), ('{', '#_'), ('}', '_#'), (';', '_##'), ('.', '_$'), (',', '_$$'), ('=', '_**'))&#xA;&#xA;def col_rename(x):&#xA;    new_cols = reduce(lambda a, kv: a.replace(*kv), reps, x)&#xA;&#xA;for i in cols:&#xA;    df = df.withColumnRenamed(i, col_rename(cols, i))&#xA;return df&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now I want to see if after replacing the special characters in the column names if there are any duplicate columns.&#xA;As we can see there is a duplicate of columns in the new_cols &lt;code&gt;abc_&amp;amp;test&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to return extra &lt;code&gt;_&lt;/code&gt;  &lt;code&gt;underscore&lt;/code&gt; when this happens.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My new_cols shoul be like below&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;['abc__&amp;amp;test', 'test*_abc', 'eng_*test', 'abc_&amp;amp;test']&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How can I achieve what I want?&lt;/p&gt;&#xA;" OwnerUserId="9367133" LastEditorUserId="9367133" LastEditDate="2018-03-07T05:00:13.710" LastActivityDate="2018-03-07T06:27:57.870" Title="Replace duplicate columns in data frame" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49120894" PostTypeId="1" CreationDate="2018-03-05T23:00:18.423" Score="0" ViewCount="20" Body="&lt;p&gt;I am trying to output a dataframe only with columns identified with different values after comparing two dataframes. I am finding difficulty in identifying an approach to proceed.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    **Code:**&#xA;df_a = sql_context.createDataFrame([(&quot;a&quot;, 3,&quot;apple&quot;,&quot;bear&quot;,&quot;carrot&quot;), (&quot;b&quot;, 5,&quot;orange&quot;,&quot;lion&quot;,&quot;cabbage&quot;), (&quot;c&quot;, 7,&quot;pears&quot;,&quot;tiger&quot;,&quot;onion&quot;),(&quot;c&quot;, 8,&quot;jackfruit&quot;,&quot;elephant&quot;,&quot;raddish&quot;),(&quot;c&quot;, 8,&quot;watermelon&quot;,&quot;giraffe&quot;,&quot;tomato&quot;)], [&quot;name&quot;, &quot;id&quot;,&quot;fruit&quot;,&quot;animal&quot;,&quot;veggie&quot;])&#xA;df_b = sql_context.createDataFrame([(&quot;a&quot;, 3,&quot;apple&quot;,&quot;bear&quot;,&quot;carrot&quot;), (&quot;b&quot;, 5,&quot;orange&quot;,&quot;lion&quot;,&quot;cabbage&quot;), (&quot;c&quot;, 7,&quot;banana&quot;,&quot;tiger&quot;,&quot;onion&quot;),(&quot;c&quot;, 8,&quot;jackfruit&quot;,&quot;camel&quot;,&quot;raddish&quot;)], [&quot;name&quot;, &quot;id&quot;,&quot;fruit&quot;,&quot;animal&quot;,&quot;veggie&quot;])&#xA;df_a = df_a.alias('df_a')&#xA;df_b = df_b.alias('df_b')&#xA;df = df_a.join(df_b, (df_a.id == df_b.id) &amp;amp; (df_a.name == df_b.name),'leftanti').select('df_a.*').show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Trying to match based on the ids (id,name) between dataframe1 &amp;amp; dataframe2&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Dataframe 1:&#xA;+----+---+----------+--------+-------+&#xA;|name| id|     fruit|  animal| veggie|&#xA;+----+---+----------+--------+-------+&#xA;|   a|  3|     apple|    bear| carrot|&#xA;|   b|  5|    orange|    lion|cabbage|&#xA;|   c|  7|     pears|   tiger|  onion|&#xA;|   c|  8| jackfruit|elephant|raddish|&#xA;|   c|  9|watermelon| giraffe| tomato|&#xA;+----+---+----------+--------+-------+&#xA;&#xA;Dataframe 2:&#xA;+----+---+---------+------+-------+&#xA;|name| id|    fruit|animal| veggie|&#xA;+----+---+---------+------+-------+&#xA;|   a|  3|    apple|  bear| carrot|&#xA;|   b|  5|   orange|  lion|cabbage|&#xA;|   c|  7|   banana| tiger|  onion|&#xA;|   c|  8|jackfruit| camel|raddish|&#xA;+----+---+---------+------+-------+&#xA;&#xA;&#xA;&#xA;Expected dataframe&#xA;+----+---+----------+--------+&#xA;|name| id|     fruit|  animal|&#xA;+----+---+----------+--------+&#xA;|   c|  7|     pears|   tiger|&#xA;|   c|  8| jackfruit|elephant|&#xA;|   c|  9|watermelon| giraffe|&#xA;+----+---+----------+--------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="8742569" LastEditorUserId="8742569" LastEditDate="2018-03-06T12:51:07.340" LastActivityDate="2018-03-06T12:51:07.340" Title="pyspark two dataframes subtractbykey issue" Tags="&lt;python-3.x&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;comparison&gt;&lt;spark-dataframe&gt;" AnswerCount="0" CommentCount="7" />
  <row Id="49120965" PostTypeId="1" CreationDate="2018-03-05T23:07:50.913" Score="0" ViewCount="15" Body="&lt;p&gt;I have installed Pyspark and Findspark using conda environment and added their paths to environment variables.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I execute following code:&lt;br&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import findspark&#xA;import pyspark&#xA;findspark.find()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I get the output as:&lt;br&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;'C:/Users/myname/AppData/Local/Continuum/anaconda3/Scripts'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then I execute:&lt;br&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;findspark.init(&quot;C:/Users/myname/AppData/Local/Continuum/anaconda3/Scripts&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The output I get is:&lt;br&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/Bxg9h.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Bxg9h.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="8850552" LastActivityDate="2018-03-05T23:34:15.250" Title="Unable to set-up Pyspark" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;installation&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49121822" PostTypeId="1" CreationDate="2018-03-06T00:56:17.027" Score="0" ViewCount="22" Body="&lt;p&gt;I would like to read some data from kafka using pySpark and write to the console. I have tried the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark = SparkSession.builder.appName(&quot;test&quot;).getOrCreate()&#xA;df = spark.readStream.format(&quot;kafka&quot;) \&#xA;    .option(&quot;kafka.bootstrap.servers&quot;, &quot;a.b.com:9092&quot;) \&#xA;    .option(&quot;subscribe&quot;, &quot;mytopic&quot;) \&#xA;    .option(&quot;startingOffsets&quot;, &quot;earliest&quot;) \&#xA;    .load()&#xA;data = df.selectExpr(&quot;CAST(value AS STRING)&quot;)&#xA;data.writeStream.outputMode(&quot;Append&quot;).format(&quot;console&quot;). \&#xA;    option(&quot;truncate&quot;,&quot;false&quot;).start().awaitTermination()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;but no data is printed to the console. I see this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;18/03/06 00:40:19 INFO kafka010.KafkaSource: GetBatch called with start = None, end = {&quot;mytopic&quot;:{&quot;8&quot;:63&#xA;54907922,&quot;11&quot;:6356757239,&quot;2&quot;:6337382595,&quot;5&quot;:6337469233,&quot;14&quot;:6343793980,&quot;13&quot;:6342800638,&quot;4&quot;:6347020892,&quot;7&quot;:6355652271,&quot;1&quot;:6&#xA;346118560,&quot;10&quot;:6339632595,&quot;9&quot;:6346766862,&quot;3&quot;:6359468983,&quot;12&quot;:6347305301,&quot;15&quot;:6351615474,&quot;6&quot;:6340953390,&quot;0&quot;:6352163282}}   &#xA;18/03/06 00:40:19 INFO kafka010.KafkaSource: Partitions added: Map()                                                   &#xA;18/03/06 00:40:19 INFO kafka010.KafkaSource: GetBatch generating RDD of offset range: KafkaSourceRDDOffsetRange(mytopic-0,6079653944,6352163282,Some(executor_a1.b.com_2)), KafkaSourceRDDOffsetRange(mytopic-1,6074484008,6346118560,Some(executor_a2.b.com_1)), KafkaSourceRDDOffsetRange(mytopic-10,6067060882,6339632595,Some(executor_a3.b.com_2)), ...)                                                                                                                &#xA;&#xA;Batch: 0                                    &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If I try to use console-consumer.sh against the same topic, I do see data. What am I doing wrong? I'm using spark 2.2.0&lt;/p&gt;&#xA;" OwnerUserId="1757991" LastEditorUserId="350613" LastEditDate="2018-03-06T11:26:39.660" LastActivityDate="2018-03-06T11:26:39.660" Title="How to write a kafka console consumer in python" Tags="&lt;python&gt;&lt;apache-kafka&gt;&lt;spark-structured-streaming&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49122064" PostTypeId="1" CreationDate="2018-03-06T01:27:13.043" Score="0" ViewCount="102" Body="&lt;p&gt;I am new to Pyspark and nothing seems to be working out. Please rescue.&#xA;I want to read a parquet file with Pyspark. I wrote the following codes.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark.sql import SQLContext&#xA;&#xA;sqlContext = SQLContext(sc)&#xA;&#xA;sqlContext.read.parquet(&quot;my_file.parquet&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I got the following error&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Py4JJavaError                            Traceback (most recent call&#xA;  last) /usr/local/spark/python/pyspark/sql/utils.py in deco(*a, **kw)&#xA;       62         try:&#xA;  ---&gt; 63             return f(*a, **kw)&#xA;       64         except py4j.protocol.Py4JJavaError as e:&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in&#xA;  get_return_value(answer, gateway_client, target_id, name)&#xA;      318                     &quot;An error occurred while calling {0}{1}{2}.\n&quot;.&#xA;  --&gt; 319                     format(target_id, &quot;.&quot;, name), value)&#xA;      320             else:&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;then I tried the following codes &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark.sql import SQLContext&#xA;&#xA;sc = SparkContext.getOrCreate()&#xA;&#xA;SQLContext.read.parquet(&quot;my_file.parquet&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then the error was as follows :&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;AttributeError: 'property' object has no attribute 'parquet'&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="5363621" LastEditorUserId="6682076" LastEditDate="2018-03-06T06:27:35.963" LastActivityDate="2018-03-06T09:31:16.983" Title="Reading parquet file with PySpark" Tags="&lt;pyspark&gt;&lt;parquet&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49122232" PostTypeId="1" CreationDate="2018-03-06T01:48:34.210" Score="0" ViewCount="13" Body="&lt;p&gt;Sorry if this is already posted.  I am trying to upload a local file to Zeppelin using Apache Spark through Zeppelin.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;%livy.pyspark&#xA;master = spark.sparkContext.textFile(&quot;/private/tmp/Master.csv&quot;)&#xA;master1 = master.filter(lambda x: 'lahmanID'not in x)&#xA;master1.count()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It does not give me an error until I put in the count command.  Here is the error.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;An error occurred while calling &#xA;z:org.apache.spark.api.python.PythonRDD.collectAndServe.&#xA;: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: &#xA;wasb://week7-2018-03-05t23-16-43-&#xA;761z@lewis.blob.core.windows.net/private/tmp/Master.csv&#xA;at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)&#xA;at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)&#xA;at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)&#xA;at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:202)&#xA;at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)&#xA;at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)&#xA;at scala.Option.getOrElse(Option.scala:121)&#xA;at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)&#xA;at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)&#xA;at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)&#xA;at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)&#xA;at scala.Option.getOrElse(Option.scala:121)&#xA;at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)&#xA;at org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:53)&#xA;at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:252)&#xA;at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:250)&#xA;at scala.Option.getOrElse(Option.scala:121)&#xA;at org.apache.spark.rdd.RDD.partitions(RDD.scala:250)&#xA;at org.apache.spark.SparkContext.runJob(SparkContext.scala:1968)&#xA;at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)&#xA;at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&#xA;at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)&#xA;at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)&#xA;at org.apache.spark.rdd.RDD.collect(RDD.scala:935)&#xA;at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)&#xA;at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)&#xA;at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)&#xA;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#xA;at java.lang.reflect.Method.invoke(Method.java:498)&#xA;at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#xA;at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#xA;at py4j.Gateway.invoke(Gateway.java:280)&#xA;at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#xA;at py4j.commands.CallCommand.execute(CallCommand.java:79)&#xA;at py4j.GatewayConnection.run(GatewayConnection.java:214)&#xA;at java.lang.Thread.run(Thread.java:748)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="8535241" LastActivityDate="2018-03-06T01:48:34.210" Title="Spark Zeppelin uploading errors using Python" Tags="&lt;python&gt;&lt;azure&gt;&lt;apache-spark&gt;&lt;zeppelin&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49122326" PostTypeId="1" CreationDate="2018-03-06T02:02:35.360" Score="1" ViewCount="16" Body="&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/Gyn8e.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/Gyn8e.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have projects with dependency as shown. The &lt;code&gt;own spark library&lt;/code&gt; depends on &lt;code&gt;hadoop&lt;/code&gt; library. Spark internally uses log4j for logging which I found difficult to change. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;log4j&lt;/code&gt; library has been excluded from all other projects and we use &lt;code&gt;logback&lt;/code&gt; everywhere. I basically want to write &lt;code&gt;sifting appender&lt;/code&gt; for the &lt;code&gt;log4j&lt;/code&gt; logger.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Trans and IE are the projects where I write logs. At runtime, the logger is automatically set to &lt;code&gt;log4j&lt;/code&gt;. However, since log4j library is excluded, I can't write any code to implement sifting appender.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I achieve this?&lt;/p&gt;&#xA;" OwnerUserId="3288346" LastActivityDate="2018-03-06T02:02:35.360" Title="Implementing sifting appender for log4j logger" Tags="&lt;maven&gt;&lt;apache-spark&gt;&lt;logging&gt;&lt;log4j&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49122354" PostTypeId="1" AcceptedAnswerId="49144069" CreationDate="2018-03-06T02:07:27.797" Score="0" ViewCount="36" Body="&lt;p&gt;I've never heard of this but it happens here:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a dataframe generated through sqlContext.sql operation in Scala.&#xA;Presumably there should be 12 records as shown in its mysql data source.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I use:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;result.show(7)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;7 results are shown up normally;&#xA;When I use:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;result.show(8)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I received the following error:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;18/03/05 21:02:48 INFO HadoopRDD: Input split:&#xA;  hdfs://nn01.itversity.com:8020/user/paslechoix/products/part-m-00000:0+86996&#xA;  18/03/05 21:02:48 INFO BlockManagerInfo: Removed broadcast_20_piece0&#xA;  on localhost:42892 in memory (size: 2008.0 B, free: 511.1 MB) 18/03/05&#xA;  21:02:48 ERROR Executor: Exception in task 0.0 in stage 19.0 (TID 22)&#xA;  &lt;strong&gt;java.lang.NumberFormatException: empty String&lt;/strong&gt;&#xA;          at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1842)&#xA;          at sun.misc.FloatingDecimal.parseFloat(FloatingDecimal.java:122)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;It would be greatly appreciated if someone can tell me what's wrong here?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The expected result from mysql is as below for your reference:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    mysql&amp;gt; select * from products where product_name like 'Per%';&#xA;+------------+---------------------+--------------------------------------------+---------------------+---------------+----------------------------------------------------------------------------+&#xA;| product_id | product_category_id | product_name                               | product_description | product_price | product_image                                                              |&#xA;+------------+---------------------+--------------------------------------------+---------------------+---------------+----------------------------------------------------------------------------+&#xA;|        362 |                  17 | Perfect Fitness Perfect Ab Strap Pro       |                     |         29.99 | http://images.acmesports.sports/Perfect+Fitness+Perfect+Ab+Strap+Pro       |&#xA;|        365 |                  17 | Perfect Fitness Perfect Rip Deck           |                     |         59.99 | http://images.acmesports.sports/Perfect+Fitness+Perfect+Rip+Deck           |&#xA;|        372 |                  17 | Perfect Ab Carver Pro                      |                     |         39.99 | http://images.acmesports.sports/Perfect+Ab+Carver+Pro                      |&#xA;|        373 |                  17 | Perfect Fitness Multi Gym Pro              |                     |         39.99 | http://images.acmesports.sports/Perfect+Fitness+Multi+Gym+Pro              |&#xA;|        374 |                  17 | Perfect Pushup BASIC                       |                     |         19.99 | http://images.acmesports.sports/Perfect+Pushup+BASIC                       |&#xA;|        376 |                  17 | Perfect Pushup V2 Performance              |                     |         29.99 | http://images.acmesports.sports/Perfect+Pushup+V2+Performance              |&#xA;|        377 |                  17 | Perfect Pullup Basic                       |                     |         19.99 | http://images.acmesports.sports/Perfect+Pullup+Basic                       |&#xA;|        379 |                  17 | Perfect Multi-Gym - As Seen on TV!         |                     |         29.99 | http://images.acmesports.sports/Perfect+Multi-Gym+-+As+Seen+on+TV%21       |&#xA;|       1013 |                  46 | Perception Sport Swifty Deluxe 9.5 Kayak   |                     |        349.99 | http://images.acmesports.sports/Perception+Sport+Swifty+Deluxe+9.5+Kayak   |&#xA;|       1030 |                  46 | Perception Sport Striker 11.5 Angler Kayak |                     |        499.99 | http://images.acmesports.sports/Perception+Sport+Striker+11.5+Angler+Kayak |&#xA;|       1065 |                  48 | Perception Sport Swifty Deluxe 9.5 Kayak   |                     |        349.99 | http://images.acmesports.sports/Perception+Sport+Swifty+Deluxe+9.5+Kayak   |&#xA;|       1093 |                  49 | Perception Sport Swifty Deluxe 9.5 Kayak   |                     |        349.99 | http://images.acmesports.sports/Perception+Sport+Swifty+Deluxe+9.5+Kayak   |&#xA;+------------+---------------------+--------------------------------------------+---------------------+---------------+----------------------------------------------------------------------------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If I run the script below:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val results12 = sqlContext.sql(&quot;SELECT * FROM products&quot;) &#xA;results12.show() &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It returns default 20 records with no error.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If you need extra info from the error, let me know and I will post the full error message. Thank you very much.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Update 1:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I do:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val result2 = sqlContext.sql(&quot;select * from products where productID = 379&quot;)&#xA;result2.show(1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I receive the following error:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;java.lang.NumberFormatException: empty String&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;So it seems 379 is causing the problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I continue with 377, same error message when &lt;code&gt;show(1)&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However I don't understand that 377 is included in result1:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val result1 = sqlContext.sql(&quot;select * from products where name like 'Per%'&quot;)&#xA;&#xA;scala&amp;gt; result1.show(7)&#xA;&#xA;+---------+-----------+--------------------+--------+-----+--------------------+&#xA;|productID|productCode|                name|quantity|price|               image|&#xA;+---------+-----------+--------------------+--------+-----+--------------------+&#xA;|      362|         17|Perfect Fitness P...|        |29.99|http://images.acm...|&#xA;|      365|         17|Perfect Fitness P...|        |59.99|http://images.acm...|&#xA;|      372|         17|Perfect Ab Carver...|        |39.99|http://images.acm...|&#xA;|      373|         17|Perfect Fitness M...|        |39.99|http://images.acm...|&#xA;|      374|         17|Perfect Pushup BASIC|        |19.99|http://images.acm...|&#xA;|      376|         17|Perfect Pushup V2...|        |29.99|http://images.acm...|&#xA;|      377|         17|Perfect Pullup Basic|        |19.99|http://images.acm...|&#xA;+---------+-----------+--------------------+--------+-----+--------------------+&#xA;only showing top 7 rows&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="9074574" LastEditorUserId="7579547" LastEditDate="2018-03-06T05:37:18.487" LastActivityDate="2018-03-07T04:21:18.170" Title="Error when showing x amount of result for a Spark Dataframe" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;apache-spark-sql&gt;" AnswerCount="2" CommentCount="10" />
  <row Id="49122468" PostTypeId="1" CreationDate="2018-03-06T02:23:04.380" Score="0" ViewCount="34" Body="&lt;p&gt;I would like to save RDD to text file grouped by key, currently I can't figure out how to split the output to multiple files, it seems all the output spanning across multiple keys which share the same partition gets written to the same file. I would like to have different files for each key. Here's my code snippet :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;JavaPairRDD&amp;lt;String, Iterable&amp;lt;Customer&amp;gt;&amp;gt; groupedResults = customerCityPairRDD.groupByKey();&#xA;&#xA;groupedResults.flatMap(x -&amp;gt; x._2().iterator())&#xA;              .saveAsTextFile(outputPath + &quot;/cityCounts&quot;);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="1002509" LastEditorUserId="5880706" LastEditDate="2018-03-06T04:35:47.987" LastActivityDate="2018-03-07T22:10:55.953" Title="Spark Save as Text File grouped by Key" Tags="&lt;java&gt;&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;spark-dataframe&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="0" />
  <row Id="49122563" PostTypeId="1" CreationDate="2018-03-06T02:37:30.123" Score="0" ViewCount="28" Body="&lt;p&gt;but csv file is added with extra double quotes which results all cloumns into single  column &lt;/p&gt;&#xA;&#xA;&lt;p&gt;there are four columns,header and 2 rows   &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&quot;&quot;&quot;SlNo&quot;&quot;,&quot;&quot;Name&quot;&quot;,&quot;&quot;Age&quot;&quot;,&quot;&quot;contact&quot;&quot;&quot;&#xA;&quot;1,&quot;&quot;Priya&quot;&quot;,78,&quot;&quot;Phone&quot;&quot;&quot;&#xA;&quot;2,&quot;&quot;Jhon&quot;&quot;,20,&quot;&quot;mail&quot;&quot;&quot;&#xA;&#xA;val df = sqlContext.read.format(&quot;com.databricks.spark.csv&quot;).option(&quot;header&quot;,&quot;true&quot;).option(&quot;delimiter&quot;,&quot;,&quot;).option(&quot;inferSchema&quot;,&quot;true&quot;).load (&quot;bank.csv&quot;) &#xA;df: org.apache.spark.sql.DataFrame = [&quot;SlNo&quot;,&quot;Name&quot;,&quot;Age&quot;,&quot;contact&quot;: string]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="7429472" LastEditorUserId="5880706" LastEditDate="2018-03-06T02:40:26.763" LastActivityDate="2018-03-06T03:10:16.847" Title="creating dataframe by loading csv file using scala in spark" Tags="&lt;scala&gt;&lt;csv&gt;&lt;apache-spark&gt;&lt;dataframe&gt;&lt;apache-spark-sql&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49122656" PostTypeId="1" AcceptedAnswerId="49127234" CreationDate="2018-03-06T02:50:39.297" Score="0" ViewCount="21" Body="&lt;p&gt;I am using anaconda python and installed pyspark on top of it. In the pyspark program, I am using the dataframe as the data structure. The program goes like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark.sql import SparkSession&#xA;&#xA;spark_session = SparkSession.builder.appName(&quot;test&quot;).getOrCreate()&#xA;sdf = spark_session.read.orc(&quot;../data/&quot;)&#xA;sdf.createOrReplaceTempView(&quot;data&quot;)&#xA;df = spark_session.sql(&quot;select field1, field2 from data group by field1&quot;)&#xA;df.write.csv(&quot;result.csv&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;While this works but it is slow and the memory usage is very low (~2GB). There is much more physical memory installed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried to increase the memory usage by:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark import SparkContext&#xA;SparkContext.setSystemProperty('spark.executor.memory', '16g')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But it does not seem to help at all.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ways to speedup the program? Especially, how to fully utilize the system memory?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;" OwnerUserId="5236286" LastActivityDate="2018-03-06T09:17:57.820" Title="pyspark memory consumption is very low" Tags="&lt;dataframe&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="49122708" PostTypeId="1" AcceptedAnswerId="49122834" CreationDate="2018-03-06T02:56:57.470" Score="0" ViewCount="32" Body="&lt;p&gt;I am trying to build an Edge RDD for GraphX. I am reading a csv file and converting to DataFrame Then trying to convert to an Edge RDD:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val staticDataFrame = spark.&#xA;  read.&#xA;  option(&quot;header&quot;, true).&#xA;  option(&quot;inferSchema&quot;, true).&#xA;  csv(&quot;/projects/pdw/aiw_test/aiw/haris/Customers_DDSW-withDN$.csv&quot;)&#xA;&#xA;val edgeRDD: RDD[Edge[(VertexId, VertexId, String)]]  = &#xA;  staticDataFrame.select(&#xA;    &quot;dealer_customer_number&quot;,&#xA;    &quot;parent_dealer_cust_number&quot;,&#xA;    &quot;dealer_code&quot;&#xA;  ).map{ (row: Array) =&amp;gt; &#xA;    Edge((&#xA;      row.getAs[Long](&quot;dealer_customer_number&quot;), &#xA;      row.getAs[Long](&quot;parent_dealer_cust_number&quot;),&#xA;      row(&quot;dealer_code&quot;)&#xA;    ))&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But I am getting this error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;console&amp;gt;:81: error: class Array takes type parameters&#xA;       val edgeRDD: RDD[Edge[(VertexId, VertexId, String)]]  = staticDataFrame.select(&quot;dealer_customer_number&quot;, &quot;parent_dealer_cust_number&quot;, &quot;dealer_code&quot;).map((row: Array) =&amp;gt; Edge((row.getAs[Long](&quot;dealer_customer_number&quot;), row.getAs[Long](&quot;parent_dealer_cust_number&quot;), row(&quot;dealer_code&quot;))))&#xA;                                                                                                                                                                      ^&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The result for&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;staticDataFrame.select(&quot;dealer_customer_number&quot;, &quot;parent_dealer_cust_number&quot;, &quot;dealer_code&quot;).take(1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;is&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;res3: Array[org.apache.spark.sql.Row] = Array([0000101,null,B110])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="7788725" LastEditorUserId="2707792" LastEditDate="2018-03-06T03:11:03.920" LastActivityDate="2018-03-06T03:17:20.570" Title="Scala: GraphX: error: class Array takes type parameters" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;spark-graphx&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="49122833" PostTypeId="1" CreationDate="2018-03-06T03:16:58.460" Score="0" ViewCount="32" Body="&lt;p&gt;Am using now kafka in Python.&#xA;Was wondering if Spark Kafka is needed or can we use just use kafka&#xA;through pyKafka.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My concern was Spark creates overhead (pyspark) in the process,&#xA;and if we don't use any spark functions, just Kafka streaming is required.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What are the inconvenients of using Pyspark and kafka spark ?&lt;/p&gt;&#xA;" OwnerUserId="6368214" LastEditorUserId="2308683" LastEditDate="2018-03-06T03:57:31.267" LastActivityDate="2018-03-06T03:57:31.267" Title="kafka streaming or spark streaming" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;&lt;apache-kafka&gt;" AnswerCount="0" CommentCount="5" />
  <row Id="49122920" PostTypeId="1" CreationDate="2018-03-06T03:29:46.347" Score="0" ViewCount="17" Body="&lt;p&gt;I am trying to run the following command in Scala 2.2 &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;   val x_test0 = cn_train.map( { case row =&amp;gt; row.toSeq.toArray } )&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And I keep getting the following mistake&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; error: Unable to find encoder for type stored in a Dataset.  Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have already imported implicits._ through the following commands:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val spark = SparkSession.builder.getOrCreate()&#xA;import spark.implicits._&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="9449033" LastActivityDate="2018-03-06T03:44:36.723" Title="Scala - Encoder missing for type stored in dataset" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;encoder&gt;" AnswerCount="1" CommentCount="6" />
  <row Id="49123227" PostTypeId="1" CreationDate="2018-03-06T04:07:41.397" Score="0" ViewCount="53" Body="&lt;p&gt;I need to collect a few key pieces of information from a large number of somewhat complex nested JSON messages which are evolving over time. Each message refers to the same type of event but the messages are generated by several producers and come in two (and likely more in the future) schemas. The key information from each message is similar but the mapping to those fields is dependent on the message type.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can’t share the actual data but here is an example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Message A&#xA;—header:&#xA;|—attribute1&#xA;|—attribute2&#xA;—typeA:&#xA;|—typeAStruct1:&#xA;||—property1&#xA;|-typeAStruct2:&#xA;||-property2&#xA;&#xA;&#xA;Message B&#xA;-attribute1&#xA;-attribute2&#xA;-contents:&#xA;|-message:&#xA;||-TypeB:&#xA;|||-property1&#xA;|||-TypeBStruct:&#xA;||||-property2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to produce a table of data which looks something like this regardless of message type:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;| MessageSchema | Property1 | Property2 |&#xA;| :———————————- | :———————— | :———————— |&#xA;| MessageA      | A1        | A2        |&#xA;| MessageB      | B1        | B2        |&#xA;| MessageA      | A3        | A4        |&#xA;| MessageB      | B3        | B4        |&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My current strategy is read the data with schema A and union with the data read with Schema B. Then I can filter the nulls that result from parsing a type A message with a B schema and vice versa. This seems very inefficient especially once a third or fourth schema emerge. I would like to be able to parse the message correctly on the first pass and apply the correct schema.&lt;/p&gt;&#xA;" OwnerUserId="5318242" LastEditorUserId="2308683" LastEditDate="2018-03-06T04:08:55.503" LastActivityDate="2018-03-06T20:00:25.690" Title="Parsing multiple JSON schemas with Spark" Tags="&lt;json&gt;&lt;hadoop&gt;&lt;apache-spark&gt;&lt;apache-spark-sql&gt;" AnswerCount="1" CommentCount="3" FavoriteCount="1" />
  <row Id="49123944" PostTypeId="1" CreationDate="2018-03-06T05:27:52.413" Score="0" ViewCount="28" Body="&lt;p&gt;I have a .dat file with (\u0002\n) as row delimiter and (\u0001) as column delimiter. I am able to get only 1 record in spark DataFrame when I use this approach.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sc.hadoopConfiguration.set(&quot;textinputformat.record.delimiter&quot;, unescapeJava(rowDelim));&#xA;    val header = Seq(&quot;col0&quot;, &quot;col1&quot;, &quot;col2&quot;)&#xA;    val schema = StructType(header.map(name =&amp;gt; StructField(name, StringType)))&#xA;&#xA;// Load data as RDD&#xA;val dataFileTypVal = escapeJava(&quot;\u0001&quot;);&#xA;val datafile = sc.textFile(&quot;some dat file path&quot;)&#xA;&#xA;// Convert to Row RDD&#xA;&#xA;val rdd1 = datafile.map(_.split( unescapeJava(dataFileTypVal) )).map(arr =&amp;gt; Row.fromSeq(arr))&#xA;val rdd2 =  datafile.map(_.split( unescapeJava(dataFileTypVal) ).to[List]).map(arr =&amp;gt; Row.fromSeq(arr)) &#xA;&#xA;// Create DataFrame from Row RDD and schema&#xA;&#xA;val df1 = sqlContext.createDataFrame(rdd1, schema)&#xA;val df2 = sqlContext.createDataFrame(rdd2, schema)&#xA;&#xA;But df1.show() return only first row &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;//df1, df2 -&gt; return only 1 row.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+----+----+-----+&#xA;|col0|col1| col2|&#xA;+----+----------+&#xA;| A1 | B1 | C1  |&#xA;+----+----------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But my file has 3 rows and I am able to see all 3 rows shown when I print as &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;rdd1.collect().foreach(println)&#xA;[A1,B1,C1&#xA; A2,B2,C2&#xA; A3,B3,C3&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How do I get all records from .dat file into Dataframe&lt;/p&gt;&#xA;" OwnerUserId="5484751" LastEditorUserId="5484751" LastEditDate="2018-03-06T19:00:20.933" LastActivityDate="2018-03-06T19:00:20.933" Title="Create Dataframe from custom row delim and custom column delim file from dat file" Tags="&lt;apache-spark&gt;&lt;dataframe&gt;&lt;spark-dataframe&gt;&lt;rdd&gt;&lt;databricks&gt;" AnswerCount="0" CommentCount="3" FavoriteCount="0" />
  <row Id="49123993" PostTypeId="1" CreationDate="2018-03-06T05:32:26.787" Score="0" ViewCount="19" Body="&lt;p&gt;i am trying to submit my spark job using python 3.5.2 v the job executed successfully but i have WARN on python i attached message screenshot could you any one find solution &lt;a href=&quot;https://i.stack.imgur.com/DKikv.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;please click this link to see the Error massage&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;        at java.lang.Thread.run(Thread.java:745)&#xA;18/03/06 12:44:45 WARN ScriptBasedMapping: Exception running /etc/hadoop/conf/topology_script.py 10.1.7.90&#xA;ExitCodeException exitCode=1:   File &quot;/opt/python3/lib/python3.5/site.py&quot;, line 176&#xA;    file=sys.stderr)&#xA;        ^&#xA;SyntaxError: invalid syntax&#xA;&#xA;        at org.apache.hadoop.util.Shell.runCommand(Shell.java:933)&#xA;        at org.apache.hadoop.util.Shell.run(Shell.java:844)&#xA;        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1123)&#xA;        at org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping.runResolveCommand(ScriptBasedMapping.java:251)&#xA;        at org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping.resolve(ScriptBasedMapping.java:188)&#xA;        at org.apache.hadoop.net.CachedDNSToSwitchMapping.resolve(CachedDNSToSwitchMapping.java:119)&#xA;        at org.apache.hadoop.yarn.util.RackResolver.coreResolve(RackResolver.java:101)&#xA;        at org.apache.hadoop.yarn.util.RackResolver.resolve(RackResolver.java:81)&#xA;        at org.apache.spark.scheduler.cluster.YarnScheduler.getRackForHost(YarnScheduler.scala:37)&#xA;        at org.apache.spark.scheduler.TaskSetManager.dequeueTask(TaskSetManager.scala:388)&#xA;        at org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:437)&#xA;        at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$org$apache$spark$scheduler$TaskSchedulerImpl$$resourceOfferSingleTaskSet$1.apply$mcVI$sp(TaskSchedulerImpl.scala:257)&#xA;        at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)&#xA;        at org.apache.spark.scheduler.TaskSchedulerImpl.org$apache$spark$scheduler$TaskSchedulerImpl$$resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:252)&#xA;        at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3$$anonfun$apply$8.apply(TaskSchedulerImpl.scala:322)&#xA;        at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3$$anonfun$apply$8.apply(TaskSchedulerImpl.scala:320)&#xA;        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)&#xA;        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)&#xA;        at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3.apply(TaskSchedulerImpl.scala:320)&#xA;        at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$3.apply(TaskSchedulerImpl.scala:320)&#xA;        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&#xA;        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)&#xA;        at org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:320)&#xA;        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$makeOffers(CoarseGrainedSchedulerBackend.scala:220)&#xA;        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receive$1.applyOrElse(CoarseGrainedSchedulerBackend.scala:138)&#xA;        at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)&#xA;        at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)&#xA;        at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)&#xA;        at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:213)&#xA;        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;i am just running command  pyspark --master yarn --deploy-mode client&lt;/p&gt;&#xA;" OwnerUserId="8025374" LastEditorUserId="8025374" LastEditDate="2018-03-06T07:16:07.027" LastActivityDate="2018-03-06T07:16:07.027" Title="pyspark and python 3.5 Error File &quot;/opt/python3/lib/python3.5/site.py&quot;, line 176" Tags="&lt;python-3.x&gt;&lt;python-2.7&gt;&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49124203" PostTypeId="1" AcceptedAnswerId="49126131" CreationDate="2018-03-06T05:52:30.680" Score="0" ViewCount="26" Body="&lt;p&gt;I have a task wherein I am receiving a list of files(size of each file is very small) which should be used for processing.&#xA;There are millions of such files stored in my AWS S3 bucket, I need to filter and process only those files which are present in the above list.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone please let me know the best practice to do this in Spark?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Eg. There are millions of files present in AWS S3 bucket of XYZ university. Each file having a unique ID as a filename. I get the list of 1000 unique ID's to be processed. Now i have to do the processing only on these files to aggregate and generate an output csv file.&lt;/p&gt;&#xA;" OwnerUserId="2899524" LastActivityDate="2018-03-06T08:22:24.597" Title="Apache spark filtering files for processing in AWS S3" Tags="&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;awss3&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49124266" PostTypeId="1" CreationDate="2018-03-06T05:57:31.417" Score="0" ViewCount="26" Body="&lt;p&gt;I am Inserting JavaRDD into elastic search and I want to remove one value from it. I can't apply filter function as I need that value as a record id in elastic search&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;{&quot;elastic_search_id&quot;:&quot;11&quot;,&quot;persona_email&quot;:&quot;abc@gmail.com&quot;}&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;the code i am using is &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Map confParent = new HashMap();&#xA;&#xA;**// setting elasticsearch id**&#xA;confParent.put(&quot;es.mapping.id&quot; , &quot;elastic_search_id&quot;);&#xA;&#xA;**// trying to remove key but not working**&#xA;confParent.put(&quot;es.mapping.exclude&quot;, &quot;elastic_search_id&quot;);&#xA;&#xA;confParent.put(&quot;es.resource&quot; , index + &quot;/&quot; + type);&#xA;&#xA;JavaEsSpark.saveJsonToEs(parentRecordsData,  confParent);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="7239716" LastActivityDate="2018-03-06T05:57:31.417" Title="How to exclude value from JavaRDD&lt;String&gt; before inserting into elasticsearch" Tags="&lt;apache-spark&gt;&lt;elasticsearch&gt;&lt;spark-streaming&gt;&lt;rdd&gt;" AnswerCount="0" CommentCount="5" />
  <row Id="49124654" PostTypeId="1" CreationDate="2018-03-06T06:29:10.253" Score="0" ViewCount="9" Body="&lt;p&gt;I am trying to run a Linear regression in spark and want to avail of pValues and tValues on my model for feature significance for my model. There is a note in the docs that limits that my feature set has to be &amp;lt; 4096&#xA;My question are two fold:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;I am working at a categorical value that i turned to one-hot encoded values that resulted to 29996 feature and that is not within the 4096 feature limitation. Is there any other metric that I can use for feature significance for a LR model?&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Secondly, has anybody encountered this situation and what have you done to resolve the issue. An approach&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Also I was also looking for a package in pyspark that can assist for doing random coefficient effect for LR aside from the GLM available out there? Does it support random coefficient and hierarchical form?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="990608" LastActivityDate="2018-03-06T06:29:10.253" Title="pyspark.ml.regression - how to work with &gt; 4096 features" Tags="&lt;pyspark&gt;&lt;linear-regression&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49125450" PostTypeId="1" CreationDate="2018-03-06T07:25:55.783" Score="0" ViewCount="16" Body="&lt;p&gt;This question is regarding the nature of design of MLlib pipeline components (transformers and estimators). Spark documentation &#xA; &lt;a href=&quot;https://spark.apache.org/docs/2.1.0/ml-pipeline.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://spark.apache.org/docs/2.1.0/ml-pipeline.html&lt;/a&gt; says that Transformer.transform and Estimator.fit are stateless in nature. And there is a possibility that in future they may implement stateful algorithms.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am not able to understand the depth of this line. Here is my limited understanding:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Every time a transform or fit call is made, no information is recorded by the class (transformer or estimator). This means that previous method calls have no impact on the current call. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;If this is correct, then i am not able to think in what scenarios this might be useful to keep state of these calls. In fact what kind of state can be kept in these algorithms?  (predictions, features or what ?)&lt;/p&gt;&#xA;" OwnerUserId="1567684" LastEditorUserId="1567684" LastEditDate="2018-03-06T07:36:20.590" LastActivityDate="2018-03-06T07:36:20.590" Title="How MLlib transform and fit methods are stateless in nature&quot;?" Tags="&lt;apache-spark&gt;&lt;machine-learning&gt;&lt;apache-spark-mllib&gt;&lt;stateless&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49125735" PostTypeId="1" AcceptedAnswerId="49127041" CreationDate="2018-03-06T07:45:16.010" Score="0" ViewCount="61" Body="&lt;p&gt;I have a &lt;code&gt;DataFrame&lt;/code&gt; like the following.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+---+-------------+-----+&#xA;| id|AccountNumber|scale|&#xA;+---+-------------+-----+&#xA;|  1|      1500847|    6|&#xA;|  2|      1501199|    7|&#xA;|  3|      1119024|    3|&#xA;+---+-------------+-----+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have to populate a second &lt;code&gt;DataFrame&lt;/code&gt;, which would initially be empty, as follows.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;id  AccountNumber   scale&#xA;1   1500847         6&#xA;2   1501199         6&#xA;3   1119024         3&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h3&gt;Output explaination&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;First row in the first &lt;code&gt;DataFrame&lt;/code&gt; has a &lt;code&gt;scale&lt;/code&gt; of 6. Check for that value minus 1 (so &lt;code&gt;scale&lt;/code&gt; equals 5) in the result. There none, so simply add the row &lt;code&gt;(1,1500847,6)&lt;/code&gt; to the output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The second row in the output has a &lt;code&gt;scale&lt;/code&gt; of 7. The original table already has a row with &lt;code&gt;scale&lt;/code&gt; 7 - 1, so add this row but with that scale &lt;code&gt;(2, 15001199, 6)&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The third row works as the first one.&lt;/p&gt;&#xA;" OwnerUserId="3082677" LastEditorUserId="3314107" LastEditDate="2018-03-06T08:17:17.320" LastActivityDate="2018-03-06T09:45:47.283" Title="Loop through dataframe and update the lookup table simultaneously: spark scala" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="0" />
  <row Id="49126700" PostTypeId="1" AcceptedAnswerId="49129445" CreationDate="2018-03-06T08:50:10.577" Score="1" ViewCount="25" Body="&lt;p&gt;&lt;strong&gt;The Idea.&lt;/strong&gt;&#xA;I would like to build a function like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;location_affinity(user_a, user_b)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;which establish a location affinity between two users. In particular, this function will return a float number between 0 (no affinity) and 1 (max affinity) indicating how much places user_a has been correspond to places user_b has been. e.g.: If user_a ALWAYS stays with user_b and follows him to every places he go, I'm expecting a &quot;1&quot; as result. If user_a lives far away from user_b and they never got even close to each other, I'm expecting a &quot;0&quot; as result.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;The Data.&lt;/strong&gt;&#xA;Each user has a list of points(latitude, longitude) where he has been, and those points were already extracted from user's Facebook geotags.&#xA;To visualize this: &lt;a href=&quot;https://i.stack.imgur.com/qYUrF.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;IMAGE&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Red &quot;X&quot;s are points(lat, lng) user_a has been.&lt;/li&gt;&#xA;&lt;li&gt;Green &quot;X&quot;s are points(lat, lng) user_b has been.&lt;/li&gt;&#xA;&lt;li&gt;Blue area represent the overlap.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;The Question.&lt;/strong&gt;&#xA;Are there any known algorithms which, based on two users' map points list, can establish the affinity (which I gather it depends on the overlap area)?&#xA;If not, which keywords should I search for?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Additional.&lt;/strong&gt;&#xA;I'm trying to build Python functions with Spark. Are there any integrations?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you.&lt;/p&gt;&#xA;" OwnerUserId="9135052" LastActivityDate="2018-03-06T15:56:43.063" Title="Establish location-affinity between two users from their geotags?" Tags="&lt;python&gt;&lt;dictionary&gt;&lt;apache-spark&gt;&lt;tags&gt;&lt;geotagging&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49127719" PostTypeId="1" CreationDate="2018-03-06T09:43:56.487" Score="-2" ViewCount="21" Body="&lt;p&gt;I am trying to create hive table with Spark.&#xA;I am getting the below error -&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; +- TungstenAggregate(key=[rpt_prd#244,country_code#240,product_code#242], functions=[(count(1),mode=Partial,isDistinct=false)], output=[rpt_prd#244,co&#xA;untry_code#240,product_code#242,count#832L])&#xA;                     +- HiveTableScan [rpt_prd#244,country_code#240,product_code#242], MetastoreRelation gfrrtnsd_standardization, pln_arrg_dim, None, [(country_code#24&#xA;0 = HK)]&#xA;&#xA;org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:&#xA;TungstenExchange hashpartitioning(rpt_prd#236,200), None&#xA;+- Sort [rpt_prd#236 ASC], true, 0&#xA;   +- ConvertToUnsafe&#xA;      +- Exchange rangepartitioning(rpt_prd#236 ASC,200), None&#xA;         +- ConvertToSafe&#xA;            +- TungstenAggregate(key=[rpt_prd#244,country_code#240,product_code#242], functions=[(count(1),mode=Final,isDistinct=false)], output=[rpt_prd#236,Dim_countr&#xA;y_code#237,Dim_product_code#238,Dim_recordCount#239L])&#xA;               +- TungstenExchange hashpartitioning(rpt_prd#244,country_code#240,product_code#242,200), None&#xA;                  +- TungstenAggregate(key=[rpt_prd#244,country_code#240,product_code#242], functions=[(count(1),mode=Partial,isDistinct=false)], output=[rpt_prd#244,co&#xA;untry_code#240,product_c`enter code here`ode#242,count#832L])&#xA;                     +- HiveTableScan [rpt_prd#244,country_code#240,product_code#242], MetastoreRelation gfrrtnsd_standardization, pln_arrg_dim, None, [(country_code#24&#xA;0 = HK)]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Please assist to create a hive table from spark.&lt;/p&gt;&#xA;" OwnerUserId="3668103" LastActivityDate="2018-03-06T12:51:47.533" Title="Unable to create Hive table from spark" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;hiveql&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49127899" PostTypeId="1" CreationDate="2018-03-06T09:52:31.320" Score="0" ViewCount="10" Body="&lt;p&gt;Just wondering , it is possible that enabling Spark History Server impacts Spark Job performance ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&#xA;Shashi&lt;/p&gt;&#xA;" OwnerUserId="4456534" LastActivityDate="2018-03-06T09:52:31.320" Title="Spark History Server | Impact on Spark Job Performance" Tags="&lt;apache-spark&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49128109" PostTypeId="1" CreationDate="2018-03-06T10:02:27.057" Score="0" ViewCount="22" Body="&lt;p&gt;&lt;br&gt;&#xA;I am working with Python Spark (v.2.2.0) and i am probably missing something.&lt;br&gt;&#xA;I want to validate the input against the schema, and i read about the 'FAILFAST' option mode.&lt;br&gt;&#xA;I would expect an exception where actually i am receiving data with nulls (which is what 'PERMISSIVE' mode should do) &lt;br&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Am i missing something?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;sample code&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark.sql import SparkSession&#xA;&#xA;spark = SparkSession.builder \&#xA;    .appName(&quot;test&quot;) \&#xA;    .getOrCreate()&#xA;&#xA;schema = StructType([&#xA;    StructField('id', IntegerType(), False),&#xA;    StructField('name', StringType(), False),&#xA;    StructField('value', IntegerType(), False)&#xA;])&#xA;&#xA;df = spark.read.format('json') \&#xA;.schema(schema).options(mode='FAILFAST') \&#xA;.load(&quot;sample.json&quot;)&#xA;&#xA;df.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Unexpected result&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&#xA;+----+----+-----+&lt;br&gt;&#xA;|  id|name|value|&lt;br&gt;&#xA;+----+----+-----+&lt;br&gt;&#xA;|null|null| null|&lt;br&gt;&#xA;+----+----+-----+&lt;br&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Expected&lt;/strong&gt;&lt;br&gt;&#xA;Exception&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;sample.json&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&#xA;     {'blabla':'blablabla'}&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; &lt;br&gt;&lt;br&gt;&#xA;If i am trying to load csv instead of json it's working as expected:&lt;br&gt;&#xA;    df = spark.read.format('csv') \ &lt;br&gt;&#xA;    .schema(schema).options(mode='FAILFAST') \&lt;br&gt;&#xA;    .load(&quot;sample.csv&quot;) &lt;br&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In this case it's throwing an exception as expected&lt;/p&gt;&#xA;" OwnerUserId="5841978" LastEditorUserId="5841978" LastEditDate="2018-03-06T12:20:16.827" LastActivityDate="2018-03-06T12:20:16.827" Title="Python Spark - Load json - mode FAILFAST is not working as expected" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49128213" PostTypeId="1" CreationDate="2018-03-06T10:07:36.060" Score="-1" ViewCount="62" Body="&lt;p&gt;After performing few transformations in Spark-shell, I got the output as below. I am copy and pasting from REPL. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;scala&gt; s.collect&#xA;res44: Array[(String, String)] = Array((45000,Pavan,Amit), (50000,Bhupesh,Tejas,Dinesh)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now i need to generate individual files for each ID, with ID included in File Name as below&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;The file with name ID45000.txt should have below content&#xA;45000,Pavan&#xA;45000,Amit&#xA;&#xA;The file with name ID50000.txt should have below content.&#xA;50000,Bhupesh&#xA;50000,Tejas&#xA;50000,Dinesh&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Tried below code but not working&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;s.foreach{case(k,v) =&amp;gt; flatMapValues(x=&amp;gt;x.split(&quot;,&quot;)).saveAsTextFile(&quot;ID&quot;+k+&quot;.txt&quot;)}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Could experts kindly help me.&lt;/p&gt;&#xA;" OwnerUserId="9450440" LastEditorUserId="9450440" LastEditDate="2018-03-07T12:08:57.290" LastActivityDate="2018-03-07T16:12:00.247" Title="Spark scala flatmapvalues" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="3" FavoriteCount="0" />
  <row Id="49128580" PostTypeId="1" AcceptedAnswerId="49128712" CreationDate="2018-03-06T10:27:18.573" Score="0" ViewCount="20" Body="&lt;p&gt;Here is my schema &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;root&#xA; |-- DataPartition: string (nullable = true)&#xA; |-- TimeStamp: string (nullable = true)&#xA; |-- PeriodId: long (nullable = true)&#xA; |-- FinancialAsReportedLineItemName: struct (nullable = true)&#xA; |    |-- _VALUE: string (nullable = true)&#xA; |    |-- _languageId: long (nullable = true)&#xA; |-- FinancialLineItemSource: long (nullable = true)&#xA; |-- FinancialStatementLineItemSequence: long (nullable = true)&#xA; |-- FinancialStatementLineItemValue: double (nullable = true)&#xA; |-- FiscalYear: long (nullable = true)&#xA; |-- IsAnnual: boolean (nullable = true)&#xA; |-- IsAsReportedCurrencySetManually: boolean (nullable = true)&#xA; |-- IsCombinedItem: boolean (nullable = true)&#xA; |-- IsDerived: boolean (nullable = true)&#xA; |-- IsExcludedFromStandardization: boolean (nullable = true)&#xA; |-- IsFinal: boolean (nullable = true)&#xA; |-- IsTotal: boolean (nullable = true)&#xA; |-- ParentLineItemId: long (nullable = true)&#xA; |-- PeriodPermId: struct (nullable = true)&#xA; |    |-- _VALUE: long (nullable = true)&#xA; |    |-- _objectTypeId: long (nullable = true)&#xA; |-- ReportedCurrencyId: long (nullable = true)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;From the above schema i am trying to do like this&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val temp = tempNew1&#xA;      .withColumn(&quot;FinancialAsReportedLineItemName&quot;, $&quot;FinancialAsReportedLineItemName._VALUE&quot;)&#xA;      .withColumn(&quot;FinancialAsReportedLineItemName_languageId&quot;, $&quot;FinancialAsReportedLineItemName._languageId&quot;)&#xA;      .withColumn(&quot;PeriodPermId&quot;, $&quot;PeriodPermId._VALUE&quot;)&#xA;      .withColumn(&quot;PeriodPermId_objectTypeId&quot;, $&quot;PeriodPermId._objectTypeId&quot;).drop($&quot;AsReportedItem&quot;).drop($&quot;AsReportedItem&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I don't know what i am missing here .&#xA;I get below error &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Exception in thread &quot;main&quot; org.apache.spark.sql.AnalysisException:&#xA;  Can't extract value from FinancialAsReportedLineItemName#2262: need&#xA;  struct type but got string;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="9175539" LastActivityDate="2018-03-06T10:38:23.663" Title="Getting error like need struct type but got string in spark scala for simple struct type" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="49128770" PostTypeId="1" CreationDate="2018-03-06T10:37:27.357" Score="0" ViewCount="47" Body="&lt;p&gt;I have one csv file which size is 8.2G, I only do read and write with one worker of 1 core/2G RAM. Spark version is 2.2.1.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val data = spark.read.format(&quot;com.databricks.spark.csv&quot;).option(&quot;header&quot;, &quot;true&quot;)&#xA;  .option(&quot;inferSchema&quot;, &quot;false&quot;).option(&quot;delimiter&quot;, &quot;;&quot;).load(&quot;file:///home/ming/scala_performance/usb/sirene_L_M.csv&quot;)&#xA;&#xA;data.write.mode(&quot;overwrite&quot;).format(&quot;csv&quot;)&#xA;  .option(&quot;header&quot;, &quot;true&quot;).option(&quot;delimiter&quot;, &quot;;&quot;)&#xA;.save(&quot;file:///home/ming/scala_performance/sirene_L_M.csv&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This code generates a list of files which size is 96MB~99MB.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;etblFull.rdd.getNumPartitions = 66&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My first question is why the partition is 66 and the size of file is ~96, instead of 64MB or 128MB?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;After this, I configured Hadoop and yarn and put file to hdfs for reading. &lt;strong&gt;In my opinion&lt;/strong&gt;, if I have 3 worker node and each one treat one partition each time, then when workers write to hdfs, they will write their work to the nearest location, so I will have something like this:    &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;worker001                   worker002                worker003&#xA;file_part_00001             file_part_00002          file_part_00003&#xA;file_part_00004             file_part_00005          file_part_00006&#xA;file_part_00007             file_part_00008          file_part_00009&#xA;file_part_000010            file_part_000011          file_part_000012  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I will give each worker 4G RAM and configure 3 workers for this job. I will complete my question with the result once it is done.&lt;/p&gt;&#xA;" OwnerUserId="4825559" LastActivityDate="2018-03-06T10:37:27.357" Title="Does spark worker writes a part of the result to nearest hdfs?" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;hdfs&gt;&lt;yarn&gt;" AnswerCount="0" CommentCount="0" FavoriteCount="0" />
  <row Id="49128863" PostTypeId="1" CreationDate="2018-03-06T10:42:24.070" Score="0" ViewCount="26" Body="&lt;p&gt;I have a following situation. I have large Cassandra table (with large number of columns) which i would like process with Spark.  I want only selected columns to be loaded in to Spark ( Apply select and filtering on Cassandra server itself)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; val eptable = &#xA; sc.cassandraTable(&quot;test&quot;,&quot;devices&quot;).select(&quot;device_ccompany&quot;,&quot;device_model&quot;,&quot;devi&#xA; ce_type&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Above statement gives a CassandraTableScanRDD but how do i convert this in to DataSet/DataFrame ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Si there any other way i can do server side filtering of columns and get dataframes?&lt;/p&gt;&#xA;" OwnerUserId="4003880" LastEditorUserId="5880706" LastEditDate="2018-03-06T10:43:34.027" LastActivityDate="2018-03-06T10:56:09.207" Title="Converting CassandraTableScanRDD org.apache.spark.rdd.RDD" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;dataframe&gt;&lt;cassandra&gt;&lt;rdd&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49128879" PostTypeId="1" CreationDate="2018-03-06T10:43:27.423" Score="-1" ViewCount="24" Body="&lt;p&gt;I am trying to &lt;strong&gt;delete&lt;/strong&gt; a row from hive table in spark by using &lt;strong&gt;HiveContext&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my sample code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;hiveContext.sql(&quot;delete from default.college1 where clg_id=7&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;when i do the same thing in case of &lt;strong&gt;insert&lt;/strong&gt; its working fine. Only &lt;strong&gt;update&lt;/strong&gt; , &lt;strong&gt;delete&lt;/strong&gt; not working. i have added all the &lt;strong&gt;ACID properties&lt;/strong&gt; to &lt;strong&gt;hive-site.xml&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;when i run code i got the following exception:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Exception in thread &quot;main&quot; org.apache.spark.sql.catalyst.parser.ParseException: &#xA;Operation not allowed: delete from(line 1, pos 0)&#xA;&#xA;== SQL ==&#xA;delete from default.college1 where clg_id=7;&#xA;^^^&#xA;&#xA;    at org.apache.spark.sql.catalyst.parser.ParserUtils$.operationNotAllowed(ParserUtils.scala:43)&#xA;    at org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitFailNativeCommand$1.apply(SparkSqlParser.scala:874)&#xA;    at org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitFailNativeCommand$1.apply(SparkSqlParser.scala:865)&#xA;    at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:96)&#xA;    at org.apache.spark.sql.execution.SparkSqlAstBuilder.visitFailNativeCommand(SparkSqlParser.scala:865)&#xA;    at org.apache.spark.sql.execution.SparkSqlAstBuilder.visitFailNativeCommand(SparkSqlParser.scala:52)&#xA;    at org.apache.spark.sql.catalyst.parser.SqlBaseParser$FailNativeCommandContext.accept(SqlBaseParser.java:948)&#xA;    at org.antlr.v4.runtime.tree.AbstractParseTreeVisitor.visit(AbstractParseTreeVisitor.java:42)&#xA;    at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleStatement$1.apply(AstBuilder.scala:65)&#xA;    at org.apache.spark.sql.catalyst.parser.AstBuilder$$anonfun$visitSingleStatement$1.apply(AstBuilder.scala:65)&#xA;    at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:96)&#xA;    at org.apache.spark.sql.catalyst.parser.AstBuilder.visitSingleStatement(AstBuilder.scala:64)&#xA;    at org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply(ParseDriver.scala:54)&#xA;    at org.apache.spark.sql.catalyst.parser.AbstractSqlParser$$anonfun$parsePlan$1.apply(ParseDriver.scala:53)&#xA;    at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:82)&#xA;    at org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:45)&#xA;    at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(ParseDriver.scala:53)&#xA;    at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)&#xA;    at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:682)&#xA;    at newipg170103.update$.main(update.scala:49)&#xA;    at newipg170103.update.main(update.scala)&#xA;    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#xA;    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#xA;    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#xA;    at java.lang.reflect.Method.invoke(Method.java:498)&#xA;    at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736)&#xA;    at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)&#xA;    at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)&#xA;    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)&#xA;    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Please help me out.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in Advance.&lt;/p&gt;&#xA;" OwnerUserId="6325753" LastActivityDate="2018-03-06T10:43:27.423" Title="DELETE command not working in Spark by using HiveContext" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;hive&gt;&lt;sql-delete&gt;&lt;acid&gt;" AnswerCount="0" CommentCount="0" FavoriteCount="0" />
  <row Id="49129507" PostTypeId="1" AcceptedAnswerId="49136078" CreationDate="2018-03-06T11:15:15.407" Score="0" ViewCount="34" Body="&lt;p&gt;How to set following Cassandra write parameters in spark scala code for &#xA;version - DataStax Spark Cassandra Connector &lt;strong&gt;1.6.3&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Spark version - 1.6.2 &lt;/p&gt;&#xA;&#xA;&lt;p&gt;spark.cassandra.output.batch.size.rows&lt;/p&gt;&#xA;&#xA;&lt;p&gt;spark.cassandra.output.concurrent.writes&lt;/p&gt;&#xA;&#xA;&lt;p&gt;spark.cassandra.output.batch.size.bytes&lt;/p&gt;&#xA;&#xA;&lt;p&gt;spark.cassandra.output.batch.grouping.key&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks,&#xA;Chandra&lt;/p&gt;&#xA;" OwnerUserId="5433741" LastActivityDate="2018-03-07T23:43:56.853" Title="spark Cassandra tuning" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;cassandra&gt;&lt;datastax&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49129756" PostTypeId="1" CreationDate="2018-03-06T11:27:12.690" Score="0" ViewCount="12" Body="&lt;p&gt;I have a dataFrame&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Dataset&amp;lt;Row&amp;gt; dataset = getSparkInstance().createDataFrame(newRDD, struct);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;dataset.schema()&lt;/code&gt; is returning me a StructType.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But I want the actual schema to store in &lt;code&gt;sample.avsc&lt;/code&gt; file &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically I want to convert StructType to Avro Schema file (.avsc).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;any Idea?&lt;/p&gt;&#xA;" OwnerUserId="7770010" LastActivityDate="2018-03-07T12:42:42.707" Title="How to get the avro schema from StructType" Tags="&lt;spark-avro&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49130372" PostTypeId="1" CreationDate="2018-03-06T12:02:45.913" Score="0" ViewCount="15" Body="&lt;p&gt;I have already asked this question months earlier but I will ask again to be 100% sure and I will describe my issue here :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a streaming topic that I'm aggregating every minute with a sliding window in Spark Structured Streaming.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, with a window of 30 minutes sliding every minute, we can say that I'm already doing some kind of moving statistics. I'm not only computing on every minute independantly but taking into account the last 29 minutes too, and from here moving my aggregating window every minute.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But is there a way to compute in real-time with Spark Structured Streaming :&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;Change over time : &lt;img src=&quot;https://chart.googleapis.com/chart?cht=tx&amp;amp;chl=C_%7Bm%2B1%7D%20%3D%20%5Cfrac%7B(X_%7Bm%2B1%7D%20-%20X_m)%7D%7Bt_%7Bm%2B1%7D%20-%20t_m%7D&quot; alt=&quot;change&quot;&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Rate of change : &lt;img src=&quot;https://chart.googleapis.com/chart?cht=tx&amp;amp;chl=RT_m%20%3D%20%5Cfrac%7B(C_%7Bm%2B1%7D%20-%20C_m)%7D%7Bt_%7Bm%2B1%7D%20-%20t_m%7D&quot; alt=&quot;rate&quot;&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Growth / Decay : &lt;img src=&quot;https://chart.googleapis.com/chart?cht=tx&amp;amp;chl=G_%7Bm%2B1%7D%20%3D%20%5Cfrac%7B(X_%7Bm%2B1%7D%20-%20X_m)%7D%7BX_%7Bm%2B1%7D%7D&quot; alt=&quot;growth&quot;&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;For example every minute, I compute an average which would be &lt;img src=&quot;https://chart.googleapis.com/chart?cht=tx&amp;amp;chl=X_%7Bm&quot; alt=&quot;rate&quot;&gt;. With these formulas, I would be able to compare the evolution of my average every minute in my sliding window : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+------------+-----------+&#xA;|   Window   |  Average  |&#xA;+------------+-----------+&#xA;|12:30-13:00 |     100   |&#xA;|12:31-13:01 |     103   |&#xA;|12:32-13:02 |     106   |&#xA;|12:33-13:03 |     111   |&#xA;+------------+-----------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For this example, I would want to add a new feature which would be the growth/decay of my average but in real time. At instant t, I want to be able to compute the growth of my average compared to my average at instant t-1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks for your insights for my issue,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Have a good day.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS : I found these two posts which could be interesting, maybe people in this community have a way to do this now :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/48530830/pyspark-streaming-window-and-transform&quot;&gt;PySpark streaming: window and transform&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/42747236/how-to-define-udaf-over-event-time-windows-in-pyspark-2-1-0&quot;&gt;How to define UDAF over event-time windows in PySpark 2.1.0&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="6434448" LastActivityDate="2018-03-06T12:02:45.913" Title="Advanced Aggregations in Structured Streaming" Tags="&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;spark-streaming&gt;&lt;structured-streaming&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49130381" PostTypeId="1" AcceptedAnswerId="49130634" CreationDate="2018-03-06T12:03:18.563" Score="1" ViewCount="8" Body="&lt;p&gt;customer - c_id, c_name, c_address&#xA;product - p_id, p_name, price&#xA;supplier - s_id, s_name, s_address&#xA;orders - o_id, c_id, p_id, quantity, time&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SELECT o.o_id,&#xA;       c.c_id,&#xA;       c.c_name,&#xA;       p.p_id,&#xA;       p.p_name,&#xA;       p.price * o.quantity AS amount&#xA;FROM customer c&#xA;JOIN orders o ON o.c_id = c.c_id&#xA;JOIN product p ON p.p_id = o.p_id;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to execute above query without fetching 3 tables as individual data frames in pyspark and performing joins on dataframes. &lt;/p&gt;&#xA;" OwnerUserId="3598769" LastActivityDate="2018-03-06T12:29:33.863" Title="instead of fetching multiple tables using pyspark how can we execute join query using jdbc" Tags="&lt;pyspark&gt;&lt;pyspark-sql&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49131133" PostTypeId="1" CreationDate="2018-03-06T12:42:29.340" Score="-2" ViewCount="31" Body="&lt;p&gt;I am working on a pyspark project which has to find the users with top followers in twitter globally. Can anyone help me in achieving this? I am new to pyspark. I am unable to find any tweepy APIs&lt;/p&gt;&#xA;" OwnerUserId="3748067" LastActivityDate="2018-03-06T12:42:29.340" Title="Find top 20 users who have highest followers globally in twitter using python" Tags="&lt;python-2.7&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;spark-streaming&gt;&lt;tweepy&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49131147" PostTypeId="1" CreationDate="2018-03-06T12:43:39.847" Score="0" ViewCount="26" Body="&lt;p&gt;I have a table in hive&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;db.table_name&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I run the following in hive I get results back&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SELECT * FROM db.table_name;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I run the following in a spark-shell &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark.read.table(&quot;db.table_name&quot;).show&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It shows nothing. Similarly &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sql(&quot;SELECT * FROM db.table_name&quot;).show&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Also shows nothing. Selecting arbitrary columns out before the show also displays nothing. Performing a count states the table has 0 rows.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Running the same queries works against other tables in the same database.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Spark Version: 2.2.0.cloudera1&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The table is created using&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;table.write.mode(SaveMode.Overwrite).saveAsTable(&quot;db.table_name&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And if I read the file using the parquet files directly it works.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark.read.parquet(&amp;lt;path-to-files&amp;gt;).show&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="892041" LastEditorUserId="892041" LastEditDate="2018-03-08T17:31:48.093" LastActivityDate="2018-03-08T17:31:48.093" Title="Spark returns Empty DataFrame but Populated in Hive" Tags="&lt;apache-spark&gt;&lt;hive&gt;&lt;apache-spark-sql&gt;" AnswerCount="0" CommentCount="8" />
  <row Id="49131729" PostTypeId="1" AcceptedAnswerId="49132170" CreationDate="2018-03-06T13:13:48.920" Score="2" ViewCount="33" Body="&lt;p&gt;I need to achieve the following, and am having difficulty coming up with an approach to accomplish it due to my inexperience with Spark:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Read data from .json.gz files stored in S3.&lt;/strong&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Each file includes a partial day of Google Analytics data with the schema as specified in &lt;a href=&quot;https://support.google.com/analytics/answer/3437719?hl=en&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://support.google.com/analytics/answer/3437719?hl=en&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;File names are in the pattern ga_sessions_20170101_Part000000000000_TX.json.gz where 20170101 is a YYYYMMDD date specification and 000000000000 is an incremental counter when there are multiple files for a single day (which is usually the case).&lt;/li&gt;&#xA;&lt;li&gt;An entire day of data is therefore composed of multiple files with incremental &quot;part numbers&quot;.&lt;/li&gt;&#xA;&lt;li&gt;There are generally 3 to 5 files per day.&lt;/li&gt;&#xA;&lt;li&gt;All fields in the JSON files are stored with qoute (&quot;) delimiters, regardless of the data type specified in the aforementioned schema documentation. The data frame which results from reading the files (via sqlContext.read.json) therefore has every field typed as string, even though some are actually integer, boolean, or other data types.&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Convert the all-string data frame to a properly typed data frame according to the schema specification.&lt;/strong&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;My goal is to have the data frame typed properly so that when it is saved in Parquet format the data types are correct.&lt;/li&gt;&#xA;&lt;li&gt;Not all fields in the schema specification are present in every input file, or even every day's worth of input files (the schema may have changed over time). The conversion will therefore need to be dynamic, converting the data types of only the fields actually present in the data frame.&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Write the data in the properly typed data frame back to S3 in Parquet format.&lt;/strong&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The data should be partitioned by day, with each partition stored in a separate folder named &quot;partition_date=YYYYMMDD&quot; where &quot;YYYYMMDD&quot; is the actual date associated with the data (from the original input file names).&lt;/li&gt;&#xA;&lt;li&gt;I don't think the number of files per day matters. The goal is simply to have partitioned Parquet format data that I can point Spectrum at.&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I have been able to read and write the data successfully, but have been unsuccessful with several aspects of the overall task:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;I don't know how to approach the problem to ensure that I'm effectively utilizing the AWS EMR cluster to its full potential for parallel/distributed processing, either in reading, converting, or writing the data. I would like to size up the cluster as needed to accomplish the task within whatever time frame I choose (within reason).&lt;/li&gt;&#xA;&lt;li&gt;I don't know how to best accomplish the data type conversion. Not knowing which fields will or will not be present in any particular batch of input files requires dynamic code to retype the data frame. I also want to make sure this task is distributed effectively and isn't done inefficiently (I'm concerned about creating a new data frame as each field is retyped).&lt;/li&gt;&#xA;&lt;li&gt;I don't understand how to manage partitioning of the data appropriately.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Any help working through an overall approach would be greatly appreciated!&lt;/p&gt;&#xA;" OwnerUserId="6362093" LastActivityDate="2018-03-06T14:06:47.077" Title="Converting JSON to Parquet in Amazon EMR" Tags="&lt;apache-spark&gt;&lt;emr&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49132151" PostTypeId="1" AcceptedAnswerId="49173120" CreationDate="2018-03-06T13:36:19.093" Score="0" ViewCount="25" Body="&lt;p&gt;I am trying to distribute Keras training on a cluster and use Elephas for that. But, when running the basic example from the doc of Elephas (&lt;a href=&quot;https://github.com/maxpumperla/elephas&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/maxpumperla/elephas&lt;/a&gt;):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from elephas.utils.rdd_utils import to_simple_rdd&#xA;rdd = to_simple_rdd(sc, x_train, y_train)&#xA;from elephas.spark_model import SparkModel&#xA;from elephas import optimizers as elephas_optimizers&#xA;sgd = elephas_optimizers.SGD()&#xA;spark_model = SparkModel(sc, model, optimizer=sgd, frequency='epoch', mode='asynchronous', num_workers=2)&#xA;spark_model.train(rdd, nb_epoch=epochs, batch_size=batch_size, verbose=1, validation_split=0.1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I get the following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; ImportError: No module named elephas.spark_model&#xA;&#xA;&#xA;&#xA;```Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.&#xA;: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 4 times, most recent failure: Lost task 1.3 in stage 5.0 (TID 58, xxxx, executor 8): org.apache.spark.api.python.PythonException: Traceback (most recent call last):&#xA;  File &quot;/xx/xx/hadoop/yarn/local/usercache/xx/appcache/application_151xxx857247_19188/container_1512xxx247_19188_01_000009/pyspark.zip/pyspark/worker.py&quot;, line 163, in main&#xA;    func, profiler, deserializer, serializer = read_command(pickleSer, infile)&#xA;  File &quot;/xx/xx/hadoop/yarn/local/usercache/xx/appcache/application_151xxx857247_19188/container_1512xxx247_19188_01_000009/pyspark.zip/pyspark/worker.py&quot;, line 54, in read_command&#xA;    command = serializer._read_with_length(file)&#xA;  File /yarn/local/usercache/xx/appcache/application_151xxx857247_19188/container_1512xxx247_19188_01_000009/pyspark.zip/pyspark/serializers.py&quot;, line 169, in _read_with_length&#xA;    return self.loads(obj)&#xA;  File &quot;/yarn//local/usercache/xx/appcache/application_151xxx857247_19188/container_1512xxx247_19188_01_000009/pyspark.zip/pyspark/serializers.py&quot;, line 454, in loads&#xA;    return pickle.loads(obj)&#xA;ImportError: No module named elephas.spark_model&#xA;&#xA;    at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)&#xA;    at org.apache.spark.api.python.PythonRunner$$anon$1.&amp;lt;init&amp;gt;(PythonRDD.scala:234)&#xA;    at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)&#xA;    at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)&#xA;    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)&#xA;    at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)&#xA;    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)&#xA;    at org.apache.spark.scheduler.Task.run(Task.scala:99)&#xA;    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;    at java.lang.Thread.run(Thread.java:748)```&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Also, the model is actually created, I can do &lt;code&gt;print(spark_model)&lt;/code&gt; and will get this &lt;code&gt;&amp;lt;elephas.spark_model.SparkModel object at 0x7efce0abfcd0&amp;gt;&lt;/code&gt;. The error occurs during &lt;code&gt;spark_model.train&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've installed elephas using &lt;code&gt;pip2 install git+https://github.com/maxpumperla/elephas&lt;/code&gt;, maybe this is relevant.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I use PySpark 2.1.1, Keras 2.1.4 and Python 2.7.&#xA;I've tried running it with &lt;em&gt;spark-submit&lt;/em&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;PYSPARK_DRIVER_PYTHON=`which python` spark-submit --driver-memory 1G  filname.py&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And also directly in a Jupyter Notebook. Both result in the same problem.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone give me any pointers? Is this elephas related or is it a PySpark problem?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: I also upload the zip file of the virtual environment and call it within the script:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;virtualenv spark_venv --relocatable&#xA;cd spark_venv &#xA;zip -qr ../spark_venv.zip *&#xA;&#xA;PYSPARK_DRIVER_PYTHON=`which python` spark-submit --driver-memory 1G --py-files spark_venv.zip filename.py&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then in the file I do:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sc.addPyFile(&quot;spark_venv.zip&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;After this keras is imported without any problems, but I still get the &lt;code&gt;elephas&lt;/code&gt; error from above.&lt;/p&gt;&#xA;" OwnerUserId="4984066" LastEditorUserId="4984066" LastEditDate="2018-03-07T09:02:58.657" LastActivityDate="2018-03-08T16:29:16.493" Title="Elephas not loaded in PySpark: No module named elephas.spark_model" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;keras&gt;&lt;distributed-computing&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49132158" PostTypeId="1" CreationDate="2018-03-06T13:36:27.217" Score="0" ViewCount="5" Body="&lt;p&gt;How do I set the below property in spark1.5  to false &lt;/p&gt;&#xA;&#xA;&lt;p&gt;spark.sql.sources.partitionColumnTypeInference.enabled&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it like &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val conf = new SparkConf()&#xA;      .set(&quot;spark.sql.sources.partitionColumnTypeInference.enabled&quot;,&quot;false&quot;)&#xA;val sc = new SparkContext(conf)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;or &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val sqlContext = new HiveContext(sc)&#xA;sqlContext.sql(&quot;spark.sql.sources.partitionColumnTypeInference.enabled&quot;,&quot;false&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="3240790" LastActivityDate="2018-03-06T13:36:27.217" Title="How do I set spark property in 1.5" Tags="&lt;apache-spark-sql&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49132275" PostTypeId="1" AcceptedAnswerId="49132401" CreationDate="2018-03-06T13:43:10.397" Score="-1" ViewCount="28" Body="&lt;p&gt;I am facing an issue while I am trying to pass the join elements as variables in pyspark dataframe join function. I am getting primary key fields from a file while I am trying pass it as variable in a join statement, it throws an error as &quot;cannot resolve the column name&quot; since it is passed as a string. Please assist me on this.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;for i in range(len(pr_list)):&#xA;    if i != len(pr_list)-1:&#xA;        pr_str += &quot; (df_a.&quot; + pr_list[i] + &quot; == df_b.&quot; +pr_list[i] +&quot;) &amp;amp; &quot;&#xA;    else:&#xA;        pr_str += &quot;(df_a.&quot; + pr_list[i]  + &quot; == df_b.&quot; +pr_list[i]  +&quot;)&quot;&#xA;print (pr_str)&#xA;&#xA;df1_with_db2 = df_a.join(df_b, pr_str ,'inner').select('df_a.*')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="8742569" LastActivityDate="2018-03-07T07:44:27.430" Title="Pyspark dataframe join elements as variables" Tags="&lt;string&gt;&lt;python-3.x&gt;&lt;apache-spark&gt;&lt;dataframe&gt;&lt;pyspark&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49133004" PostTypeId="1" CreationDate="2018-03-06T14:19:27.027" Score="0" ViewCount="25" Body="&lt;p&gt;I would like to test my AWS Glue PySpark job with a small subset of the data available. How can this be achieved?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My first try was to convert the Glue dynamic frame to a spark data frame, and use the take(n) method to limit the number of rows to be processed as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;datasource0 = glueContext.create_dynamic_frame.from_catalog(&#xA;    database = &quot;my_db&quot;,&#xA;    table_name = &quot;my_table&quot;,&#xA;    transformation_ctx = &quot;ds0&quot;)&#xA;&#xA;applymapping1 = ApplyMapping.apply(&#xA;    frame = datasource0, &#xA;    mappings = [(&quot;foo&quot;, &quot;string&quot;, &quot;bar&quot;, &quot;string&quot;)],&#xA;    transformation_ctx = &quot;am1&quot;)&#xA;&#xA;truncated_df = applymapping1.toDF().take(1000)&#xA;&#xA;datasink2 = glueContext.write_dynamic_frame.from_options(&#xA;    frame = DynamicFrame.fromDF(truncated_df, glueContext, &quot;tdf&quot;),&#xA;    connection_type = &quot;s3&quot;, &#xA;    ... )&#xA;&#xA;job.commit()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This fails with the following error message: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;AttributeError: 'list' object has no attribute '_jdf'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any ideas?&lt;/p&gt;&#xA;" OwnerUserId="2069922" LastActivityDate="2018-03-06T14:19:27.027" Title="AWS Glue Limit Input Size" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;dataframe&gt;&lt;aws-glue&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49133228" PostTypeId="1" CreationDate="2018-03-06T14:31:46.720" Score="0" ViewCount="27" Body="&lt;p&gt;I'm loading parquet data into a dataframe via &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark.read.parquet('hdfs:///path/goes/here/...')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There are around 50k files in that path due to parquet partitioning.  When I run that command, spark spawns off dozens of small jobs, which is strange because I haven't technically called a spark 'action' yet, so the load command should lazily evaluate.  Here's what the jobs look like in the spark UI:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/u5aIZ.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/u5aIZ.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As you can see, spark rather unhelpfully describes each job as running &lt;code&gt;parquet at NativeMethodAccessorImpl.java&lt;/code&gt;.  These run for a couple minutes in if the data is on HDFS, or over 10 minutes if the data is on S3.  This creates a significant bottleneck in my job that doesn't get smaller if I increase cluster size.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How can I prevent spark from creating those dozens of jobs?&lt;/strong&gt;  Bonus points for a solution that also works in pyspark.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt; I accept that it may be necessary for spark to run thousands of tasks in order to collect statistics on the input files, but how can I at least put all of those tasks into fewer jobs?  Ideally there would just be one big job with ~50k tasks.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;My theory of what's going on&lt;/em&gt;:  I think behind the scenes a parquet loader is scanning all the files to figure out how big they are, to calculate 'split sizes'.  Each task probably represents a parquet file or row group.  Scanning in the name of balanced partitions is useful in some situations, but not in mine, as I'll soon repartition the data anyway and all these scanning jobs take longer than repartitioning. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm running spark 2.2.1 via pyspark on hadoop 2.8.3.&lt;/p&gt;&#xA;" OwnerUserId="189336" LastEditorUserId="189336" LastEditDate="2018-03-08T10:56:27.563" LastActivityDate="2018-03-08T10:56:27.563" Title="Prevent Spark Parquet Loader from Spawning lots of little Jobs" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="49133252" PostTypeId="1" AcceptedAnswerId="49134071" CreationDate="2018-03-06T14:32:37.060" Score="0" ViewCount="40" Body="&lt;p&gt;now has JSON data as follows&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{&quot;Id&quot;:11,&quot;data&quot;:[{&quot;package&quot;:&quot;com.browser1&quot;,&quot;activetime&quot;:60000},{&quot;package&quot;:&quot;com.browser6&quot;,&quot;activetime&quot;:1205000},{&quot;package&quot;:&quot;com.browser7&quot;,&quot;activetime&quot;:1205000}]}&#xA;{&quot;Id&quot;:12,&quot;data&quot;:[{&quot;package&quot;:&quot;com.browser1&quot;,&quot;activetime&quot;:60000},{&quot;package&quot;:&quot;com.browser6&quot;,&quot;activetime&quot;:1205000}]} &#xA;......&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This JSON is the activation time of app, the purpose of which is to analyze the total activation time of each app&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I use sparK SQL to parse JSON&lt;/p&gt;&#xA;&#xA;&lt;p&gt;scala&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val sqlContext = sc.sqlContext&#xA;val behavior = sqlContext.read.json(&quot;behavior-json.log&quot;)&#xA;behavior.cache()&#xA;behavior.createOrReplaceTempView(&quot;behavior&quot;)&#xA;val appActiveTime = sqlContext.sql (&quot;SELECT data FROM behavior&quot;) // SQL query&#xA;appActiveTime.show (100100) // print dataFrame&#xA;appActiveTime.rdd.foreach(println) // print RDD&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But the printed dataFrame is like this&lt;/p&gt;&#xA;&#xA;&lt;p&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+----------------------------------------------------------------------+&#xA;&#xA;| data|&#xA;&#xA;+----------------------------------------------------------------------+&#xA;&#xA;| [[60000, com.browser1], [12870000, com.browser]]|&#xA;&#xA;| [[60000, com.browser1], [120000, com.browser]]|&#xA;&#xA;| [[60000, com.browser1], [120000, com.browser]]|&#xA;&#xA;| [[60000, com.browser1], [1207000, com.browser]]|&#xA;&#xA;| [[120000, com.browser]]|&#xA;&#xA;| [[60000, com.browser1], [1204000, com.browser5]]|&#xA;&#xA;| [[60000, com.browser1], [12075000, com.browser]]|&#xA;&#xA;| [[60000, com.browser1], [120000, com.browser]]|&#xA;&#xA;| [[60000, com.browser1], [1204000, com.browser]]|&#xA;&#xA;| [[60000, com.browser1], [120000, com.browser]]|&#xA;&#xA;| [[60000, com.browser1], [1201000, com.browser]]|&#xA;&#xA;| [[1200400, com.browser5]]|&#xA;&#xA;| [[60000, com.browser1], [1200400, com.browser]]|&#xA;&#xA;|[[60000, com.browser1], [1205000, com.browser6], [1205000, com.browser7]]|&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;RDD is like this&lt;/p&gt;&#xA;&#xA;&lt;p&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[WrappedArray ([60000, com.browser1], [60000, com.browser1])]&#xA;&#xA;[WrappedArray ([120000, com.browser])]&#xA;&#xA;[WrappedArray ([60000, com.browser1], [1204000, com.browser5])]&#xA;&#xA;[WrappedArray ([12075000, com.browser], [12075000, com.browser])]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And I want to turn the data into&lt;/p&gt;&#xA;&#xA;&lt;p&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Com.browser1 60000&#xA;&#xA;Com.browser1 60000&#xA;&#xA;Com.browser 12075000&#xA;&#xA;Com.browser 12075000&#xA;&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to turn the array elements of each line in RDD into one row. Of course, it can be another structure that is easy to analyze.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Because I only learn  spark and Scala a lot, I have try it for a long time but fail, so I hope you can guide me.&lt;/p&gt;&#xA;" OwnerUserId="6907343" LastEditorUserId="5880706" LastEditDate="2018-03-06T15:19:34.587" LastActivityDate="2018-03-07T02:33:26.837" Title="How to use Spark SQL parse the JSON of the array of objects" Tags="&lt;json&gt;&lt;scala&gt;&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;bigdata&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="0" />
  <row Id="49133358" PostTypeId="1" CreationDate="2018-03-06T14:37:07.423" Score="0" ViewCount="17" Body="&lt;p&gt;Right now I am reading a lot of csv files, do some mapping, and write them into elasticsearch.&#xA;The problem is my main key a timestamp can appear more than once within a file.&#xA;When I use functions like groupBy or window functions my order will not always be the same.&lt;br&gt;&#xA; Now I want to make sure that the order is exactly the same as in the files I read.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Code is as follows&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val text = spark.sparkContext.newAPIHadoopFile(srcPath + path + filenames, fc, kc, vc, spark.sparkContext.hadoopConfiguration)&#xA;&#xA;    val linesWithFileNames = text.asInstanceOf[NewHadoopRDD[LongWritable, Text]]&#xA;      .mapPartitionsWithInputSplit((inputSplit, iterator) =&amp;gt; {&#xA;        val file = inputSplit.asInstanceOf[FileSplit]&#xA;        iterator.map(tup =&amp;gt; (file.getPath, tup._2))&#xA;      })&#xA;&#xA;    //load data from file&#xA;    val lineregex =&#xA;      &quot;&quot;&quot;\(\D\)(;|\s)\d{4}-\d{2}-\d{2}(;|\s|T)\d{2}:\d{2}:\d{2}.*&quot;&quot;&quot;&#xA;    //filter data&#xA;    val filterData = linesWithFileNames.filter(f =&amp;gt; f._2.toString.matches(lineregex))&#xA;&#xA;    val mapRawData = MapRawDataWithIndex&#xA;&#xA;    //parse Data to case class&#xA;    val dsToRDD = filterData.zipWithIndex().flatMap(x =&amp;gt; mapRawData.mapRawLine(x._1, x._2))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I don't think there is a problem with this approach but it just does not look as nice as I want it to.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Right now it works like this: If FileA contains 10 rows and File B contains 10 rows the index of the first row of FileB will be 11 (10 because it starts at 0)&#xA;Is there an easy way of adding a rowindex that resets for every file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't want to read/write every file on it's own since there will be a lot of overhead. I am reading 1k+ files.&lt;/p&gt;&#xA;" OwnerUserId="2811630" LastActivityDate="2018-03-06T14:37:07.423" Title="Index rows on fileread and reset on new file" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49133747" PostTypeId="1" AcceptedAnswerId="49161121" CreationDate="2018-03-06T14:56:06.173" Score="1" ViewCount="34" Body="&lt;p&gt;I am conducting Principal Component Analysis in Spark Scala. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My output only display the principal component score vectors. But (1) how can I get displayed the principal component loadings and (2) select the variables that contribute to the most extent to the 1st and 2nd components(i.e., have particularly high loadings on the components). I suggest to look at the first two components as they usually capture the most variance. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help &amp;amp; advice appreciated!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I implemented so far:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// import org.apache.spark.ml.feature.{PCA, VectorAssembler, StandardScaler}&#xA;// import org.apache.spark.ml.linalg.Vectors&#xA;&#xA;   val dataset = (spark.createDataFrame(&#xA;   Seq((1.2, 1.3, 1.7, 1.9), (2.2, 2.3, 2.7, 2.9), (3.2, 3.3, 3.5, 3.7))&#xA;   ).toDF(&quot;f1&quot;, &quot;f2&quot;, &quot;f3&quot;, &quot;f4&quot;))    &#xA;&#xA;   val assembler = new VectorAssembler()&#xA;   .setInputCols(Array(&quot;f1&quot;, &quot;f2&quot;, &quot;f3&quot;, &quot;f4&quot;))&#xA;   .setOutputCol(&quot;features&quot;)&#xA;&#xA;   val output = assembler.transform(dataset) &#xA;&#xA; // Normalizing each feature to have a zero mean&#xA;&#xA;   val scaler = new StandardScaler()&#xA;    .setInputCol(&quot;features&quot;)&#xA;    .setOutputCol(&quot;scaledFeatures&quot;)&#xA;    .setWithStd(false)&#xA;    .setWithMean(true)&#xA;&#xA;  val scalerModel = scaler.fit(output)&#xA;&#xA;  val scaledData = scalerModel.transform(output)&#xA;&#xA;&#xA; // Compute PCA on a vector of features &#xA;&#xA;// Display the principal component score vectors&#xA;&#xA; val pca = new PCA()&#xA;  .setInputCol(&quot;scaledFeatures&quot;)&#xA;  .setOutputCol(&quot;pcaFeatures&quot;)&#xA;  .setK(2)&#xA;  .fit(scaledData)&#xA;&#xA; // ? How to extract the variables contributing most to principal components?&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="1389061" LastActivityDate="2018-03-07T21:01:02.267" Title="Select most important variables in terms of their contributions to PCA in Spark" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;pca&gt;&lt;apache-spark-ml&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49133836" PostTypeId="1" AcceptedAnswerId="49134365" CreationDate="2018-03-06T14:59:52.747" Score="1" ViewCount="30" Body="&lt;p&gt;Reading the &lt;a href=&quot;https://stackoverflow.com/questions/44404817/how-to-use-dataset-to-groupby&quot;&gt;this&lt;/a&gt; post I wonder how can we group the a Dataset but with multiple columns.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val test = Seq((&quot;New York&quot;, &quot;Jack&quot;, &quot;jdhj&quot;),&#xA;    (&quot;Los Angeles&quot;, &quot;Tom&quot;, &quot;ff&quot;),&#xA;    (&quot;Chicago&quot;, &quot;David&quot;, &quot;ff&quot;),&#xA;    (&quot;Houston&quot;, &quot;John&quot;, &quot;dd&quot;),&#xA;    (&quot;Detroit&quot;, &quot;Michael&quot;, &quot;fff&quot;),&#xA;    (&quot;Chicago&quot;, &quot;Andrew&quot;, &quot;ddd&quot;),&#xA;    (&quot;Detroit&quot;, &quot;Peter&quot;, &quot;dd&quot;),&#xA;    (&quot;Detroit&quot;, &quot;George&quot;, &quot;dkdjkd&quot;)&#xA;  )&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I would like to get&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Chicago, [( &quot;David&quot;, &quot;ff&quot;), (&quot;Andrew&quot;, &quot;ddd&quot;)] &lt;/p&gt;&#xA;" OwnerUserId="2282202" LastEditorUserId="2282202" LastEditDate="2018-03-06T15:01:44.673" LastActivityDate="2018-03-06T15:39:56.013" Title="How to use Dataset to group by, but entire rows" Tags="&lt;apache-spark&gt;&lt;dataset&gt;" AnswerCount="2" CommentCount="0" FavoriteCount="1" />
  <row Id="49134427" PostTypeId="1" CreationDate="2018-03-06T15:28:37.457" Score="0" ViewCount="17" Body="&lt;p&gt;I have the following code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;date = lectura.map(lambda x: ((x.split(&quot;,&quot;)[6], x.split(&quot;,&quot;)[7],x.split(&quot;,&quot;)[9]), (float(x.split(&quot;,&quot;)[3], float(x.split(&quot;,&quot;)[4] , float(x.split(&quot;,&quot;)[5]))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This generate a tuple with the following structure.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    type   model    activity      x            y           z&#xA;[(('a', 'nexus4', 'stand'), ('-5.958191', '0.6880646', '8.135345'))]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;X,Y,X are coordinates where the person is.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ok, then I calculate the MAX,MIN tuple from each model type and activity.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;with this code: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;max_ = datos.reduceByKey(lambda x, y: max(x,y))&#xA;min_ = datos.reduceByKey(lambda x, y: min(x,y))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I get ... Output: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Max = [(('a', 'nexus4', 'stand'), ('-5.958191', '0.6880646', '8.135345'))]&#xA;Min = [(('a', 'nexus5', 'stand'), ('-3.958191', '0.6880646', '8.145345'))]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then I have to calculate the AVG from X,Y,Z respect each Index.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't know how to calculate the AVG and STD from each model to generate an output with the following structure.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;User,Model,gt,mean(x,y,z),std(x,y,z),max(x,y,z),min(x,y,z)&#xA;  a, nexus4,stand,-3.0,0.7,8.2,2.8,0.14,0.0,-1.0,0.8,8.2,-5.0,0.6,8.2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I know that after calculating each MAX,MIN (I got it right now) STD and MEAN, I have to group for example using groupby, but my problem is that I don't really know how to count and to sum each value from X,Y,Z to calculate MEAN AND STD.&lt;/p&gt;&#xA;" OwnerUserId="9326486" LastActivityDate="2018-03-06T15:28:37.457" Title="CombineByKey or calculating sum and avg - RDD Pyspark" Tags="&lt;pyspark&gt;&lt;tuples&gt;&lt;average&gt;&lt;rdd&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49134479" PostTypeId="1" CreationDate="2018-03-06T15:30:32.427" Score="0" ViewCount="23" Body="&lt;p&gt;I'm trying to distribute data with IDs in cluster then call another class to do complex logic on these IDs in parallel in pySpark. I'm so confused of how to sort things out as the code below did not work&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have file myprocess.py contains&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def class MyProcess():&#xA;   def __init__(self, sqlcontext, x):&#xA;      /* code here */&#xA;   def complex_calculation(self):&#xA;      /* lots of sql and statiscal steps */&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then I have my main wrapper control.py&#xA;warehouse_location = abspath('spark-warehouse')&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark = SparkSession \&#xA;     .builder \&#xA;     .appName(&quot;complexlogic&quot;) \&#xA;     .config(&quot;spark.sql.warehouse.dir&quot;, warehouse_location) \&#xA;     .enableHiveSupport() \&#xA;     .getOrCreate()&#xA;&#xA;&#xA;sc = spark.sparkContext&#xA;sc.setLogLevel(&quot;ERROR&quot;)&#xA;&#xA;&#xA;sc.addPyFile(r&quot;myprocess.py&quot;)&#xA;&#xA;from myprocess import MyProcess&#xA;&#xA;&#xA;sqlContext = SQLContext(sc)&#xA;settings_bc = sc.broadcast({&#xA;    'mysqlContext': sqlContext&#xA;})&#xA;&#xA;/**&#xA;some code to create df_param&#xA;**/&#xA;df = df_param.repartition(&quot;id&quot;)&#xA;print('number of partitions', df.rdd.getNumPartitions())&#xA;rdd__param = df.rdd.map(lambda x: MyProcess(settings_bc.value, x).complex_calculation()).collect()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The error I get&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;_pickle.PicklingError: Could not serialize broadcast: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I understand that error is probably regarding passing sqlContext, but I think my issue is larger than that error, it is what is the right way to do what I'm trying to achieve (&lt;strong&gt;Edit&lt;/strong&gt;: I'm trying to use the ID to filter 17 hive tables with that id and use these 17 tables to do complex math. If I move outside map how I will achieve parallelism) . Any help is greatly appreciated.&lt;/p&gt;&#xA;" OwnerUserId="9020557" LastEditorUserId="9020557" LastEditDate="2018-03-06T16:36:34.350" LastActivityDate="2018-03-06T16:36:34.350" Title="Call class from external file in parallel pyspark" Tags="&lt;apache-spark&gt;&lt;parallel-processing&gt;&lt;pyspark&gt;&lt;rdd&gt;&lt;distributed-computing&gt;" AnswerCount="0" CommentCount="4" />
  <row Id="49134566" PostTypeId="1" CreationDate="2018-03-06T15:35:37.483" Score="-1" ViewCount="26" Body="&lt;p&gt;I've been looking now for some time, and finding a lot of broken examples and links from the past, but I have a 2 GB file of json data that I need to process line by line, run a significant amount of code on each line, and save out reformatted data to the cluster.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've been trying to do this in Spark 2.0/PySpark, but am not having much luck.  I can do it on a smaller file, but on my actual file my director runs out of heap memory.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I try and break up the file, I get the error listed here (&lt;a href=&quot;https://stackoverflow.com/questions/34443475/spark-getnewargs-error&quot;&gt;Spark __getnewargs__ error&lt;/a&gt;) but for obviously different reasons, as I'm not referencing columns.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm on CentOS6 with Hortonworks, single machine cluster for now.  I'm actually looking more for &quot;what I should be doing&quot; than just how to do it.  I know that Spark &lt;em&gt;can&lt;/em&gt; do this, but if there's a better way, then I'm happy to explore that as well.&lt;/p&gt;&#xA;" OwnerUserId="9395665" LastEditorUserId="1158351" LastEditDate="2018-03-07T08:01:51.733" LastActivityDate="2018-03-07T09:55:10.717" Title="How to process (iterate through) a large JSON file on a hadoop/Spark cluster?" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="5" />
  <row Id="49135078" PostTypeId="1" AcceptedAnswerId="49135288" CreationDate="2018-03-06T15:57:47.357" Score="0" ViewCount="20" Body="&lt;p&gt;I have a log like :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[Pipeline] timestamps&#xA;[Pipeline] {&#xA;[Pipeline] echo&#xA;20:33:05 0&#xA;[Pipeline] echo&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am trying to only extract the &lt;code&gt;time&lt;/code&gt; information here (20:33:05).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have tried to do the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val lines = sc.textFile(&quot;/logs/log7.txt&quot;)  &#xA;val individualLines=lines.flatMap(_.split(&quot;\n&quot;)) //Splitting file contentinto individual lines&#xA;val dates=individualLines.filter(value=&amp;gt;value.startsWith(&quot;[0-9]&quot;))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am getting the output as &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;MapPartitionsRDD[3] at filter at DateExtract.scala:30&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How should the regex be defined here?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help would be much appreciated.&lt;/p&gt;&#xA;" OwnerUserId="5279133" LastEditorUserId="6551426" LastEditDate="2018-03-06T16:27:33.023" LastActivityDate="2018-03-06T16:27:33.023" Title="Extracting timestamp from string with regex in Spark RDD" Tags="&lt;regex&gt;&lt;hadoop&gt;&lt;apache-spark&gt;&lt;rdd&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49135148" PostTypeId="1" CreationDate="2018-03-06T16:00:58.030" Score="0" ViewCount="26" Body="&lt;p&gt;I try to run simple spark code on kubernetes cluster using spark 2.3 native kubernetes deployment feature.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a kubernetes cluster running. At this time, the spark code does not read or write data. It creates an RDD from list and print out the result, just to validate the ability to run kubernetes on spark. Also, copied the spark app jar in the kubernetes container image too.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Below is the command i run. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;bin/spark-submit --master k8s://https://k8-master --deploy-mode cluster --name sparkapp --class com.sparrkonk8.rdd.MockWordCount --conf spark.executor.instances=5 --conf spark.kubernetes.container.image=myapp/sparkapp:1.0.0 local:///SparkApp.jar&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;2018-03-06 10:31:28 INFO  LoggingPodStatusWatcherImpl:54 - State&#xA;  changed, new state:    pod name:&#xA;  sparkapp-6e475a6ae18d3b7a89ca2b5f6ae7aae4-driver   namespace: default&#xA;     labels: spark-app-selector -&gt;&#xA;  spark-9649dd66e9a946d989e2136d342ef249, spark-role -&gt; driver   pod&#xA;  uid: 6d3e98cf-2153-11e8-85af-1204f474c8d2      creation time:&#xA;  2018-03-06T15:31:23Z   service account name: default   volumes:&#xA;  default-token-vwxvr    node name: 192-168-1-1.myapp.engg.com   start&#xA;  time: 2018-03-06T15:31:23Z     container images:&#xA;  dockerhub.com/myapp/sparkapp:1.0.0     phase: Failed   status:&#xA;  [ContainerStatus(containerID=docker://3617a400e4604600d5fcc69df396facafbb2d9cd485a63bc324c1406e72f0d35,&#xA;  image=dockerhub.com/myapp/sparkapp:1.0.0,&#xA;  imageID=docker-pullable://dockerhub.com/sparkapp@sha256:f051d86384422dff3e8c8a97db823de8e62af3ea88678da4beea3f58cdb924e5,&#xA;  lastState=ContainerState(running=null, terminated=null, waiting=null,&#xA;  additionalProperties={}), name=spark-kubernetes-driver, ready=false,&#xA;  restartCount=0, state=ContainerState(running=null,&#xA;  terminated=ContainerStateTerminated(containerID=docker://3617a400e4604600d5fcc69df396facafbb2d9cd485a63bc324c1406e72f0d35,&#xA;  exitCode=1, finishedAt=Time(time=2018-03-06T15:31:24Z,&#xA;  additionalProperties={}), message=null, reason=Error, signal=null,&#xA;  startedAt=Time(time=2018-03-06T15:31:24Z, additionalProperties={}),&#xA;  additionalProperties={}), waiting=null, additionalProperties={}),&#xA;  additionalProperties={})] 2018-03-06 10:31:28 INFO &#xA;  LoggingPodStatusWatcherImpl:54 - Container final statuses:&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Container name: spark-kubernetes-driver Container image:&#xA;  myapp/sparkapp:1.0.0 Container state: Terminated Exit code: 1&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="6707230" LastEditorUserId="3202696" LastEditDate="2018-03-06T21:02:02.257" LastActivityDate="2018-03-06T21:02:02.257" Title="Spark execution on kubernetes - driver pod fails" Tags="&lt;apache-spark&gt;&lt;kubernetes&gt;&lt;k8s&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49135248" PostTypeId="1" CreationDate="2018-03-06T16:06:41.467" Score="0" ViewCount="8" Body="&lt;p&gt;I'm trying to write a spark dataframe to a thrift server. I am able to read tables, but I want to programmatically update them as data comes in. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have tried to ways:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1) &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df.write.mode(&quot;append&quot;).format(&quot;jdbc&quot;).option(&quot;url&quot;,&quot;jdbc:hive2://192.168.2.187:10000/mytest&quot;).option(&quot;dbtable&quot;, &quot;my_test_table&quot;).save()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;java.sql.SQLException: Method not supported&#xA;    at org.apache.hive.jdbc.HivePreparedStatement.addBatch(HivePreparedStatement.java:75)&#xA;    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:651)&#xA;    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:821)&#xA;    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:821)&#xA;    at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:929)...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;2)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df.write.mode(&quot;append&quot;).jdbc(url,table,prop)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;output:&#xA; the exact same.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;the table and database exist. If there is a better way to do this I'm open to suggestions.&lt;/p&gt;&#xA;" OwnerUserId="9420497" LastActivityDate="2018-03-06T16:06:41.467" Title="Spark SQL’s DataSource API to write table to Spark Thrift Server" Tags="&lt;apache-spark&gt;&lt;hive&gt;&lt;spark-thriftserver&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49135432" PostTypeId="1" CreationDate="2018-03-06T16:16:27.923" Score="-4" ViewCount="20" Body="&lt;p&gt;I have to find a solution to this coding below, instructions are given underneath the coding, can anyone help?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;def search (item, list_to_search):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&quot;&quot;&quot;&#xA;&#xA;Function Arguments: Your function takes two arguments. &#xA;    item: this the item (object) you need to search in a list. &#xA;    list_to_search: this is the python list you will search the item in. &#xA;&#xA;Task: You will search the list to find the item. &#xA;    if the item is in the &quot;list_to_search&quot;, then return the index of the item. &#xA;    if the item is not in the list_to_search, return -1&#xA;&#xA;What it returns: This will return the index of the item (integer) or -1&#xA;&#xA;&quot;&quot;&quot;&#xA;&#xA;return&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="9452264" LastActivityDate="2018-03-06T16:16:27.923" Title="Apache Spark Python coding issue" Tags="&lt;python&gt;&lt;apache-spark&gt;" AnswerCount="0" CommentCount="2" ClosedDate="2018-03-06T23:54:10.433" />
  <row Id="49136194" PostTypeId="1" AcceptedAnswerId="49136530" CreationDate="2018-03-06T16:58:16.417" Score="0" ViewCount="23" Body="&lt;p&gt;I'm trying to run some Spark Scala code : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import org.apache.spark.{SparkConf, SparkContext}&#xA;import scala.collection.mutable.ListBuffer&#xA;&#xA;object EzRecoMRjobs {&#xA;&#xA;  def main(args: Array[String]) {&#xA;&#xA;    val conf = new SparkConf()&#xA;    conf.setMaster(&quot;local&quot;)&#xA;    conf.setAppName(&quot;Product Cardinalities&quot;)&#xA;    val sc = new SparkContext(conf)&#xA;&#xA;    val dataset = sc.textFile(args(0))&#xA;    // Load parameters&#xA;    val customerIndex = args(1).toInt - 1&#xA;    val ProductIndex = args(2).toInt - 1&#xA;    val outputPath = args(3).toString&#xA;&#xA;&#xA;    val resu = dataset.map( line =&amp;gt; { val orderId = line.split(&quot;\t&quot;)(0)&#xA;                                      val cols = line.split(&quot;\t&quot;)(1).split(&quot;;&quot;)&#xA;                                      cols(ProductIndex)&#xA;                                     })&#xA;        .map( x =&amp;gt; (x,1) )&#xA;        .reduceByKey(_ + _)&#xA;        .saveAsTextFile(outputPath)&#xA;&#xA;    sc.stop()&#xA;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This code works in Intellij and write the result in the &quot;outputPath&quot; folder. &#xA;From my Intellij project I have generate a .jar file and I want to run this code with a spark-submit. So in my terminal I launch :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark-submit \&#xA;  --jars /Users/users/Documents/TestScala/ezRecoPreBuild/target/ezRecoPreBuild-1.0-SNAPSHOT.jar \&#xA;  --class com.np6.scala.EzRecoMRjobs \&#xA;  --master local \&#xA;  /Users/users/Documents/DATA/data.txt 1 2 /Users/users/Documents/DATA/dossier &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But I got this error : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Exception in thread &quot;main&quot; java.lang.NumberFormatException: For input string: &quot;/Users/users/Documents/DATA/dossier&quot;&#xA;    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)&#xA;    at java.lang.Integer.parseInt(Integer.java:569)&#xA;    at java.lang.Integer.parseInt(Integer.java:615)&#xA;    at scala.collection.immutable.StringLike$class.toInt(StringLike.scala:272)&#xA;    at scala.collection.immutable.StringOps.toInt(StringOps.scala:29)&#xA;    at com.np6.scala.EzRecoMRjobs$.main(ezRecoMRjobs.scala:51)&#xA;    at com.np6.scala.EzRecoMRjobs.main(ezRecoMRjobs.scala)&#xA;    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#xA;    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#xA;    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#xA;    at java.lang.reflect.Method.invoke(Method.java:498)&#xA;    at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775)&#xA;    at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)&#xA;    at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)&#xA;    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)&#xA;    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What's the reason of this error ? Thank's&lt;/p&gt;&#xA;" OwnerUserId="9366049" LastActivityDate="2018-03-06T17:18:24.570" Title="Input string error after a spark-submit" Tags="&lt;string&gt;&lt;scala&gt;&lt;apache-spark&gt;&lt;input&gt;&lt;submit&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="49136584" PostTypeId="1" CreationDate="2018-03-06T17:21:12.530" Score="0" ViewCount="45" Body="&lt;p&gt;I have below data and would like to match the data from ID column of df1 to df2.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;df1:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ID    key&#xA;1     &#xA;2     &#xA;3&#xA;4&#xA;5&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Df2:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;first  second third  key&#xA;--------------------------&#xA;1       9       9    777&#xA;9       8       8    878&#xA;8       1      10    765&#xA;10      12      19   909&#xA;11      2       20   708&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val finalDF = Df1.join(DF2.withColumnRenamed(&quot;key&quot;, &quot;key2&quot;), $&quot;ID&quot; === $&quot;first&quot; || $&quot;ID&quot; === $&quot;second&quot; || $&quot;ID&quot; === $&quot;third&quot;,&quot;left&quot;).select($&quot;ID&quot;, $&quot;key2&quot;.as(&quot;key&quot;)).show(false)&#xA;&#xA;val notMatchingDF = finalDF.filter($&quot;key&quot; === &quot;&quot;)&#xA;val matchingDF = finalDF.except(notMatchingDF)&#xA;val columnsToCheck = DF2.columns.toSet - &quot;key&quot; toList&#xA;&#xA;import org.apache.spark.sql.functions._&#xA;val tempSelectedDetailsDF = DF2.select(array(columnsToCheck.map(col): _*).as(&quot;array&quot;), col(&quot;key&quot;).as(&quot;key2&quot;))&#xA;val arrayContains = udf((array: collection.mutable.WrappedArray[String], value: String) =&amp;gt; array.contains(value))&#xA;val finalDF = df1.join(tempSelectedDetailsDF, arrayContains($&quot;array&quot;, $&quot;ID&quot;), &quot;left&quot;)&#xA;  .select($&quot;ID&quot;, $&quot;key2&quot;.as(&quot;key&quot;))&#xA;  .na.fill(&quot;&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am getting the output as below,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ID    key&#xA;1     777&#xA;1     765&#xA;2     708     &#xA;3&#xA;4&#xA;5&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However i am expecting as below,here the logic is from df1 we have id column  value 1 and in df2 the value 1 is matching more than once hence i am getting above output. but i should not match second occurrence when it matches in the first occurrence.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Expected output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ID    key&#xA;1     777&#xA;2     708     &#xA;3&#xA;4&#xA;5&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Kindly help.&lt;/p&gt;&#xA;" OwnerUserId="6059250" LastActivityDate="2018-03-08T17:53:17.847" Title="Scala: How to match a data from one column to other columns in data frame" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49136952" PostTypeId="1" CreationDate="2018-03-06T17:43:20.437" Score="0" ViewCount="15" Body="&lt;p&gt;I've been using Spark SQL &amp;amp; Spark-XML to parse an XML file, but then I want to be able to do the following to a file I parse:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Modify the value within the element tags (then save the xml)&lt;/li&gt;&#xA;&lt;li&gt;Modify the attribute of an element (then save the xml)&lt;/li&gt;&#xA;&lt;li&gt;Combine two XMLs, one with a tag like &lt;code&gt;&amp;lt;ChangeThisPreviousElement&amp;gt;value&amp;lt;/...&amp;gt;&lt;/code&gt; And that &quot;ChangeThisPreviousElement's value&quot; will change &lt;code&gt;&amp;lt;PreviousElement&amp;gt;value1&amp;lt;/PreviousElement&amp;gt;&lt;/code&gt; and then save the xml&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Are these things possible with Spark-XML &amp;amp; Spark-SQL? Or should I be doing some Dom manipulation library?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically I want to radically improve an XML by modifying a lot of its elements.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or should I be using Spark-XML just to READ the data, then generating the newer XML with another library using the new data classes I create?&lt;/p&gt;&#xA;" OwnerUserId="443664" LastActivityDate="2018-03-06T17:43:20.437" Title="Add new values and columns to an XML and save" Tags="&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;apache-spark-xml&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49136986" PostTypeId="1" AcceptedAnswerId="49138486" CreationDate="2018-03-06T17:46:09.633" Score="-2" ViewCount="37" Body="&lt;p&gt;I am reading CSV file in spark using scala. And in the CSV file I am getting null in line 10(lets say). So my code throwing nullpointerexception at this line. So it is not printing next records.&#xA;Below is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import org.apache.spark.sql.SparkSession&#xA;import java.lang.Long&#xA;&#xA;&#xA;object HighestGDP {&#xA;   def main(args:Array[String]){&#xA;     val spark = SparkSession.builder().appName(&quot;GDP&quot;).master(&quot;local&quot;).getOrCreate()&#xA;     val data  = spark.read.csv(&quot;D:\\BGH\\Spark\\World_Bank_Indicators.csv&quot;).rdd&#xA;&#xA;     val result = data.filter(line=&amp;gt;line.getString(1).substring(4,8).equals(&quot;2009&quot;)||line.getString(1).substring(4,8).equals(&quot;2010&quot;))&#xA;&#xA;     result.foreach(println)&#xA;     var gdp2009 = result.filter(rec=&amp;gt;rec.getString(1).substring(4,8).equals(&quot;2009&quot;))&#xA;     .map{line=&amp;gt;{&#xA;       var GDP= 0L&#xA;       if(line.getString(19).equals(null))&#xA;         GDP=0L&#xA;       else&#xA;         GDP= line.getString(19).replaceAll(&quot;,&quot;, &quot;&quot;).toLong&#xA;       (line.getString(0),GDP)&#xA;     }}&#xA;&#xA;     gdp2009.foreach(println)&#xA;     result.foreach(println)&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So is there any way where I can set the value to 0 where value is null. I tried with if else but still its not working.&#xA;ERROR:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;18/03/06 22:56:01 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1208 bytes result sent to driver&#xA;18/03/06 22:56:01 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 297 ms on localhost (1/1)&#xA;18/03/06 22:56:01 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool &#xA;18/03/06 22:56:01 INFO DAGScheduler: ResultStage 1 (foreach at HighestGDP.scala:12) finished in 0.297 s&#xA;18/03/06 22:56:01 INFO DAGScheduler: Job 1 finished: foreach at HighestGDP.scala:12, took 0.346954 s&#xA;18/03/06 22:56:01 INFO SparkContext: Starting job: foreach at HighestGDP.scala:21&#xA;18/03/06 22:56:01 INFO DAGScheduler: Got job 2 (foreach at HighestGDP.scala:21) with 1 output partitions&#xA;18/03/06 22:56:01 INFO DAGScheduler: Final stage: ResultStage 2 (foreach at HighestGDP.scala:21)&#xA;18/03/06 22:56:01 INFO DAGScheduler: Parents of final stage: List()&#xA;18/03/06 22:56:01 INFO DAGScheduler: Missing parents: List()&#xA;18/03/06 22:56:01 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at map at HighestGDP.scala:14), which has no missing parents&#xA;18/03/06 22:56:01 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.3 KB, free 355.2 MB)&#xA;18/03/06 22:56:01 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.1 KB, free 355.2 MB)&#xA;18/03/06 22:56:01 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 13.133.209.137:57085 (size: 7.1 KB, free: 355.5 MB)&#xA;18/03/06 22:56:01 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1012&#xA;18/03/06 22:56:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at map at HighestGDP.scala:14)&#xA;18/03/06 22:56:01 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks&#xA;18/03/06 22:56:01 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5918 bytes)&#xA;18/03/06 22:56:01 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)&#xA;18/03/06 22:56:01 INFO FileScanRDD: Reading File path: file:///D:/BGH/Spark/World_Bank_Indicators.csv, range: 0-260587, partition values: [empty row]&#xA;(Afghanistan,425)&#xA;(Albania,3796)&#xA;(Algeria,3952)&#xA;18/03/06 22:56:01 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)&#xA;java.lang.NullPointerException&#xA;    at HighestGDP$$anonfun$3.apply(HighestGDP.scala:15)&#xA;    at HighestGDP$$anonfun$3.apply(HighestGDP.scala:14)&#xA;    at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)&#xA;    at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#xA;    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#xA;    at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$27.apply(RDD.scala:894)&#xA;    at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$27.apply(RDD.scala:894)&#xA;    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899)&#xA;    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899)&#xA;    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)&#xA;    at org.apache.spark.scheduler.Task.run(Task.scala:86)&#xA;    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)&#xA;    at java.lang.Thread.run(Unknown Source)&#xA;18/03/06 22:56:01 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2, localhost): java.lang.NullPointerException&#xA;    at HighestGDP$$anonfun$3.apply(HighestGDP.scala:15)&#xA;    at HighestGDP$$anonfun$3.apply(HighestGDP.scala:14)&#xA;    at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)&#xA;    at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#xA;    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#xA;    at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$27.apply(RDD.scala:894)&#xA;    at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$27.apply(RDD.scala:894)&#xA;    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899)&#xA;    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899)&#xA;    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)&#xA;    at org.apache.spark.scheduler.Task.run(Task.scala:86)&#xA;    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)&#xA;    at java.lang.Thread.run(Unknown Source)&#xA;&#xA;18/03/06 22:56:01 ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job&#xA;18/03/06 22:56:01 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool &#xA;18/03/06 22:56:01 INFO TaskSchedulerImpl: Cancelling stage 2&#xA;18/03/06 22:56:01 INFO DAGScheduler: ResultStage 2 (foreach at HighestGDP.scala:21) failed in 0.046 s&#xA;18/03/06 22:56:01 INFO DAGScheduler: Job 2 failed: foreach at HighestGDP.scala:21, took 0.046961 s&#xA;Exception in thread &quot;main&quot; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost): java.lang.NullPointerException&#xA;    at HighestGDP$$anonfun$3.apply(HighestGDP.scala:15)&#xA;    at HighestGDP$$anonfun$3.apply(HighestGDP.scala:14)&#xA;    at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)&#xA;    at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#xA;    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#xA;    at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$27.apply(RDD.scala:894)&#xA;    at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$27.apply(RDD.scala:894)&#xA;    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899)&#xA;    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899)&#xA;    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)&#xA;    at org.apache.spark.scheduler.Task.run(Task.scala:86)&#xA;    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)&#xA;    at java.lang.Thread.run(Unknown Source)&#xA;&#xA;Driver stacktrace:&#xA;    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)&#xA;    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)&#xA;    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)&#xA;    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&#xA;    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)&#xA;    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)&#xA;    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)&#xA;    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)&#xA;    at scala.Option.foreach(Option.scala:257)&#xA;    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)&#xA;    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)&#xA;    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)&#xA;    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)&#xA;    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)&#xA;    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)&#xA;    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873)&#xA;    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886)&#xA;    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899)&#xA;    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913)&#xA;    at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:894)&#xA;    at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:892)&#xA;    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&#xA;    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)&#xA;    at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)&#xA;    at org.apache.spark.rdd.RDD.foreach(RDD.scala:892)&#xA;    at HighestGDP$.main(HighestGDP.scala:21)&#xA;    at HighestGDP.main(HighestGDP.scala)&#xA;Caused by: java.lang.NullPointerException&#xA;    at HighestGDP$$anonfun$3.apply(HighestGDP.scala:15)&#xA;    at HighestGDP$$anonfun$3.apply(HighestGDP.scala:14)&#xA;    at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)&#xA;    at scala.collection.Iterator$class.foreach(Iterator.scala:891)&#xA;    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)&#xA;    at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$27.apply(RDD.scala:894)&#xA;    at org.apache.spark.rdd.RDD$$anonfun$foreach$1$$anonfun$apply$27.apply(RDD.scala:894)&#xA;    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899)&#xA;    at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899)&#xA;    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)&#xA;    at org.apache.spark.scheduler.Task.run(Task.scala:86)&#xA;    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)&#xA;    at java.lang.Thread.run(Unknown Source)&#xA;18/03/06 22:56:01 INFO SparkContext: Invoking stop() from shutdown hook&#xA;18/03/06 22:56:01 INFO SparkUI: Stopped Spark web UI at http://13.133.209.137:4040&#xA;18/03/06 22:56:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!&#xA;18/03/06 22:56:01 INFO MemoryStore: MemoryStore cleared&#xA;18/03/06 22:56:01 INFO BlockManager: BlockManager stopped&#xA;18/03/06 22:56:01 INFO BlockManagerMaster: BlockManagerMaster stopped&#xA;18/03/06 22:56:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!&#xA;18/03/06 22:56:01 INFO SparkContext: Successfully stopped SparkContext&#xA;18/03/06 22:56:01 INFO ShutdownHookManager: Shutdown hook called&#xA;18/03/06 22:56:01 INFO ShutdownHookManager: Deleting directory C:\Users\kumar.harsh\AppData\Local\Temp\spark-65330823-f67a-4a9d-acaf-42478e3b7109&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="9323888" LastEditorUserId="9323888" LastEditDate="2018-03-06T18:04:38.360" LastActivityDate="2018-03-06T19:33:42.143" Title="Getting NullPointerException in scala./Spark code" Tags="&lt;scala&gt;&lt;csv&gt;&lt;apache-spark&gt;&lt;nullpointerexception&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="49137397" PostTypeId="1" AcceptedAnswerId="49144437" CreationDate="2018-03-06T18:11:32.383" Score="0" ViewCount="29" Body="&lt;p&gt;I just upgraded my spark project from 2.2.1 to 2.3.0 to find the versioning exception below. I have dependencies on the spark-cassandra-connector.2.0.7 and cassandra-driver-core.3.4.0 from datastax which in turn have dependencies on netty 4.x whereas spark 2.3.0 uses 3.9.x. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The class raising the exception, org.apache.spark.network.util.NettyMemoryMetrics, has been introduced in spark 2.3.0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is downgrading my Cassandra dependencies the only way round the exception? Thanks!&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Exception in thread &quot;main&quot; java.lang.NoSuchMethodError: io.netty.buffer.PooledByteBufAllocator.metric()Lio/netty/buffer/PooledByteBufAllocatorMetric;&#xA;at org.apache.spark.network.util.NettyMemoryMetrics.registerMetrics(NettyMemoryMetrics.java:80)&#xA;at org.apache.spark.network.util.NettyMemoryMetrics.&amp;lt;init&amp;gt;(NettyMemoryMetrics.java:76)&#xA;at org.apache.spark.network.client.TransportClientFactory.&amp;lt;init&amp;gt;(TransportClientFactory.java:109)&#xA;at org.apache.spark.network.TransportContext.createClientFactory(TransportContext.java:99)&#xA;at org.apache.spark.rpc.netty.NettyRpcEnv.&amp;lt;init&amp;gt;(NettyRpcEnv.scala:71)&#xA;at org.apache.spark.rpc.netty.NettyRpcEnvFactory.create(NettyRpcEnv.scala:461)&#xA;at org.apache.spark.rpc.RpcEnv$.create(RpcEnv.scala:57)&#xA;at org.apache.spark.SparkEnv$.create(SparkEnv.scala:249)&#xA;at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:175)&#xA;at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:256)&#xA;at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:423)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="5074669" LastActivityDate="2018-03-07T04:58:37.793" Title="Spark 2.3.0 netty version issue: NoSuchMethod io.netty.buffer.PooledByteBufAllocator.metric()" Tags="&lt;apache-spark&gt;&lt;cassandra&gt;&lt;netty&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49137526" PostTypeId="1" AcceptedAnswerId="49141321" CreationDate="2018-03-06T18:19:51.713" Score="0" ViewCount="24" Body="&lt;p&gt;How does Spark implement intersection method? Does it require 2 RDDs to colocate on a single machine?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From &lt;a href=&quot;https://stackoverflow.com/questions/34542623/spark-intersection-is-taking-much-time&quot;&gt;here&lt;/a&gt; it says that it uses hashtables, which is a bit odd as it's probably not scalable and sorting both rdds and then comparing item by item might have provided a more scalable solution.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any thoughts on the subject are welcome&lt;/p&gt;&#xA;" OwnerUserId="2418409" LastActivityDate="2018-03-06T22:47:50.173" Title="Spark intersection implementation" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49137953" PostTypeId="1" CreationDate="2018-03-06T18:47:05.210" Score="0" ViewCount="28" Body="&lt;p&gt;I am performing a join by bucketizing one of my dataframes and, given the frequent use of the dataframes I have persisted them to cache. However I see that the total time spent in GC has increased to &gt;10% of execution time. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it possible for me to perform in scala a partial &lt;code&gt;unpersist&lt;/code&gt; of the dataframe to actively prune cached data as and when it turns obsolete for my use case.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EX: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;srcDF.persist()&#xA;srcDF.count()&#xA;val df1 = srcDF.filter(col(&quot;bucket_id&quot;) === lit(1))&#xA;val result = df1.join(otherDF, Seq(&quot;field1&quot;))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Given this execution is it possible for me to do a partial &lt;code&gt;unpersist()&lt;/code&gt; to exclude everything with &quot;bucket_id&quot; = &lt;code&gt;1&lt;/code&gt;&lt;/p&gt;&#xA;" OwnerUserId="1206986" LastActivityDate="2018-03-06T18:47:05.210" Title="Partial unpersist of spark dataframe in scala" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;spark-dataframe&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49138350" PostTypeId="1" CreationDate="2018-03-06T19:14:27.210" Score="0" ViewCount="11" Body="&lt;p&gt;I am trying to understand the concept of window operation in spark steaming but whenever I am running the code I'm running into this problem:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the first batch interval it is printing the output but after the first batch interval I am not able to print my output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have tried running it for a longer period also and it is able to read the data but not to print the output.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have single node spark 1.6.2 with yarn as master.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Trying to run below code in spark-shell.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val c_streamingContext = new StreamingContext(sc, Seconds(10))&#xA;val Steam = c_streamingContext.textFileStream(&quot;/user/abc/input/data&quot;)&#xA;val DSteramdata = dpiSteam.map {x=&amp;gt;(x.split(&quot;,&quot;)(0),x.split(&quot;,&quot;)(1),x.split(&quot;,&quot;)(2),x.split(&quot;,&quot;)(3))}.cache()&#xA;val WINDOW_LENGTH = new Duration(30 * 1000)&#xA;val WindowDStream = DSteramdata.window(WINDOW_LENGTH)&#xA;&#xA;WindowDStream.foreachRDD(aRDD =&amp;gt; {&#xA;println(&quot;coming...&quot;)&#xA;if (aRDD.count() &amp;gt; 0){&#xA;&#xA;val sqlContext = org.apache.spark.sql.SQLContext.getOrCreate(org.apache.spark.SparkContext.getOrCreate())&#xA;val DF = sqlContext.createDataFrame(aRDD ).toDF(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;)&#xA;DF.registerTempTable(&quot;DPI_LOCATION_STREAMING&quot;)  &#xA;println(&quot;Print DPI of dataframe &quot;+ DF.show())&#xA;DF.printSchema();&#xA;}else{&#xA;println(&quot;No Data Found.&quot;)&#xA;}&#xA;})&#xA;&#xA;c_streamingContext.start();&#xA;c_streamingContext.awaitTermination();&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="3951575" LastEditorUserId="9442379" LastEditDate="2018-03-07T00:22:18.323" LastActivityDate="2018-03-07T00:22:18.323" Title="Spark Streaming Window operation not giving output after window interval" Tags="&lt;apache-spark&gt;&lt;spark-dataframe&gt;&lt;spark-streaming&gt;&lt;yarn&gt;&lt;sliding-window&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49138772" PostTypeId="1" AcceptedAnswerId="49140062" CreationDate="2018-03-06T19:42:48.180" Score="-1" ViewCount="40" Body="&lt;p&gt;I am new to Scala, while running one spark program I am getting null Pointer exception. Can anyone point me how to solve this.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val data = spark.read.csv(&quot;C:\\File\\Path.csv&quot;).rdd&#xA;&#xA;val result = data.map{ line =&amp;gt; {&#xA;  val population  = line.getString(10).replaceAll(&quot;,&quot;,&quot;&quot;)&#xA;   var popNum = 0L&#xA;    if (population.length()&amp;gt; 0)&#xA;&#xA;     popNum = Long.parseLong(population)&#xA;     (popNum, line.getString(0))&#xA;&#xA;   }}&#xA;&#xA;.sortByKey(false)&#xA;.first()&#xA;&#xA;//spark.sparkContext.parallelize(Seq(result)).saveAsTextFile(args(1))&#xA;&#xA;println(&quot;The result is: &quot;+ result)&#xA;&#xA;spark.stop&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Error message :&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Caused by: java.lang.NullPointerException&lt;br&gt;&#xA;      at com.nfs.WBI.KPI01.HighestUrbanPopulation$$anonfun$1.apply(HighestUrbanPopulation.scala:23)&lt;br&gt;&#xA;      at com.nfs.WBI.KPI01.HighestUrbanPopulation$$anonfun$1.apply(HighestUrbanPopulation.scala:22)&lt;br&gt;&#xA;      at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="7203772" LastEditorUserId="205233" LastEditDate="2018-03-06T19:44:58.513" LastActivityDate="2018-03-06T21:54:53.767" Title="Null Pointer exception in Map function of Spark program" Tags="&lt;scala&gt;&lt;dictionary&gt;&lt;apache-spark&gt;&lt;nullpointerexception&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49139228" PostTypeId="1" CreationDate="2018-03-06T20:12:59.517" Score="0" ViewCount="24" Body="&lt;p&gt;As I understand it is a the total number of cores in the cluster. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;But I was unable to confirm this at the moment.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;And one more point, each executor is basically a JVM, right? If that's the case, can we have more executors in one single Machine? Would that be recommended?&lt;/p&gt;&#xA;" OwnerUserId="3245356" LastActivityDate="2018-03-06T20:12:59.517" Title="Is the default number of partitions in an RDD in Spark a Result of total number of cores, or number of executors?" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;bigdata&gt;" AnswerCount="0" CommentCount="0" ClosedDate="2018-03-07T08:12:09.007" />
  <row Id="49139239" PostTypeId="1" CreationDate="2018-03-06T20:13:27.170" Score="-2" ViewCount="20" Body="&lt;p&gt;How do I connect RStudio desktop (windows machine) to remote Spark on Hadoop cluster?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried sparklyr library but could not find any example to connect to remote Spark cluster.&lt;/p&gt;&#xA;" OwnerUserId="6042337" LastActivityDate="2018-03-06T21:53:01.923" Title="Connect RStudio to Spark on Hadoop Cluster" Tags="&lt;r&gt;&lt;apache-spark&gt;&lt;rstudio&gt;&lt;sparkr&gt;&lt;sparklyr&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49139272" PostTypeId="1" CreationDate="2018-03-06T20:16:24.080" Score="0" ViewCount="19" Body="&lt;p&gt;I try to implement a simple spark-streaming job wich consume a json string from a kafka topic and execute two action:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;print schema&lt;/li&gt;&#xA;&lt;li&gt;show the dataframe created from the rdd of json string&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;the problem is that the ConsumerRecord in my Dstream, recover from kafka, is not serializable. But the code seem very basic.&#xA;here is a simple code reproducing the error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import org.apache.kafka.common.serialization.StringDeserializer&#xA;import org.apache.log4j.{Level, Logger}&#xA;import org.apache.spark.sql.SQLContext&#xA;import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe&#xA;import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent&#xA;import org.apache.spark.streaming.kafka010._&#xA;import org.apache.spark.streaming.{Seconds, StreamingContext}&#xA;import org.apache.spark.{SparkConf, SparkContext}&#xA;&#xA;object ExempleStackOverFlow {&#xA;&#xA;  def main(args: Array[String]): Unit = {&#xA;&#xA;    Logger.getLogger(&quot;org&quot;).setLevel(Level.ERROR)&#xA;    Logger.getLogger(&quot;akka&quot;).setLevel(Level.ERROR)&#xA;&#xA;    val conf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;exemple&quot;)&#xA;    val sc = new SparkContext(conf)&#xA;    val sqlContext = new SQLContext(sc)&#xA;    val ssc = new StreamingContext(sc, Seconds(2))&#xA;&#xA;    val kafkaParams = Map[String, Object](&#xA;      &quot;bootstrap.servers&quot; -&amp;gt; &quot;localhost:9092&quot;,&#xA;      &quot;key.deserializer&quot; -&amp;gt; classOf[StringDeserializer],&#xA;      &quot;value.deserializer&quot; -&amp;gt; classOf[StringDeserializer],&#xA;      &quot;group.id&quot; -&amp;gt; &quot;use_a_separate_group_id_for_each_stream&quot;,&#xA;      &quot;auto.offset.reset&quot; -&amp;gt; &quot;latest&quot;,&#xA;      &quot;enable.auto.commit&quot; -&amp;gt; (false: java.lang.Boolean)&#xA;    )&#xA;&#xA;    val topics = Array(&quot;test&quot;)&#xA;    val stream = KafkaUtils.createDirectStream[String, String](&#xA;      ssc,&#xA;      PreferConsistent,&#xA;      Subscribe[String, String](topics, kafkaParams)&#xA;    )&#xA;&#xA;    stream&#xA;      .map(record =&amp;gt;&#xA;        record.value)&#xA;      .foreachRDD { rdd =&amp;gt;&#xA;        if (!rdd.isEmpty) {&#xA;          val df = sqlContext.read.json(rdd)&#xA;          df.printSchema&#xA;          df.show(false)&#xA;        }&#xA;      }&#xA;    stream.print&#xA;&#xA;    ssc.start() // Start the computation&#xA;    ssc.awaitTermination() // Wait for the computation to terminate&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and the build.sbt file: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;name := &quot;exemple&quot;&#xA;version := &quot;1.0.0&quot;&#xA;scalaVersion := &quot;2.11.11&quot;&#xA;&#xA;libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-sql&quot; % &quot;2.2.0&quot;&#xA;libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-streaming&quot; % &quot;2.2.0&quot;&#xA;libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-streaming-kafka-0-10&quot; % &quot;2.2.0&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I inject the following string in the kafka topic named test:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{&quot;id&quot;:1}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;here is the output console throwing the error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;-------------------------------------------&#xA;Time: 1520365418000 ms&#xA;-------------------------------------------&#xA;&#xA;-------------------------------------------&#xA;Time: 1520365420000 ms&#xA;-------------------------------------------&#xA;&#xA;root&#xA; |-- id: long (nullable = true)&#xA;&#xA;+---+&#xA;|id |&#xA;+---+&#xA;|1  |&#xA;+---+&#xA;&#xA;18/03/06 19:43:46 ERROR Executor: Exception in task 0.0 in stage 11.0 (TID 11)&#xA;java.io.NotSerializableException: org.apache.kafka.clients.consumer.ConsumerRecord&#xA;Serialization stack:&#xA;    - object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = test, partition = 0, offset = 1, CreateTime = 1520365419826, checksum = 3190363273, serialized key size = -1, serialized value size = 8, key = null, value = {&quot;id&quot;:1}))&#xA;    - element of array (index: 0)&#xA;    - array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1)&#xA;    at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)&#xA;    at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)&#xA;    at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)&#xA;    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:383)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;    at java.lang.Thread.run(Thread.java:748)&#xA;18/03/06 19:43:46 ERROR TaskSetManager: Task 0.0 in stage 11.0 (TID 11) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord&#xA;Serialization stack:&#xA;    - object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = test, partition = 0, offset = 1, CreateTime = 1520365419826, checksum = 3190363273, serialized key size = -1, serialized value size = 8, key = null, value = {&quot;id&quot;:1}))&#xA;    - element of array (index: 0)&#xA;    - array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1); not retrying&#xA;18/03/06 19:43:46 ERROR JobScheduler: Error running job streaming job 1520365422000 ms.1&#xA;org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 11.0 (TID 11) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord&#xA;Serialization stack:&#xA;    - object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = test, partition = 0, offset = 1, CreateTime = 1520365419826, checksum = 3190363273, serialized key size = -1, serialized value size = 8, key = null, value = {&quot;id&quot;:1}))&#xA;    - element of array (index: 0)&#xA;    - array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1)&#xA;    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)&#xA;    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)&#xA;    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)&#xA;    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&#xA;    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)&#xA;    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)&#xA;    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)&#xA;    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)&#xA;    at scala.Option.foreach(Option.scala:257)&#xA;    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)&#xA;    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)&#xA;    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)&#xA;    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)&#xA;    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)&#xA;    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)&#xA;    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)&#xA;    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)&#xA;    at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:123)&#xA;    at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:50)&#xA;    at org.apache.spark.streaming.dstream.DStream$$anonfun$print$2$$anonfun$foreachFunc$3$1.apply(DStream.scala:735)&#xA;    at org.apache.spark.streaming.dstream.DStream$$anonfun$print$2$$anonfun$foreachFunc$3$1.apply(DStream.scala:734)&#xA;    at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)&#xA;    at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)&#xA;    at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)&#xA;    at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)&#xA;    at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)&#xA;    at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)&#xA;    at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)&#xA;    at scala.util.Try$.apply(Try.scala:192)&#xA;    at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)&#xA;    at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)&#xA;    at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)&#xA;    at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)&#xA;    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)&#xA;    at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;    at java.lang.Thread.run(Thread.java:748)&#xA;Exception in thread &quot;main&quot; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 11.0 (TID 11) had a not serializable result: org.apache.kafka.clients.consumer.ConsumerRecord&#xA;Serialization stack:&#xA;    - object not serializable (class: org.apache.kafka.clients.consumer.ConsumerRecord, value: ConsumerRecord(topic = test, partition = 0, offset = 1, CreateTime = 1520365419826, checksum = 3190363273, serialized key size = -1, serialized value size = 8, key = null, value = {&quot;id&quot;:1}))&#xA;    - element of array (index: 0)&#xA;    - array (class [Lorg.apache.kafka.clients.consumer.ConsumerRecord;, size 1)&#xA;    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)&#xA;    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)&#xA;    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)&#xA;    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&#xA;    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)&#xA;    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)&#xA;    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)&#xA;    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)&#xA;    at scala.Option.foreach(Option.scala:257)&#xA;    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)&#xA;    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)&#xA;    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)&#xA;    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)&#xA;    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)&#xA;    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)&#xA;    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)&#xA;    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)&#xA;    at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:123)&#xA;    at org.apache.spark.streaming.kafka010.KafkaRDD.take(KafkaRDD.scala:50)&#xA;    at org.apache.spark.streaming.dstream.DStream$$anonfun$print$2$$anonfun$foreachFunc$3$1.apply(DStream.scala:735)&#xA;    at org.apache.spark.streaming.dstream.DStream$$anonfun$print$2$$anonfun$foreachFunc$3$1.apply(DStream.scala:734)&#xA;    at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)&#xA;    at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)&#xA;    at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)&#xA;    at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)&#xA;    at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)&#xA;    at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)&#xA;    at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)&#xA;    at scala.util.Try$.apply(Try.scala:192)&#xA;    at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)&#xA;    at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:257)&#xA;    at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)&#xA;    at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:257)&#xA;    at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)&#xA;    at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;    at java.lang.Thread.run(Thread.java:748)&#xA;-------------------------------------------&#xA;Time: 1520365424000 ms&#xA;-------------------------------------------&#xA;&#xA;-------------------------------------------&#xA;Time: 1520365426000 ms&#xA;-------------------------------------------&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Do you have any idea?&lt;/p&gt;&#xA;" OwnerUserId="6596760" LastActivityDate="2018-03-06T20:16:24.080" Title="Basic spark-streaming can't read json string from kafka" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;spark-streaming&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49139377" PostTypeId="1" CreationDate="2018-03-06T20:23:42.307" Score="0" ViewCount="7" Body="&lt;p&gt;Are Spark 2.3 Stream-Stream Joins by default a stateful operation? since a row from one data frame has to wait for a matching row in another data frame. Isn't it by default a stateful operation? How long to wait depends on the watermark set by the user I am assuming. so for the period of watermark time, it becomes a stateful operation and what if I don't set a watermark for the inner-join case. Is it still stateful? If so, does it have a default watermark if the user doesn't specify one for inner join case? &lt;/p&gt;&#xA;" OwnerUserId="1870400" LastActivityDate="2018-03-06T20:23:42.307" Title="Are Spark 2.3 Stream-Stream Joins by default a stateful operation?" Tags="&lt;apache-spark&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49139725" PostTypeId="1" CreationDate="2018-03-06T20:50:55.587" Score="0" ViewCount="20" Body="&lt;p&gt;I am trying to share a globalTempView registered using spark and make it accessible among different JupyterHub notebooks.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Is there a way I can access the same spark session/context/golbalTempView in another JupyterHub notebook?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;I have tried Livy, but it is fully REST-based and doesn't allow you to share the sparkContext itself. It rather holds it to itself and allows you to run your code over it and get the output as a JSON. This doesn't solve my problem as the object is returned as a string, and moreover I can't still access the golbalTempView or the sparkContext from other notebooks.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;I tried using memcached, but when I try to get the object from another notebook like&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;sc = client.get('sparkContext')&#xA;sqlContext = SQLContext(sc)&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;It throws the error &lt;code&gt;AttributeError: 'bytes' object has no attribute '_jsc'&lt;/code&gt;&lt;/p&gt;&#xA;" OwnerUserId="5102599" LastActivityDate="2018-03-06T20:50:55.587" Title="Sharing a sqlContext or a sparkContext in different JupyterHub notebooks" Tags="&lt;apache-spark&gt;&lt;memcached&gt;&lt;jupyter-notebook&gt;&lt;jupyterhub&gt;&lt;livy&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49141042" PostTypeId="1" CreationDate="2018-03-06T22:27:29.863" Score="0" ViewCount="20" Body="&lt;p&gt;Begineer SparkR and ElasticSearch question here!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How do I write a sparkR dataframe or RDD to ElasticSearch with multiple nodes?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There exists a &lt;a href=&quot;https://github.com/ropensci/elastic/blob/master/vignettes/elastic_intro.Rmd&quot; rel=&quot;nofollow noreferrer&quot;&gt;specific R package for elastic&lt;/a&gt; but says nothing about hadoop or distributed dataframes. When I try to use it I get the following errors:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;install.packages(&quot;elastic&quot;, repos = &quot;http://cran.us.r-project.org&quot;)&#xA;library(elastic)&#xA;df &amp;lt;- read.json('/hadoop/file/location')&#xA;connect(es_port = 9200, es_host = 'https://hostname.dev.company.com', es_user = 'username', es_pwd = 'password')&#xA;docs_bulk(df)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Error: no 'docs_bulk' method for class SparkDataFrame&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;If this were pyspark, I would use the &lt;a href=&quot;https://stackoverflow.com/questions/46762678/how-to-push-a-spark-dataframe-to-elastic-search-pyspark&quot;&gt;&lt;code&gt;rdd.saveAsNewAPIHadoopFile()&lt;/code&gt; function as shown here&lt;/a&gt;, but I can't find any information about it in sparkR from googling. ElasticSearch also has &lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;good documentation, but only for Scala and Java&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm sure there is something obvious I am missing; any guidance appreciated!&lt;/p&gt;&#xA;" OwnerUserId="8075015" LastEditorUserId="8075015" LastEditDate="2018-03-07T14:42:19.680" LastActivityDate="2018-03-07T14:42:19.680" Title="How to read and write to ElasticSearch with SparkR?" Tags="&lt;r&gt;&lt;apache-spark&gt;&lt;elasticsearch&gt;&lt;apache-zeppelin&gt;&lt;sparkr&gt;" AnswerCount="0" CommentCount="4" />
  <row Id="49141270" PostTypeId="1" AcceptedAnswerId="49143385" CreationDate="2018-03-06T22:43:20.730" Score="0" ViewCount="60" Body="&#xA;&#xA;&lt;p&gt;I have the following tuple.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;#                                 x           y        z&#xA;[(('a', 'nexus4', 'stand'), ((-5.958191, 0.6880646, 8.135345), 1))]&#xA;#           part A (key)               part B (value)         count&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As you can see, I have a tuple which is my Key(PART A), I have another tuple which is my Value (PART B) and the number which is my count of different values from my Key Part.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My code for doing this is the following one.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;# Cargo los datos&#xA;lectura = sc.textFile(&quot;asdasd.csv&quot;)&#xA;&#xA;datos = lectura.map(lambda x: ((x.split(&quot;,&quot;)[6], x.split(&quot;,&quot;)[7], x.split(&quot;,&quot;)[9]),(float(x.split(&quot;,&quot;)[3]),float(x.split(&quot;,&quot;)[4]), float(x.split(&quot;,&quot;)[5])))) &#xA;&#xA;meanRDD = (datos.mapValues(lambda x: (x, 1)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Ok, now I want to SUM all the values that have the same KEY, to calculate the MEAN from X column, Y column or Z column.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think I can do it by using reduceByKey, but I'm not applying this function correctly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Example of my code that is not working:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;sum = meanRDD.reduceByKey(lambda x, y: (x[0][0] + y[0][1],x[0][1] + y[1][1], x[0][2] + y[1][2]))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I know after that I have to apply another MapValues function to divide my values by my count part, but the sum isn't working correctly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;example &quot;asdasd.csv&quot; file&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt; Index,Arrival_Time,Creation_Time,x,y,z,User,Model,Device,gt&#xA;0,1424696633908,1424696631913248572,-5.958191,0.6880646,8.135345,a,nexus4,nexus4_1,stand&#xA;1,1424696633909,1424696631918283972,-5.95224,0.6702118,8.136536,a,nexus4,nexus4_1,stand&#xA;2,1424696633918,1424696631923288855,-5.9950867,0.6535491999999999,8.204376,a,nexus4,nexus4_1,stand&#xA;3,1424696633919,1424696631928385290,-5.9427185,0.6761626999999999,8.128204,a,nexus4,nexus4_1,stand&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My key is a tuple (Model, device, gt) my value is (x,y,z)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any idea?&lt;/p&gt;&#xA;" OwnerUserId="6297869" LastEditorUserId="6297869" LastEditDate="2018-03-07T13:58:19.030" LastActivityDate="2018-03-07T15:00:00.783" Title="Sum tuples values to calculate mean - RDD" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;&lt;rdd&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49141676" PostTypeId="1" CreationDate="2018-03-06T23:24:53.387" Score="0" ViewCount="20" Body="&lt;p&gt;I want to use pyspark streaming to count words for files inside &lt;code&gt;/predix/test/&lt;/code&gt; and save output in &lt;code&gt;/predix/output/&lt;/code&gt;. Meanwhile the console print out the word count such as : &lt;code&gt;{hello: 5}&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Below is the code, but console never gives output &lt;code&gt;{hello: 5}&lt;/code&gt; . Can someone point out where my errors are?  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import findspark&#xA;findspark.init()&#xA;&#xA;from pyspark import SparkConf, SparkContext&#xA;from pyspark.streaming import StreamingContext&#xA;&#xA;if __name__ == &quot;__main__&quot;:&#xA;    conf = SparkConf().setMaster(&quot;local[2]&quot;)&#xA;    sc = SparkContext(appName='streamingWordsCount', conf=conf)&#xA;    ssc = StreamingContext(sc, 5)   # batch interval in seconds 5&#xA;    lines = ssc.textFileStream(&quot;/predix/test&quot;)  &#xA;    words = lines.flatMap(lambda line: line.split(&quot; &quot;))&#xA;    pairs = words.map(lambda word: (word, 1))&#xA;    wordCounts = pairs.reduceByKey(lambda x, y: x + y)&#xA;&#xA;    wordCounts.pprint()&#xA;    wordCounts.saveASTextFile(&quot;/predix/output&quot;)&#xA;&#xA;    ssc.start()             # Start the computation&#xA;    ssc.awaitTermination()  # Wait for the computation to terminate&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="5018506" LastEditorUserId="5018506" LastEditDate="2018-03-07T08:30:49.060" LastActivityDate="2018-03-07T08:30:49.060" Title="pyspark streaming word count example no consol output" Tags="&lt;pyspark&gt;&lt;spark-streaming&gt;&lt;word-count&gt;" AnswerCount="0" CommentCount="4" />
  <row Id="49141713" PostTypeId="1" CreationDate="2018-03-06T23:28:43.840" Score="1" ViewCount="30" Body="&lt;p&gt;I am able to write to parquet format and partitioned by a column like so:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;jobname = args['JOB_NAME']&#xA;#header is a spark DataFrame&#xA;header.repartition(1).write.parquet('s3://bucket/aws-glue/{}/header/'.format(jobname), 'append', partitionBy='date')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But I am not able to do this with Glue's DynamicFrame.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;header_tmp = DynamicFrame.fromDF(header, glueContext, &quot;header&quot;)&#xA;glueContext.write_dynamic_frame.from_options(frame = header_tmp, connection_type = &quot;s3&quot;, connection_options = {&quot;path&quot;: 's3://bucket/output/header/'}, format = &quot;parquet&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have tried passing the &lt;code&gt;partitionBy&lt;/code&gt; as a part of &lt;code&gt;connection_options&lt;/code&gt; dict, since AWS docs say for parquet Glue does not support any format options, but that didn't work.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this possible, and how? As for reasons for doing it this way, I thought it was needed for job bookmarking to work, as that is not working for me currently.&lt;/p&gt;&#xA;" OwnerUserId="1459782" LastActivityDate="2018-03-06T23:28:43.840" Title="AWS Glue write parquet with partitions" Tags="&lt;amazon-web-services&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;aws-glue&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49142086" PostTypeId="1" CreationDate="2018-03-07T00:10:33.710" Score="0" ViewCount="18" Body="&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am new to Spark streaming and fairly novice with scala and spark.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;I have a java big data wrapper application that takes data input and generates libsvm and/or csv format data. This app is independent of spark. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;The function I am developing allows the java app to open a socket, connect to a springboot app on a spark master node, instruct the app to open a spark stream, and then stream its data to spark. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Once the data is streamed, the java app shutsdown.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Most of this is working fine, but I am unable to shutdown the spark streaming context, so once the java side has shut down, I get non-stop &lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;ERROR ReceiverTracker: Deregistered receiver for stream 0: Restarting&#xA;  receiver with delay 2000ms: Connection Refused&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;There is an end of file signal that is read by the DStream. I have confirmed that it is received and parsed&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, despite having read the documentation, I am unable to find a way to shut down the StreamingContext programmatically.  Indeed, I am reading online that &lt;code&gt;StreamingContext.stop(true, true)&lt;/code&gt; can lead to problems.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My code is below. Any help would be deeply appreciated.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(NOTE:  &lt;code&gt;logger.info(&quot;Stopping&quot;)&lt;/code&gt; is never logged to file)     &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;var:stop=false;&#xA;&#xA;@throws(classOf[IKodaMLException])&#xA;def  startStream(ip:String,port:Int):Unit=&#xA;{&#xA; try {&#xA;  val ssc = getSparkStreamingContext(fieldVariables)&#xA;  ssc.checkpoint(&quot;./ikoda/cp&quot;)&#xA;&#xA;  val lines = ssc.socketTextStream(ip, port, StorageLevel.MEMORY_AND_DISK_SER)&#xA;  lines.print&#xA;&#xA;  val lmap=lines.map&#xA;  {&#xA;    l =&amp;gt;&#xA;      if(l.contains(&quot;IKODA_END_STREAM&quot;))&#xA;        {&#xA;          stop=true;&#xA;&#xA;        }&#xA;&#xA;      .....do stuff and return processed line&#xA;  }&#xA;&#xA; if(stop)&#xA;    {&#xA;      logger.info(&quot;Stopping&quot;)&#xA;      ssc.stop(true,true)&#xA;    }&#xA;&#xA;&#xA;    lmap.foreachRDD {&#xA;      r =&amp;gt;&#xA;        if(r.count() &amp;gt;0) {&#xA;          .......do more stufff&#xA;        }&#xA;        else&#xA;          {&#xA;            logger.info(&quot;Empty RDD. No data received&quot;)&#xA;          }&#xA;    }&#xA;  ssc.start()&#xA;  ssc.awaitTermination()&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="5082504" LastEditorUserId="5082504" LastEditDate="2018-03-07T19:43:09.240" LastActivityDate="2018-03-09T16:53:53.740" Title="Ephemeral Spark Streaming..Shutdown Programattically" Tags="&lt;apache-spark&gt;&lt;spark-streaming&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49142108" PostTypeId="1" AcceptedAnswerId="49142925" CreationDate="2018-03-07T00:13:09.890" Score="1" ViewCount="47" Body="&lt;p&gt;I ran into &lt;code&gt;Exception in thread &quot;Driver&quot; java.lang.NullPointerException&lt;/code&gt; while running my Spark application, and it was because my code was inside a &lt;code&gt;class&lt;/code&gt; rather than an &lt;code&gt;object&lt;/code&gt;, as suggested &lt;a href=&quot;https://www.hackingnote.com/en/spark/trouble-shooting/Driver-NullPointerException/&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;. However, just wondering &lt;em&gt;why&lt;/em&gt; the main class has to be an &lt;code&gt;object&lt;/code&gt;. Is it possible to use a class instead?&lt;/p&gt;&#xA;" OwnerUserId="1128392" LastActivityDate="2018-03-07T07:17:41.443" Title="Why does the Spark application code need to be an object rather than a class?" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49142334" PostTypeId="1" CreationDate="2018-03-07T00:42:35.730" Score="0" ViewCount="57" Body="&lt;p&gt;I am trying to read an AWS Glue table into pyspark. I get a NullPointerException:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark.sql(&quot;show tables&quot;).show()&#xA;+----------------+-----------------+-----------+&#xA;|        database|        tableName|isTemporary|&#xA;+----------------+-----------------+-----------+&#xA;|test_datalake_db|events2_2017_test|      false|&#xA;|test_datalake_db|      events2_old|      false|&#xA;+----------------+-----------------+-----------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Next, I tried selecting something from the table:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;df = spark.sql(&quot;select * from events2_2017_test&quot;)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;However, things got messy:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;&amp;lt;stdin&amp;gt;&quot;, line 1, in &amp;lt;module&amp;gt;&#xA;  File &quot;/usr/lib/spark/python/pyspark/sql/session.py&quot;, line 603, in sql&#xA;    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)&#xA;  File &quot;/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py&quot;, line 1133, in __call__&#xA;  File &quot;/usr/lib/spark/python/pyspark/sql/utils.py&quot;, line 63, in deco&#xA;    return f(*a, **kw)&#xA;  File &quot;/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py&quot;, line 319, in get_return_value&#xA;py4j.protocol.Py4JJavaError: An error occurred while calling o51.sql.&#xA;: java.lang.NullPointerException: Name is null&#xA;    at java.lang.Enum.valueOf(Enum.java:236)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Also fails with something like:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;myDf = spark.table(&quot;test_datalake_db.events2_2017_test&quot;)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="6138467" LastActivityDate="2018-03-09T16:59:12.160" Title="AWS EMR Spark Glue PySpark -" Tags="&lt;amazon-web-services&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;aws-glue&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49142373" PostTypeId="1" CreationDate="2018-03-07T00:47:06.233" Score="-2" ViewCount="24" Body="&lt;p&gt;I have a DataFrame and in that one column is has comma separated data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For Example : Data looks like this :&#xA;[{value:1}, {value:2, value:3}, {some value}, {somevalue, othervalue}]&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The column is of String datatype. I want to convert it to List and apply some function.&#xA;Now i have a function for doing the conversion of the String column to List &amp;amp; other applied logic. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;But which function will be better &amp;amp; optimized as we have 2 similar sounding functions &lt;strong&gt;mapPartitions&lt;/strong&gt; &amp;amp; &lt;strong&gt;foreachPartitions&lt;/strong&gt;, Does it have exact same performance &amp;amp; in which one to use in what scenario ??&lt;/p&gt;&#xA;" OwnerUserId="4265823" LastActivityDate="2018-03-07T01:15:49.923" Title="What is the Difference between mapPartitions and foreachPartition in Apache Spark" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;&lt;pyspark-sql&gt;" AnswerCount="1" CommentCount="1" FavoriteCount="1" />
  <row Id="49142678" PostTypeId="1" CreationDate="2018-03-07T01:23:16.810" Score="0" ViewCount="18" Body="&lt;p&gt;One can register a function using Scala:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark.udf.register(&quot;uuid&quot;, ()=&amp;gt;java.util.UUID.randomUUID.toString)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now, if I use Java API:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; spark.udf().register(&quot;uuid&quot;, ()=&amp;gt;java.util.UUID.randomUUID().toString());&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The code does not compile. So how can we do this in Java?&lt;/p&gt;&#xA;" OwnerUserId="4115902" LastActivityDate="2018-03-07T13:13:14.043" Title="How to Register a Function without params with Spark SQL Java API" Tags="&lt;apache-spark-sql&gt;&lt;user-defined-functions&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49142745" PostTypeId="1" CreationDate="2018-03-07T01:34:08.860" Score="0" ViewCount="30" Body="&lt;p&gt;I'm trying to use the connector, which I've used a bunch of times in the past super successfully, with the new Spark 2.3 native Kubernetes support and am running into a lot of trouble.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a super simple job that looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;package io.rhom&#xA;&#xA;import org.apache.spark.sql.SparkSession&#xA;import org.apache.spark.sql.cassandra._&#xA;&#xA;import com.datastax.spark.connector.cql.CassandraConnectorConf&#xA;import com.datastax.spark.connector.rdd.ReadConf&#xA;&#xA;/** Computes an approximation to pi */&#xA;object BackupLocations {&#xA;  def main(args: Array[String]) {&#xA;    val spark = SparkSession&#xA;      .builder&#xA;      .appName(&quot;BackupLocations&quot;)&#xA;      .getOrCreate()&#xA;&#xA;    spark.sparkContext.hadoopConfiguration.set(&#xA;      &quot;fs.defaultFS&quot;,&#xA;      &quot;wasb://&amp;lt;snip&amp;gt;&quot;&#xA;    )&#xA;&#xA;    spark.sparkContext.hadoopConfiguration.set(&#xA;      &quot;fs.azure.account.key.rhomlocations.blob.core.windows.net&quot;,&#xA;      &quot;&amp;lt;snip&amp;gt;&quot;&#xA;    )&#xA;&#xA;    val df = spark&#xA;      .read&#xA;      .format(&quot;org.apache.spark.sql.cassandra&quot;)&#xA;      .options(Map( &quot;table&quot; -&amp;gt; &quot;locations&quot;, &quot;keyspace&quot; -&amp;gt; &quot;test&quot;))&#xA;      .load()&#xA;&#xA;    df.write&#xA;      .mode(&quot;overwrite&quot;)&#xA;      .format(&quot;com.databricks.spark.avro&quot;)&#xA;      .save(&quot;wasb://&amp;lt;snip&amp;gt;&quot;)&#xA;&#xA;    spark.stop()&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;which I'm building under SBT with Scala 2.11 and packaging with a Dockerfile that looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;FROM timfpark/spark:20180305&#xA;&#xA;COPY core-site.xml /opt/spark/conf&#xA;&#xA;RUN mkdir -p /opt/spark/jars&#xA;COPY target/scala-2.11/rhom-backup-locations_2.11-0.1.0-SNAPSHOT.jar /opt/spark/jars&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and then executing with:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;bin/spark-submit --master k8s://blue-rhom-io.eastus2.cloudapp.azure.com:443  \&#xA;                             --deploy-mode cluster  \&#xA;                             --name backupLocations \&#xA;                             --class io.rhom.BackupLocations \&#xA;                             --conf spark.executor.instances=2 \&#xA;                             --conf spark.cassandra.connection.host=10.1.0.10 \&#xA;                             --conf spark.kubernetes.container.image=timfpark/rhom-backup-locations:20180306v12 \&#xA;                              --jars https://dl.bintray.com/spark-packages/maven/datastax/spark-cassandra-connector/2.0.3-s_2.11/spark-cassandra-connector-2.0.3-s_2.11.jar,http://central.maven.org/maven2/org/apache/hadoop/hadoop-azure/2.7.2/hadoop-azure-2.7.2.jar,http://central.maven.org/maven2/com/microsoft/azure/azure-storage/3.1.0/azure-storage-3.1.0.jar,http://central.maven.org/maven2/com/databricks/spark-avro_2.11/4.0.0/spark-avro_2.11-4.0.0.jar  \ &#xA;                               local:///opt/spark/jars/rhom-backup-locations_2.11-0.1.0-SNAPSHOT.jar&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;all of this works except for the Cassandra connection piece, which eventually fails with:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;2018-03-07 01:19:38 WARN  TaskSetManager:66 - Lost task 0.0 in stage 0.0 (TID 0, 10.4.0.46, executor 1): org.apache.spark.SparkException: Task failed while writing rows.&#xA;        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)&#xA;        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)&#xA;        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)&#xA;        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)&#xA;        at org.apache.spark.scheduler.Task.run(Task.scala:109)&#xA;        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)&#xA;        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;        at java.lang.Thread.run(Thread.java:748)&#xA;Caused by: java.io.IOException: Exception during preparation of SELECT &quot;user_id&quot;, &quot;timestamp&quot;, &quot;accuracy&quot;, &quot;altitude&quot;, &quot;altitude_accuracy&quot;, &quot;course&quot;, &quot;features&quot;, &quot;latitude&quot;, &quot;longitude&quot;, &quot;source&quot;, &quot;speed&quot; FROM &quot;rhom&quot;.&quot;locations&quot; WHERE token(&quot;user_id&quot;) &amp;gt; ? AND token(&quot;user_id&quot;) &amp;lt;= ?   ALLOW FILTERING: org/apache/spark/sql/catalyst/package$ScalaReflectionLock$&#xA;        at com.datastax.spark.connector.rdd.CassandraTableScanRDD.createStatement(CassandraTableScanRDD.scala:323)&#xA;        at com.datastax.spark.connector.rdd.CassandraTableScanRDD.com$datastax$spark$connector$rdd$CassandraTableScanRDD$$fetchTokenRange(CassandraTableScanRDD.scala:339)&#xA;        at com.datastax.spark.connector.rdd.CassandraTableScanRDD$$anonfun$17.apply(CassandraTableScanRDD.scala:367)&#xA;        at com.datastax.spark.connector.rdd.CassandraTableScanRDD$$anonfun$17.apply(CassandraTableScanRDD.scala:367)&#xA;        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)&#xA;        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)&#xA;        at com.datastax.spark.connector.util.CountingIterator.hasNext(CountingIterator.scala:12)&#xA;        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)&#xA;        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)&#xA;        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)&#xA;        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)&#xA;        at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:380)&#xA;        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)&#xA;        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)&#xA;        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)&#xA;        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)&#xA;        ... 8 more&#xA;Caused by: java.lang.NoClassDefFoundError: org/apache/spark/sql/catalyst/package$ScalaReflectionLock$&#xA;        at org.apache.spark.sql.catalyst.ReflectionLock$.&amp;lt;init&amp;gt;(ReflectionLock.scala:5)&#xA;        at org.apache.spark.sql.catalyst.ReflectionLock$.&amp;lt;clinit&amp;gt;(ReflectionLock.scala)&#xA;        at com.datastax.spark.connector.types.TypeConverter$.&amp;lt;init&amp;gt;(TypeConverter.scala:73)&#xA;        at com.datastax.spark.connector.types.TypeConverter$.&amp;lt;clinit&amp;gt;(TypeConverter.scala)&#xA;        at com.datastax.spark.connector.types.BigIntType$.converterToCassandra(PrimitiveColumnType.scala:50)&#xA;        at com.datastax.spark.connector.types.BigIntType$.converterToCassandra(PrimitiveColumnType.scala:46)&#xA;        at com.datastax.spark.connector.types.ColumnType$.converterToCassandra(ColumnType.scala:231)&#xA;        at com.datastax.spark.connector.rdd.CassandraTableScanRDD$$anonfun$11.apply(CassandraTableScanRDD.scala:312)&#xA;        at com.datastax.spark.connector.rdd.CassandraTableScanRDD$$anonfun$11.apply(CassandraTableScanRDD.scala:312)&#xA;        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#xA;        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#xA;        at scala.collection.Iterator$class.foreach(Iterator.scala:893)&#xA;        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)&#xA;        at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#xA;        at scala.collection.AbstractIterable.foreach(Iterable.scala:54)&#xA;        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#xA;        at scala.collection.AbstractTraversable.map(Traversable.scala:104)&#xA;        at com.datastax.spark.connector.rdd.CassandraTableScanRDD.createStatement(CassandraTableScanRDD.scala:312)&#xA;        ... 23 more&#xA;Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.catalyst.package$ScalaReflectionLock$&#xA;        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)&#xA;        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&#xA;        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)&#xA;        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&#xA;        ... 41 more&#xA;&#xA;2018-03-07 01:19:38 INFO  TaskSetManager:54 - Starting task 0.1 in stage 0.0 (TID 3, 10.4.0.46, executor 1, partition 0, ANY, 9486 bytes)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I've tried every thing I can possibly think of to resolve this - anyone have any ideas?  Is this possibly caused by another unrelated issue?&lt;/p&gt;&#xA;" OwnerUserId="851004" LastActivityDate="2018-03-08T15:33:10.647" Title="Cassandra Connector fails when run under Spark 2.3 on Kubernetes" Tags="&lt;apache-spark&gt;&lt;cassandra&gt;&lt;kubernetes&gt;&lt;spark-cassandra-connector&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49143266" PostTypeId="1" AcceptedAnswerId="49146607" CreationDate="2018-03-07T02:42:44.180" Score="1" ViewCount="30" Body="&lt;p&gt;I need to use the &lt;code&gt;limit&lt;/code&gt; function to get n entries/rows from a dataframe. I do know it's not advisable, but this is meant as a pre-processing step which will not be required when actually implementing the code. However, I've read elsewhere that the resulting dataframe from using the limit function has only 1 partition. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to measure the processing time for my job which should not be limited by this. I actually tried repartitioning but the performance improvement is minimal (if any at all). I checked the partitioning by printing out &lt;code&gt;df.rdd.getNumPartitions()&lt;/code&gt; and it's still &lt;code&gt;1&lt;/code&gt;. Is there someway to force repartitioning to happen?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: Note that the &lt;code&gt;getNumPartitions()&lt;/code&gt; was run after a &lt;code&gt;count&lt;/code&gt; action.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT2: Sample code&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df = random_data.groupBy(&quot;col&quot;).count().sort(F.desc(&quot;count&quot;)).limit(100).repartition(10)&#xA;df.count()&#xA;print(&quot;No. of partitions: {0}&quot;.format(df.rdd.getNumPartitions())) # Prints 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="6092420" LastEditorUserId="6092420" LastEditDate="2018-03-07T07:01:00.623" LastActivityDate="2018-03-07T07:44:45.223" Title="Repartitioning dataframe from Spark limit() function" Tags="&lt;apache-spark&gt;&lt;time&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="49143271" PostTypeId="1" CreationDate="2018-03-07T02:43:35.403" Score="0" ViewCount="26" Body="&lt;p&gt;since updating to Spark 2.3.0, tests which are run in my CI (Semaphore) fail due to a allegedly invalid spark url when creating the (local) spark context:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;18/03/07 03:07:11 ERROR SparkContext: Error initializing SparkContext.&#xA;org.apache.spark.SparkException: Invalid Spark URL: spark://HeartbeatReceiver@LXC_trusty_1802-d57a40eb:44610&#xA;    at org.apache.spark.rpc.RpcEndpointAddress$.apply(RpcEndpointAddress.scala:66)&#xA;    at org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:134)&#xA;    at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)&#xA;    at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:109)&#xA;    at org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:32)&#xA;    at org.apache.spark.executor.Executor.&amp;lt;init&amp;gt;(Executor.scala:155)&#xA;    at org.apache.spark.scheduler.local.LocalEndpoint.&amp;lt;init&amp;gt;(LocalSchedulerBackend.scala:59)&#xA;    at org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:126)&#xA;    at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:164)&#xA;    at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:500)&#xA;    at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2486)&#xA;    at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:930)&#xA;    at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:921)&#xA;    at scala.Option.getOrElse(Option.scala:121)&#xA;    at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The spark session is created as following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val sparkSession: SparkSession = SparkSession&#xA;.builder&#xA;.appName(s&quot;LocalTestSparkSession&quot;)&#xA;.config(&quot;spark.broadcast.compress&quot;, &quot;false&quot;)&#xA;.config(&quot;spark.shuffle.compress&quot;, &quot;false&quot;)&#xA;.config(&quot;spark.shuffle.spill.compress&quot;, &quot;false&quot;)&#xA;.master(&quot;local[3]&quot;)&#xA;.getOrCreate&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Before updating to Spark 2.3.0, no problems were encountered in version 2.2.1 and 2.1.0. Also, running the tests locally works fine.&lt;/p&gt;&#xA;" OwnerUserId="9224959" LastEditorUserId="4420967" LastEditDate="2018-03-07T07:20:06.997" LastActivityDate="2018-03-07T07:20:06.997" Title="Invalid Spark URL in local spark session" Tags="&lt;apache-spark&gt;" AnswerCount="0" CommentCount="5" />
  <row Id="49143277" PostTypeId="1" CreationDate="2018-03-07T02:44:24.833" Score="1" ViewCount="22" Body="&lt;p&gt;I am running a spark application that reads data from a few hive tables(IP addresses) and compares each element(IP address) in a dataset with all other elements(IP addresses) from the other datasets. The end result would be something like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+---------------+--------+---------------+---------------+---------+----------+--------+----------+&#xA;|     ip_address|dataset1|dataset2       |dataset3       |dataset4 |dataset5  |dataset6|      date|&#xA;+---------------+--------+---------------+---------------+---------+----------+--------+----------+&#xA;| xx.xx.xx.xx.xx|     1  |              1|              0|        0|         0|      0 |2017-11-06|&#xA;| xx.xx.xx.xx.xx|     0  |              0|              1|        0|         0|      1 |2017-11-06|&#xA;| xx.xx.xx.xx.xx|     1  |              0|              0|        0|         0|      1 |2017-11-06|&#xA;| xx.xx.xx.xx.xx|     0  |              0|              1|        0|         0|      1 |2017-11-06|&#xA;| xx.xx.xx.xx.xx|     1  |              1|              0|        1|         0|      0 |2017-11-06|&#xA;---------------------------------------------------------------------------------------------------&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For doing the comparison, I am converting the &lt;code&gt;dataframes&lt;/code&gt; resulting from the &lt;code&gt;hiveContext.sql(&quot;query&quot;)&lt;/code&gt; statement into &lt;code&gt;Fastutil&lt;/code&gt; objects. Like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val df= hiveContext.sql(&quot;query&quot;)&#xA;val dfBuffer = new it.unimi.dsi.fastutil.objects.ObjectArrayList[String](df.map(r =&amp;gt; r(0).toString).collect())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then, I am using an &lt;code&gt;iterator&lt;/code&gt; to iterate over each collection and write the rows to a file using &lt;code&gt;FileWriter&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val dfIterator = dfBuffer.iterator()&#xA;while (dfIterator.hasNext){&#xA;     val p = dfIterator.next().toString&#xA;     //logic&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am running the application with &lt;code&gt;--num-executors 20 --executor-memory 16g --executor-cores 5 --driver-memory 20g&lt;/code&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The process runs for about 18-19 hours in total for about 4-5 million records with one to one comparisons on a daily basis. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, when I checked the Application Master UI, I noticed that no activity takes place after the initial conversion of &lt;code&gt;dataframes&lt;/code&gt; to &lt;code&gt;fastutil collection objects&lt;/code&gt; is done (this takes only a few minutes after the job is launched). I see the &lt;code&gt;count&lt;/code&gt; and &lt;code&gt;collect&lt;/code&gt; statements used in the code producing new jobs till the conversion is done. After that, no new jobs are launched when the comparison is running. &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;What does this imply? Does it mean that the distributed processing is&#xA;not happening at all?   &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;I understand that collection objects are not treated as RDDs, could&lt;br&gt;&#xA;this be the reason for this?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;How is spark executing my program without using the resources&#xA;assigned?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Any help would be appreciated, Thank you!&lt;/p&gt;&#xA;" OwnerUserId="8690528" LastEditorUserId="8690528" LastEditDate="2018-03-07T02:56:36.837" LastActivityDate="2018-03-07T02:56:36.837" Title="How does spark handle/work with Java collection Objects" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;dataframe&gt;&lt;collections&gt;&lt;fastutil&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49143529" PostTypeId="1" CreationDate="2018-03-07T03:16:40.520" Score="0" ViewCount="7" Body="&lt;p&gt;​I have HDP 2.6.2 cluster, with Spark 2.1.1 and 1.6.3 versions on it. Recently it had 3 nodes (SLES11 SP3), lately I added 2 more (SLES11 SP4), following the standard procedure in Ambari. Installed roles DataNode, Yarn Nodemanager and Client.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then I launched Spark jobs which I used to run before the cluster expansion. More often than not they failed with the following error message:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;org.apache.spark.shuffle.FetchFailedException: Failed to connect to newhost/10.10.1.x:46402&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;..where newhost is either one new node or another. Port would vary widely. Happens with both Spark versions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then I stopped Yarn Nodemanager on both new nodes and restarted Spark jobs -- they completed successfully.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since I'm able to run those jobs on less nodes, I believe the problem is not about resource allocation to that Spark job. Our IT people say all ports 0-65535 are open between the hosts. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What else I could look at?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;PS Btw I initially meant to post this question on hortonworks community forum, but found that my account there somehow is denied posting. &quot;Contact the site Administrator&quot;, they say, but provide no contact option. Anybody knows how to resolve that?&lt;/p&gt;&#xA;" OwnerUserId="6069587" LastActivityDate="2018-03-07T03:16:40.520" Title="Spark jobs fail on new nodes of expanded HDP cluster" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;yarn&gt;&lt;shuffle&gt;&lt;hortonworks-data-platform&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49143686" PostTypeId="1" CreationDate="2018-03-07T03:35:39.740" Score="0" ViewCount="19" Body="&lt;p&gt;We have a database with 2 tables, users, and images. The users tables has 3 rows: password, username, and primary key. The images table has the image id, image, and the user tag(which equals the user primary key). When we try to upload an image the upload function doesn't return errors but when displaying the image there is an error:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;[qtp2073702330-26] INFO spark.http.matching.MatcherFilter - The requested route&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;[/jpeg/getImagesTen] has not been mapped in Spark for Accept:&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;[image/webp,image/apng,image/&lt;em&gt;,&lt;/em&gt;/*;q=0.8]&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;We know that the error is mentioning the get Images route, but we know it's not the problem as when doing a simple query for everything in cloudsql (in our images table), not a single entry appears. So we know the upload function is flawed somehow, or cloudsql is not allowing our images.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The function we used to upload images looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;public static String uploadImage(spark.Request request) {&#xA;    Context context = getSessionContext(request.session());&#xA;    String username = request.session().attribute(&quot;username&quot;);&#xA;    MultipartConfigElement multipartConfigElement = new MultipartConfigElement(&quot;/tmp&quot;);&#xA;    request.raw().setAttribute(&quot;org.eclipse.jetty.multipartConfig&quot;, multipartConfigElement);&#xA;    int userID = 0;&#xA;    try {&#xA;        System.out.println(username);&#xA;        String getUserID = &quot;SELECT UserID FROM users WHERE Username = '&quot; + username + &quot;';&quot;;&#xA;        PreparedStatement getUserIDStatement = context.db.conn.prepareStatement(getUserID);&#xA;        ResultSet getUserRS = getUserIDStatement.executeQuery();&#xA;        while (getUserRS.next()) {&#xA;            System.out.println(getUserRS.getInt(&quot;UserID&quot;));&#xA;            userID = getUserRS.getInt(&quot;UserID&quot;);&#xA;        }&#xA;        System.out.println(userID);&#xA;        System.out.println(request.raw().getPart(&quot;file&quot;));&#xA;        Part filePart = request.raw().getPart(&quot;file&quot;);&#xA;        InputStream fileContent = filePart.getInputStream();&#xA;        String sql = &quot;INSERT images(Image, UserTag) VALUES (?,?);&quot;;&#xA;        PreparedStatement ps = db.conn.prepareStatement(sql);&#xA;        ps.setBinaryStream(1, fileContent);&#xA;        ps.setInt(2, userID);&#xA;        ps.execute();&#xA;    } catch (Exception e) {&#xA;        System.out.println(e);&#xA;    }&#xA;    return &quot;&amp;lt;!DOCTYPE html&amp;gt;&quot;&#xA;            + &quot;&amp;lt;html&amp;gt;\&quot;&quot;&#xA;            + &quot;&amp;lt;body background=\&quot;https://webgradients.com/public/webgradients_png/019%20Malibu%20Beach.png\&quot;&amp;gt;&quot;&#xA;            + &quot;                &amp;lt;form method=\&quot;get\&quot; action=\&quot;/\&quot;&amp;gt;\n&quot;&#xA;            + &quot;                    &amp;lt;button type=\&quot;submit\&quot;&amp;gt;Go Back&amp;lt;/button&amp;gt;\n&quot;&#xA;            + &quot;                &amp;lt;/form&amp;gt;&quot;&#xA;            + &quot;&amp;lt;/body&amp;gt;&quot;&#xA;            + &quot;&amp;lt;/html&amp;gt;&quot;;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the post route:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; post(&quot;/protected/upload&quot;, (req, res) -&amp;gt; uploadImage(req));&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the HTML form that submits the image:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;form method=&quot;post&quot; action=&quot;/protected/upload&quot; enctype=&quot;multipart/form-data&quot;&amp;gt;&#xA;                    &amp;lt;input type=&quot;file&quot; name=&quot;file&quot;/&amp;gt;&#xA;                    &amp;lt;input type=&quot;submit&quot; /&amp;gt;&#xA;                &amp;lt;/form&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="9408147" LastEditorUserId="9408147" LastEditDate="2018-03-07T03:44:21.873" LastActivityDate="2018-03-07T10:46:33.840" Title="Can't upload images to cloudsql database using SparkJava" Tags="&lt;java&gt;&lt;google-cloud-sql&gt;&lt;spark-java&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49143736" PostTypeId="1" CreationDate="2018-03-07T03:42:17.377" Score="1" ViewCount="18" Body="&lt;p&gt;Looking for some help to implement the similar logic which is in informatica. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;We need to compare some columns in a dataframe to check is there any change in the record(I have to compare around 60 columns out 80 columns in the record) - If we see some difference in the columns, that means its a changed record. Otherwise no change in the record.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We are using Spark and scala for the development and really appreciate if any insights on this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks,&#xA;Vee&lt;/p&gt;&#xA;" OwnerUserId="8884239" LastActivityDate="2018-03-07T04:37:33.627" Title="Informatica equelent column comparision in Spark Scala" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="49144034" PostTypeId="1" CreationDate="2018-03-07T04:17:35.647" Score="-1" ViewCount="16" Body="&lt;p&gt;I want to execute a stored procedure on HiveContext, is it possible?. When I try with following command it throws error:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;hiveContext.sql(&quot;exec SCHEMA.PROC_NAME.FUNCTION(params)&quot;);&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I get below error:&#xA;&lt;code&gt;org.apache.spark.sql.AnalysisException: cannot recognize input near 'exec' 'SCHEMA' '.'; line 1 pos 0&lt;/code&gt;&lt;/p&gt;&#xA;" OwnerUserId="5390096" LastActivityDate="2018-03-07T04:17:35.647" Title="HiveContext execute procedure" Tags="&lt;apache-spark&gt;&lt;stored-procedures&gt;&lt;hive&gt;&lt;apache-spark-sql&gt;&lt;hivecontext&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49144178" PostTypeId="1" CreationDate="2018-03-07T04:32:44.860" Score="0" ViewCount="15" Body="&lt;p&gt;I am new to Spark. I am trying to understand the number of partitions produced by default by a &lt;code&gt;hiveContext.sql(&quot;query&quot;)&lt;/code&gt; statement. I know that we can &lt;code&gt;repartition&lt;/code&gt; the &lt;code&gt;dataframe&lt;/code&gt; after it has been created using &lt;code&gt;df.repartition&lt;/code&gt;. But, what is the number of partitions produced by default when the &lt;code&gt;dataframe&lt;/code&gt; is initially created?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I understand that &lt;code&gt;sc.parallelize&lt;/code&gt; and some other transformations produce the number of partitions according to &lt;code&gt;spark.default.parallelism&lt;/code&gt;. But what about a &lt;code&gt;dataframe&lt;/code&gt; ? I saw some answers saying that the setting &lt;code&gt;spark.sql.shuffle.partitions&lt;/code&gt; produces the set number of partitions while doing shuffle operations like join. Does this give the initial number of partitions when a dataframe is created? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Then I also saw some answers explaining the number of partitions produced by setting &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;mapred.min.split.size.&#xA;mapred.max.split.size and&#xA;hadoop block size&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then when I tried to do it practically, I read 10 million records into a dataframe in a spark-shell launched with 2 executors and 4 cores per executor. When I did &lt;code&gt;df.rdd.getNumPartitions&lt;/code&gt;, I get the value &lt;code&gt;1&lt;/code&gt;. How am I getting 1 for the number of partitions? isn't &lt;code&gt;2&lt;/code&gt; the min number of partitions?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I do a count on the &lt;code&gt;dataframe&lt;/code&gt;, I see that 200 tasks are being launched. IS this due to the &lt;code&gt;spark.sql.shuffle.partitions&lt;/code&gt; setting?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am totally confused! can someone please explain?? Any help would be appreciated. Thank you!&lt;/p&gt;&#xA;" OwnerUserId="8690528" LastActivityDate="2018-03-07T04:32:44.860" Title="hiveContext.sql produces dataframe with one partition" Tags="&lt;apache-spark&gt;&lt;dataframe&gt;&lt;parallel-processing&gt;&lt;partitioning&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49144289" PostTypeId="1" CreationDate="2018-03-07T04:44:00.880" Score="1" ViewCount="29" Body="&lt;p&gt;I am dealing with a weird situation , where I have small tables and big tables to process using spark and it must be a single spark job. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;To achieve best performance targets, I need to set a property called &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark.sql.shuffle.partitions = 12 for small tables and&#xA;spark.sql.shuffle.partitions = 500 for bigger tables&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to know how can I change these properties dynamically in spark ?&#xA;Can I have multiple configuration files and call it within the program ?&lt;/p&gt;&#xA;" OwnerUserId="4651982" LastActivityDate="2018-03-07T07:16:51.763" Title="How to set multiple spark configurations for same spark job" Tags="&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="49144306" PostTypeId="1" AcceptedAnswerId="49145010" CreationDate="2018-03-07T04:45:26.787" Score="-1" ViewCount="47" Body="&lt;p&gt;I have a &lt;code&gt;list&lt;/code&gt; like below in &lt;code&gt;python/pyspark&lt;/code&gt; like below. I want to convert the special characters in the list to something else.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have done like below.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;cols = ['abc test', 'test*abc', 'eng)test', 'abc_&amp;amp;test']&#xA;&#xA;reps = [(' ', '_&amp;amp;'), ('(', '*_'), (')', '_*'), ('{', '#_'), ('}', '_#'), (';', '_##'), ('.', '_$'), (',', '_$$'), ('=', '_**')]&#xA;&#xA;replacedCols = []&#xA;for col in cols:&#xA;    for x in reps:&#xA;        col = col.replace(x[0], x[1])&#xA;    replacedCols.append(col)&#xA;&#xA;checkCols = replacedCols[:]&#xA;for index, col in enumerate(replacedCols):&#xA;    checkCols[index] = ''&#xA;    replacedCols[index]&#xA;    if col in checkCols:&#xA;        replacedCols[index] = col.replace('_', '__')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The new list is like below:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;New_cols = ['abc__&amp;amp;test', 'test*abc', 'eng_*test', 'abc_&amp;amp;test']&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now I want to convert this list back to the original list:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;new_cols = ['abc__&amp;amp;test', 'test*abc', 'eng_*test', 'abc_&amp;amp;test']&#xA;&#xA;reps = (('_&amp;amp;', ' '), ('*_', '('), ('_*', ')'), ('#_', '{'), ('_#', '}'), ('_##', ';'), ('_$', '.'), ('_$$', ','), ('_**', '='))&#xA;&#xA;&#xA;replaced_ColsCols = []&#xA;for col in new_cols:&#xA;    for x in reps:&#xA;        col = col.replace(x[0], x[1])&#xA;    replaced_Cols.append(col)&#xA;&#xA;check_Cols = replaced_Cols[:]&#xA;for index, col in enumerate(replaced_Cols):&#xA;    check_Cols[index] = ''&#xA;    replaced_Cols[index]&#xA;    if col in check_Cols:&#xA;        replaced_Cols[index] = col.replace('__', '_')&#xA;&#xA;print(replaced_Cols)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I got the result like below which is not the same as the original list  &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;old_cols = ['abc_ test', 'test*abc', 'eng)test', 'abc test']&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What do I need to do to get the the desired result    &lt;/p&gt;&#xA;" OwnerUserId="9367133" LastActivityDate="2018-03-07T05:57:06.700" Title="replaced list back to original list in python" Tags="&lt;python&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="7" />
  <row Id="49144434" PostTypeId="1" CreationDate="2018-03-07T04:58:11.293" Score="-1" ViewCount="19" Body="&lt;p&gt;As the title says, our data lives in a managed MySQL database without sharding or clustering. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Are the architectural benefits of Spark (in-memory parallel processing, ability to join across multiple data sets) applicable in this case, or does MySQL become the bottleneck from an IO perspective, making Spark benefits void ?&lt;/p&gt;&#xA;" OwnerUserId="1804802" LastActivityDate="2018-03-07T05:18:59.893" Title="Does it makes sense to use Spark for descriptive analytics with a relational non-sharded data store (e.g. MySQL)?" Tags="&lt;mysql&gt;&lt;apache-spark&gt;&lt;bigdata&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49145313" PostTypeId="1" AcceptedAnswerId="49148494" CreationDate="2018-03-07T06:14:14.380" Score="1" ViewCount="43" Body="&lt;p&gt;I've recently started playing with Spark Sql (2.1) and I'm dealing with nested data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;here is my schema:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; root&#xA; |-- a: string (nullable = true)&#xA; |-- b: map (nullable = true)&#xA; |    |-- bb: string&#xA; |    |-- bbb: string (valueContainsNull = true)&#xA; |-- c: array (nullable = true)&#xA; |    |-- element: struct (containsNull = true)&#xA; |    |    |-- cc: map (nullable = true)&#xA; |    |    |    |-- cca: string&#xA; |    |    |    |-- ccb: struct (valueContainsNull = true)&#xA; |    |    |    |    |-- member0: string (nullable = true)&#xA; |    |    |    |    |-- member1: long (nullable = true)&#xA; |    |    |-- ccc: map (nullable = true)&#xA; |    |    |    |-- ccca: string&#xA; |    |    |    |-- cccb: string (valueContainsNull = true)&#xA; |    |    |-- cccc: map (nullable = true)&#xA; |    |    |    |-- cccca: string&#xA; |    |    |    |-- ccccb: string (valueContainsNull = true)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am trying to filter my data as follows: keep all the rows where c.ccc.key == 'data'&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I found the very relevant function exists in databricks documentation. But I wonder if there is any similar outside of databricks notebooks? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://docs.databricks.com/spark/latest/spark-sql/higher-order-functions-lambda-functions.html#exists-array-t-function-t-v-boolean-boolean&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://docs.databricks.com/spark/latest/spark-sql/higher-order-functions-lambda-functions.html#exists-array-t-function-t-v-boolean-boolean&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am willing to use sql or do it programatically, just not sure how as dataframes are not typed objects. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Reading this email thread &lt;a href=&quot;http://apache-spark-developers-list.1001551.n3.nabble.com/Will-higher-order-functions-in-spark-SQL-be-pushed-upstream-td21703.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://apache-spark-developers-list.1001551.n3.nabble.com/Will-higher-order-functions-in-spark-SQL-be-pushed-upstream-td21703.html&lt;/a&gt; it seems that high order functions from databricks will be available for all soon. But I wonder if there is an intermediate solution anybody can share?&lt;/p&gt;&#xA;" OwnerUserId="8910977" LastEditorUserId="8910977" LastEditDate="2018-03-07T07:07:59.247" LastActivityDate="2018-03-08T02:16:27.210" Title="Working with nested data in Spark Sql / High order functions" Tags="&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;spark-dataframe&gt;" AnswerCount="2" CommentCount="10" />
  <row Id="49145384" PostTypeId="1" CreationDate="2018-03-07T06:18:45.440" Score="0" ViewCount="40" Body="&lt;p&gt;How to write a case class dynamically for varying number of columns, data-type of the columns can be String.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I'm using RDD without any schema but I would like to use dataframes which needs schema.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Code Im using to fetch Headers from Databases&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  var options = Map(&#xA;    &quot;url&quot; -&amp;gt; url,&#xA;    &quot;driver&quot; -&amp;gt; databaseRead.split(&quot;,&quot;)(4),&#xA;    &quot;dbtable&quot; -&amp;gt; databaseRead.split(&quot;,&quot;)(2),&#xA;    &quot;user&quot; -&amp;gt; databaseRead.split(&quot;,&quot;)(5),&#xA;    &quot;password&quot; -&amp;gt; databaseRead.split(&quot;,&quot;)(6))&#xA;&#xA;&#xA; val fieldname = spark.read.options(options).jdbc(options(&quot;url&quot;), options(&quot;dbtable&quot;), new java.util.Properties()).schema.fieldNames.toArray&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="8199134" LastEditorUserId="8199134" LastEditDate="2018-03-07T07:06:37.957" LastActivityDate="2018-03-07T07:06:37.957" Title="How to write case class for varying no of cloumns for DataFrames" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;apache-spark-sql&gt;" AnswerCount="0" CommentCount="5" />
  <row Id="49146357" PostTypeId="1" CreationDate="2018-03-07T07:27:53.517" Score="0" ViewCount="51" Body="&lt;p&gt;Using spark streaming to read from kafka  messages where V = Json event.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The Json's schema isn't enforced, so you might fetch the following 2 events:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{&quot;foo&quot;: &quot;01/01/1973&quot;,&quot;bar&quot;: &quot;d,e&quot;}&#xA;{&quot;foo&quot;: &quot;01/01/1974&quot;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;using &lt;code&gt;read.json&lt;/code&gt; and &lt;code&gt;df.registerTempTable(&quot;tempTable&quot;)&lt;/code&gt;&#xA;How do I create a &lt;code&gt;query = &quot;select foo, bar from tempTable&quot;&lt;/code&gt; that plugs a NULL in case the field doesn't exist?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;*Using spark 1.6, but would be also happy to hear about spark2&lt;/p&gt;&#xA;" OwnerUserId="2878295" LastEditorUserId="2878295" LastEditDate="2018-03-07T07:36:59.310" LastActivityDate="2018-03-07T12:08:45.040" Title="How to handle missing columns when processing flexible-schema json in scala spark?" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;apache-kafka&gt;&lt;spark-streaming&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="49146426" PostTypeId="1" AcceptedAnswerId="49152624" CreationDate="2018-03-07T07:32:04.573" Score="0" ViewCount="29" Body="&lt;p&gt;It is unclear from documentation where exactly does the lambda given to the foreachpartition run- on the driver or on the worker?&lt;/p&gt;&#xA;" OwnerUserId="180650" LastEditorUserId="180650" LastEditDate="2018-03-07T14:04:24.243" LastActivityDate="2018-03-07T14:40:57.297" Title="Does sparks foreachPartition run on the driver or on the worker?" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="49146451" PostTypeId="1" CreationDate="2018-03-07T07:34:24.417" Score="-3" ViewCount="14" Body="&lt;p&gt;I am using the following template for recommendation:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/apache/predictionio-template-java-ecom-recommender&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/apache/predictionio-template-java-ecom-recommender&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I do not want to use custom persisted model. I want model storing to be handled by PredictionIO.&#xA;How do I change this file to implement the same:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://github.com/apache/predictionio-template-java-ecom-recommender/blob/master/src/main/java/org/example/recommendation/Model.java&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/apache/predictionio-template-java-ecom-recommender/blob/master/src/main/java/org/example/recommendation/Model.java&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="5577496" LastActivityDate="2018-03-07T07:34:24.417" Title="Avoid using custom persisted model in PredictionIO" Tags="&lt;java&gt;&lt;apache-spark&gt;&lt;machine-learning&gt;&lt;model&gt;&lt;predictionio&gt;" AnswerCount="0" CommentCount="0" ClosedDate="2018-03-07T15:39:36.473" />
  <row Id="49147751" PostTypeId="1" CreationDate="2018-03-07T08:58:59.467" Score="0" ViewCount="17" Body="&lt;pre&gt;&lt;code&gt;I have a spark version 2.2.1 and scala version 2.11.8&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;streaming for text stored in kafka is working fine. I'm storing avro encoded message in kafka via php. My aim is to receive and decode the avro message stored in a kafka topic in spark scala. My scala code used for streaming is below. I want to add the decoding avro message part between readstream and writestream. Any help will ve appreciated.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import org.apache.spark.sql.functions._&#xA;import org.apache.spark.sql.SparkSession&#xA;import org.apache.spark.sql._&#xA;import org.apache.spark.sql.catalyst.encoders._&#xA;import org.apache.spark.sql.types._&#xA;import org.apache.spark.sql.streaming.StreamingQuery&#xA;import org.apache.spark.sql.SparkSession&#xA;import org.apache.spark.sql.{Dataset, SparkSession}&#xA;import org.apache.spark.sql.Row&#xA;import org.apache.spark.sql.streaming.StreamingQueryException&#xA;import org.apache.spark.sql.streaming.StreamingQueryListener &#xA;import org.apache.spark.streaming&#xA;import org.apache.spark.streaming.kafka010._&#xA;import com.databricks.spark.avro._&#xA;import org.apache.avro.io.DecoderFactory&#xA;import org.apache.avro.Schema&#xA;import org.apache.avro.generic.{GenericDatumReader, GenericRecord}&#xA;&#xA;&#xA;object Main {&#xA;def main(args: Array[String]) {&#xA;&#xA;var bootstrapServers: String = &#xA;&quot;kafka1.sozcu.com.tr:6667,kafka2.sozcu.com.tr:6667&quot;;&#xA;var subscribeType: String = &quot;subscribe&quot;;&#xA;var topics: String =  &quot;test_topic_324&quot;&#xA;&#xA;val spark = SparkSession&#xA;  .builder()&#xA;  .appName(&quot;Kafka test&quot;)&#xA;  .getOrCreate();&#xA;&#xA;import spark.implicits._&#xA;&#xA;&#xA;var schema = new StructType().add(&quot;id&quot;, StringType) .add(&quot;name&quot;, &#xA;StringType).add(&quot;lastname&quot;, StringType).add(&quot;wifesname&quot;, StringType)&#xA;&#xA;val lines = spark&#xA;  .readStream&#xA;  .format(&quot;kafka&quot;)&#xA;  .option(&quot;kafka.bootstrap.servers&quot;, bootstrapServers)&#xA;  .option(subscribeType, topics)&#xA;  .option(&quot;failOnDataLoss&quot;, false)&#xA;  .option(&quot;startingOffsets&quot;, &quot;earliest&quot;) //console&#xA;  .load()&#xA;.select (&#xA;      json_tuple(col(&quot;value&quot;).cast(&quot;string&quot;),&quot;data&quot; ).as(&quot;data&quot;)&#xA;)&#xA;lines.writeStream&#xA;  .outputMode(&quot;append&quot;)&#xA;  .format(&quot;console&quot;)&#xA;  //      .option(&quot;checkpointLocation&quot;, &quot;/tmp/&quot; + writeStreamFolder + &#xA;&quot;/checkpointLocation&quot;)&#xA;  //      .option(&quot;path&quot;, &quot;/tmp/&quot; + writeStreamFolder + &quot;/streams/&quot; )&#xA;  //      .partitionBy(&quot;partition_date&quot;)&#xA;  .start()&#xA;  .awaitTermination()&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="8489016" LastActivityDate="2018-03-07T08:58:59.467" Title="Receiving AVRO Messages through KAFKA in a Spark Streaming Scala Application" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;streaming&gt;&lt;decode&gt;&lt;avro&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49147975" PostTypeId="1" CreationDate="2018-03-07T09:10:24.840" Score="0" ViewCount="21" Body="&lt;p&gt;Am I right in saying that &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the executor memory is the amount of memory allocated to each executor &lt;/li&gt;&#xA;&lt;li&gt;no. of executors is the number of executors allocation, and  &lt;/li&gt;&#xA;&lt;li&gt;no. of executor cores is basically the number of concurrent tasks that can&#xA;be run on one executor&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;If so, why is there a difference in performance when say I use 1G of executor memory, 2 executors, 1 core each v.s. 2G of executor memory, 1 executor, 2 cores each?&lt;/p&gt;&#xA;" OwnerUserId="6092420" LastEditorUserId="6092420" LastEditDate="2018-03-08T02:42:07.430" LastActivityDate="2018-03-08T02:42:07.430" Title="Spark Configuration: memory v.s. executors v.s. cores" Tags="&lt;apache-spark&gt;&lt;configuration&gt;&lt;yarn&gt;" AnswerCount="0" CommentCount="0" ClosedDate="2018-03-07T10:34:32.480" />
  <row Id="49148645" PostTypeId="1" CreationDate="2018-03-07T09:42:58.743" Score="2" ViewCount="56" Body="&lt;p&gt;I'm trying to use &lt;a href=&quot;https://hadoop.apache.org/docs/r2.8.2/hadoop-yarn/hadoop-yarn-site/NodeLabel.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;YARN node labels&lt;/a&gt; to tag worker nodes, but when I run applications on YARN (Spark or simple YARN app), those applications cannot start.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;with Spark, when specifying &lt;code&gt;--conf spark.yarn.am.nodeLabelExpression=&quot;my-label&quot;&lt;/code&gt;, the job cannot start (blocked on &lt;code&gt;Submitted application [...]&lt;/code&gt;, see details below).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;with a YARN application (like &lt;code&gt;distributedshell&lt;/code&gt;), when specifying &lt;code&gt;-node_label_expression my-label&lt;/code&gt;, the application cannot start neither&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Here are the tests I have made so far.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;YARN node labels setup&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;I'm using &lt;a href=&quot;https://cloud.google.com/dataproc/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Google Dataproc&lt;/a&gt; to run my cluster (example : 4 workers, 2 on &lt;a href=&quot;https://cloud.google.com/dataproc/docs/concepts/compute/preemptible-vms&quot; rel=&quot;nofollow noreferrer&quot;&gt;preemptible nodes&lt;/a&gt;). My goal is to force any YARN application master to run on a &lt;strong&gt;non-preemptible node&lt;/strong&gt;, otherwise the node can be shutdown at any time, thus making the application fail hard.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm creating the cluster using YARN properties (&lt;code&gt;--properties&lt;/code&gt;) to enable node labels :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;gcloud dataproc clusters create \&#xA;    my-dataproc-cluster \&#xA;    --project [PROJECT_ID] \&#xA;    --zone [ZONE] \&#xA;    --master-machine-type n1-standard-1 \&#xA;    --master-boot-disk-size 10 \&#xA;    --num-workers 2 \&#xA;    --worker-machine-type n1-standard-1 \&#xA;    --worker-boot-disk-size 10 \&#xA;    --num-preemptible-workers 2 \&#xA;    --properties 'yarn:yarn.node-labels.enabled=true,yarn:yarn.node-labels.fs-store.root-dir=/system/yarn/node-labels'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Versions of packaged Hadoop and Spark :&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Hadoop version : 2.8.2&lt;/li&gt;&#xA;&lt;li&gt;Spark version : 2.2.0&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;After that, I create a label (&lt;code&gt;my-label&lt;/code&gt;), and update the two non-preemptible workers with this label :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;yarn rmadmin -addToClusterNodeLabels &quot;my-label(exclusive=false)&quot;&#xA;yarn rmadmin -replaceLabelsOnNode &quot;\&#xA;    [WORKER_0_NAME].c.[PROJECT_ID].internal=my-label \&#xA;    [WORKER_1_NAME].c.[PROJECT_ID].internal=my-label&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I can see the created label in YARN Web UI :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/pXwnj.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/pXwnj.png&quot; alt=&quot;Label created on YARN&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;Spark&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;When I run a simple example (&lt;code&gt;SparkPi&lt;/code&gt;) without specifying info about node labels :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark-submit \&#xA;  --class org.apache.spark.examples.SparkPi \&#xA;  --master yarn \&#xA;  --deploy-mode client \&#xA;  /usr/lib/spark/examples/jars/spark-examples.jar \&#xA;  10&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In the Scheduler tab on YARN Web UI, I see the application launched on &lt;code&gt;&amp;lt;DEFAULT_PARTITION&amp;gt;.root.default&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But when I run the job specifying &lt;code&gt;spark.yarn.am.nodeLabelExpression&lt;/code&gt; to set the location of the Spark application master :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark-submit \&#xA;    --class org.apache.spark.examples.SparkPi \&#xA;    --master yarn \&#xA;    --deploy-mode client \&#xA;    --conf spark.yarn.am.nodeLabelExpression=&quot;my-label&quot; \&#xA;    /usr/lib/spark/examples/jars/spark-examples.jar \&#xA;    10&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The job is not launched. From YARN Web UI, I see :&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;YarnApplicationState&lt;/strong&gt;: &lt;code&gt;ACCEPTED: waiting for AM container to be allocated, launched and register with RM.&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Diagnostics&lt;/strong&gt;:  &lt;code&gt;Application is Activated, waiting for resources to be assigned for AM. Details : AM Partition = my-label ; Partition Resource = &amp;lt;memory:6144, vCores:2&amp;gt; ; Queue's Absolute capacity = 0.0 % ; Queue's Absolute used capacity = 0.0 % ; Queue's Absolute max capacity = 0.0 % ;&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;I suspect that the queue related to the label partition (not &lt;code&gt;&amp;lt;DEFAULT_PARTITION&lt;/code&gt;, the other one) does not have sufficient resources to run the job :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/fmIIH.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/fmIIH.png&quot; alt=&quot;Spark job accepted&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here, &lt;code&gt;Used Application Master Resources&lt;/code&gt; is &lt;code&gt;&amp;lt;memory:1024, vCores:1&amp;gt;&lt;/code&gt;, but the &lt;code&gt;Max Application Master Resources&lt;/code&gt; is &lt;code&gt;&amp;lt;memory:0, vCores:0&amp;gt;&lt;/code&gt;. That explains why the application cannot start, but I can't figure out how to change this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried to update different parameters, but without success :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;yarn.scheduler.capacity.root.default.accessible-node-labels=my-label&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Or increasing those properties :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;yarn.scheduler.capacity.root.default.accessible-node-labels.my-label.capacity&#xA;yarn.scheduler.capacity.root.default.accessible-node-labels.my-label.maximum-capacity&#xA;yarn.scheduler.capacity.root.default.accessible-node-labels.my-label.maximum-am-resource-percent&#xA;yarn.scheduler.capacity.root.default.accessible-node-labels.my-label.user-limit-factor&#xA;yarn.scheduler.capacity.root.default.accessible-node-labels.my-label.minimum-user-limit-percent&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;without success neither.&lt;/p&gt;&#xA;&#xA;&lt;h2&gt;YARN Application&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;The issue is the same when running a YARN application :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;hadoop jar \&#xA;    /usr/lib/hadoop-yarn/hadoop-yarn-applications-distributedshell.jar \&#xA;    -shell_command &quot;echo ok&quot; \&#xA;    -jar /usr/lib/hadoop-yarn/hadoop-yarn-applications-distributedshell.jar \&#xA;    -queue default \&#xA;    -node_label_expression my-label&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The application cannot start, and the logs keeps repeating :&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;INFO distributedshell.Client: Got application report from ASM for, appId=6, clientToAMToken=null, appDiagnostics= Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = my-label ; Partition Resource = &amp;lt;memory:6144, vCores:2&amp;gt; ; Queue's Absolute capacity = 0.0 % ; Queue's Absolute used capacity = 0.0 % ; Queue's Absolute max capacity = 0.0 % ; , appMasterHost=N/A, appQueue=default, appMasterRpcPort=-1, appStartTime=1520354045946, yarnAppState=ACCEPTED, distributedFinalState=UNDEFINED, [...]&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I don't specify &lt;code&gt;-node_label_expression my-label&lt;/code&gt;, the application start on &lt;code&gt;&amp;lt;DEFAULT_PARTITION&amp;gt;.root.default&lt;/code&gt; and succeed.&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Questions&lt;/h1&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Am I doing something wrong with the labels? However, I followed the &lt;a href=&quot;https://hadoop.apache.org/docs/r2.8.2/hadoop-yarn/hadoop-yarn-site/NodeLabel.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;official documentation&lt;/a&gt; and &lt;a href=&quot;https://developer.ibm.com/hadoop/2017/03/10/yarn-node-labels/&quot; rel=&quot;nofollow noreferrer&quot;&gt;this guide&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Is this a specific problem related to Dataproc? Because the previous guides seems to work on other environments&lt;/li&gt;&#xA;&lt;li&gt;Maybe I need to create a specific queue and associate it with my label? But since I'm running a &quot;one-shot&quot; cluster to run a single Spark job I don't need to have specific queues, running jobs on the default root one is not a problem for my use-case&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Thanks for helping&lt;/p&gt;&#xA;" OwnerUserId="7804058" LastActivityDate="2018-03-07T09:42:58.743" Title="YARN applications cannot start when specifying YARN node labels" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;yarn&gt;&lt;google-cloud-dataproc&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49148647" PostTypeId="1" AcceptedAnswerId="49149079" CreationDate="2018-03-07T09:43:05.467" Score="0" ViewCount="13" Body="&lt;p&gt;How can I access value at a certain index of a column in PySpark dataframe for example I want to access value at index 5 of a column named &quot;Category&quot;. How can I do that in PySpark syntax?&lt;/p&gt;&#xA;" OwnerUserId="1818286" LastActivityDate="2018-03-07T10:04:01.870" Title="Access a specific item in PySpark dataframe" Tags="&lt;python&gt;&lt;dataframe&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="49149359" PostTypeId="1" CreationDate="2018-03-07T10:16:34.933" Score="0" ViewCount="16" Body="&lt;p&gt;There is a method which takes acceptType(Content-Type) but accepts any content type when &quot;application/json&quot; is passed.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;public static void post(String path, String acceptType, Route route)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="5147110" LastActivityDate="2018-03-07T10:16:34.933" Title="How to enforce api to accept json in spark java" Tags="&lt;java&gt;&lt;rest&gt;&lt;spark-java&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49149663" PostTypeId="1" CreationDate="2018-03-07T10:31:10.833" Score="3" ViewCount="30" Body="&lt;p&gt;I am trying to build a sessionization application with &lt;code&gt;Spark Structured Streaming(version 2.2.0)&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In case of using &lt;code&gt;mapGroupWithState&lt;/code&gt; with Update mode, I understand that the executor will crash with an OOM exception if the state data grows large. Hence, I have to manage the memory with &lt;code&gt;GroupStateTimeout&lt;/code&gt; option.&#xA;(Ref. &lt;a href=&quot;https://stackoverflow.com/questions/45828371/how-does-spark-structured-streaming-handle-in-memory-state-when-state-data-is-gr&quot;&gt;How does Spark Structured Streaming handle in-memory state when state data is growing?&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I can't check if the state is timed-out and ready to be removed if there is no more new streaming data for the particular keys.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example, let's say I have the following code.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;myDataset&#xA;  .groupByKey(_.key)&#xA;  .flatMapGroupsWithState(OutputMode.Update, GroupStateTimeout.EventTimeTimeout)(makeSession)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;makeSession() function will check if the state is timed-out and remove the timed-out state.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, let's say the key &quot;foo&quot; has some stored state in memory already, and no new data with the key &quot;foo&quot; is streaming into the application. As a result, makeSession() does not process the data with key &quot;foo&quot; and the stored state is not being checked. Which means, the stored state with key &quot;foo&quot; persists in memory. If there are many keys like &quot;foo&quot;, the stored states will not be flushed and JVM will raise OOM exception.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I might be misunderstanding with &lt;code&gt;mapGroupWithState&lt;/code&gt;, but I suspect my OOM exception is caused by the above issue.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I am correct, what would be the solution for this case?&#xA;I want to flush all the stored states that has been timedout and have no more new streaming data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any good code example?&lt;/p&gt;&#xA;" OwnerUserId="511375" LastActivityDate="2018-03-08T06:03:06.763" Title="How does Spark Structured Streaming flush in-memory state when state data is no longer being checked?" Tags="&lt;apache-spark&gt;&lt;spark-streaming&gt;&lt;structured-streaming&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49149996" PostTypeId="1" CreationDate="2018-03-07T10:46:04.607" Score="2" ViewCount="26" Body="&lt;p&gt;I'd like to tell in advance that several related questions, like the following, DO NOT address my problems:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/31693723&quot;&gt;Spark query running very slow&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/42696455&quot;&gt;Converting mysql table to dataset is very slow...&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/48141664&quot;&gt;Spark Will Not Load Large MySql Table&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/48126271&quot;&gt;Spark MySQL Error while Reading from Database&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/35603351&quot;&gt;This one&lt;/a&gt; comes close but the stack-trace is different and it is unresolved anyways. So rest assured that I'm posting this question after several days of (&lt;em&gt;failed&lt;/em&gt;) solution-hunting.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;I'm trying to write a job that moves data (once a day) from &lt;code&gt;MySQL&lt;/code&gt; tables to &lt;code&gt;Hive&lt;/code&gt; tables stored as &lt;code&gt;Parquet&lt;/code&gt; / &lt;code&gt;ORC&lt;/code&gt; files on &lt;code&gt;Amazon S3&lt;/code&gt;. Some of the tables are quite big: &lt;strong&gt;~ 300M records&lt;/strong&gt; with &lt;strong&gt;200 GB+ size&lt;/strong&gt; (as reported by &lt;code&gt;phpMyAdmin&lt;/code&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently we are using &lt;code&gt;sqoop&lt;/code&gt; for this but we want to move to &lt;code&gt;Spark&lt;/code&gt; for the following reasons:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;To leverage it's capabilities with &lt;code&gt;DataFrame API&lt;/code&gt; (in future, we would be performing &lt;em&gt;transformations&lt;/em&gt; while moving data)&lt;/li&gt;&#xA;&lt;li&gt;We already have a &lt;em&gt;sizeable&lt;/em&gt; framework written in &lt;code&gt;Scala&lt;/code&gt; for &lt;code&gt;Spark&lt;/code&gt; jobs used elsewhere in the organization&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;I've been able to achieve this on &lt;em&gt;small&lt;/em&gt; &lt;code&gt;MySQL&lt;/code&gt; tables without any issue. But the &lt;code&gt;Spark&lt;/code&gt; job (that reads data from &lt;code&gt;MySQL&lt;/code&gt; into &lt;code&gt;DataFrame&lt;/code&gt;) fails if I try to fetch more than &lt;strong&gt;~1.5-2M records&lt;/strong&gt; at a time. I've shown the &lt;em&gt;relevant portions&lt;/em&gt; of stack-trace below, you can find the complete stack-trace &lt;a href=&quot;https://drive.google.com/file/d/1SvO_iTp-rsuogYYKsM9H3bY077VXQIhp/view?usp=sharing&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;...&#xA;javax.servlet.ServletException: java.util.NoSuchElementException: None.get&#xA;    at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:489)&#xA;    at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:427)&#xA;...&#xA;Caused by: java.util.NoSuchElementException: None.get&#xA;    at scala.None$.get(Option.scala:347)&#xA;    at scala.None$.get(Option.scala:345)&#xA;...&#xA;org.apache.spark.status.api.v1.OneStageResource.taskSummary(OneStageResource.scala:62)&#xA;    at sun.reflect.GeneratedMethodAccessor188.invoke(Unknown Source)&#xA;    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#xA;    at java.lang.reflect.Method.invoke(Method.java:498)&#xA;...&#xA;[Stage 27:&amp;gt;                                                       (0 + 30) / 32]18/03/01 01:29:09 WARN TaskSetManager: Lost task 3.0 in stage 27.0 (TID 92, ip-xxx-xx-xx-xxx.ap-southeast-1.compute.internal, executor 6): java.sql.SQLException: Incorrect key file for table '/rdsdbdata/tmp/#sql_14ae_5.MYI'; try to repair it&#xA;    at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:964)&#xA;    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973)&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;** &lt;em&gt;This stack-trace was obtained upon failure of moving a &lt;strong&gt;148 GB&lt;/strong&gt; table containing &lt;strong&gt;186M&lt;/strong&gt; records&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As apparent from (full) stack-trace, the &lt;code&gt;Spark&lt;/code&gt; read job starts &lt;em&gt;sulking&lt;/em&gt; with the &lt;a href=&quot;https://stackoverflow.com/questions/47635510&quot;&gt;false warnings&lt;/a&gt; of &lt;code&gt;None.get&lt;/code&gt; error followed by &lt;code&gt;SQLException: Incorrect key for file..&lt;/code&gt; (which is related to &lt;code&gt;MySQL&lt;/code&gt;'s &lt;a href=&quot;https://stackoverflow.com/questions/2090073&quot;&gt;&lt;em&gt;tmp table&lt;/em&gt; becoming full&lt;/a&gt;)&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Now clearly this can't be a &lt;code&gt;MySQL&lt;/code&gt; problem because in that case &lt;code&gt;sqoop&lt;/code&gt; should fail as well. As far as &lt;code&gt;Spark&lt;/code&gt; is concerned, I'm &lt;a href=&quot;https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader@jdbc(url:String,table:String,columnName:String,lowerBound:Long,upperBound:Long,numPartitions:Int,connectionProperties:java.util.Properties):org.apache.spark.sql.DataFrame&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;em&gt;parallelizing&lt;/em&gt; the read operation&lt;/a&gt; by setting &lt;code&gt;numPartitions = 32&lt;/code&gt; (we use parallelism of 40 with &lt;code&gt;sqoop&lt;/code&gt;).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;From my &lt;em&gt;limited knowledge&lt;/em&gt; of &lt;code&gt;Spark&lt;/code&gt; and &lt;code&gt;BigData&lt;/code&gt;, &lt;strong&gt;148 GB&lt;/strong&gt; shouldn't be &lt;strong&gt;overwhelming&lt;/strong&gt; for Spark by any measure. Moreover since &lt;code&gt;MySQL&lt;/code&gt;, &lt;code&gt;Spark&lt;/code&gt; (&lt;code&gt;EMR&lt;/code&gt;) and &lt;code&gt;S3&lt;/code&gt; all reside in same &lt;em&gt;region&lt;/em&gt; (&lt;code&gt;AWS&lt;/code&gt; &lt;code&gt;AP-SouthEast&lt;/code&gt;), so &lt;strong&gt;latency&lt;/strong&gt; shouldn't be the &lt;strong&gt;bottleneck&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;My questions are:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Is &lt;code&gt;Spark&lt;/code&gt; a suitable tool for this?&lt;/li&gt;&#xA;&lt;li&gt;Could &lt;code&gt;Spark&lt;/code&gt;'s &lt;code&gt;Jdbc&lt;/code&gt; &lt;em&gt;driver&lt;/em&gt; be blamed for this issue?&lt;/li&gt;&#xA;&lt;li&gt;If answer to above question is&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Yes: How can I overcome it? (alternate driver, or some other workaround)?&lt;/li&gt;&#xA;&lt;li&gt;No: What could be the possible cause?&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;Framework&lt;/em&gt; Configurations:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;Hadoop&lt;/code&gt; distribution: &lt;strong&gt;Amazon 2.8.3&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;Spark&lt;/code&gt; &lt;strong&gt;2.2.1&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;Hive&lt;/code&gt; &lt;strong&gt;2.3.2&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;Scala&lt;/code&gt; &lt;strong&gt;2.11.11&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;EMR&lt;/code&gt; Configurations:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;code&gt;EMR&lt;/code&gt; &lt;strong&gt;5.12.0&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;1 Master&lt;/code&gt;: &lt;strong&gt;r3.xlarge&lt;/strong&gt; [8 vCore, 30.5 GiB memory, 80 SSD GB storage EBS Storage:32 GiB]&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;1 Task&lt;/code&gt;: &lt;strong&gt;r3.xlarge&lt;/strong&gt; [8 vCore, 30.5 GiB memory, 80 SSD GB storage EBS Storage:none]&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;1 Core&lt;/code&gt;: &lt;strong&gt;r3.xlarge&lt;/strong&gt; [8 vCore, 30.5 GiB memory, 80 SSD GB storage&#xA;EBS Storage:32 GiB]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;** &lt;em&gt;These are the configurations of development cluster; production cluster would be better equipped&lt;/em&gt;&lt;/p&gt;&#xA;" OwnerUserId="3679900" LastEditorUserId="3679900" LastEditDate="2018-03-07T11:02:47.490" LastActivityDate="2018-03-07T11:02:47.490" Title="Spark: Reading big MySQL table into DataFrame fails" Tags="&lt;mysql&gt;&lt;apache-spark&gt;" AnswerCount="0" CommentCount="4" />
  <row Id="49150987" PostTypeId="1" CreationDate="2018-03-07T11:38:03.987" Score="0" ViewCount="6" Body="&lt;p&gt;6.1 to run a batch job which uses GraphX. As I increase my load (5k vertices and 50k edges) it starts throwing &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;ERROR LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;within 3-4 minutes.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;but the execution continues and get stuck with a stage after the spark context is stopped and eventually throws a java heap space error.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The application loads the vertices and edges from an external file and forms the graph, does some operations and finds the Kcore value of the graph. The error is thrown in 2-3 minutes into the Kcore computation. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have gone through multiple materials like &lt;a href=&quot;https://stackoverflow.com/questions/32340639/unable-to-understand-error-sparklistenerbus-has-already-stopped-dropping-event&quot;&gt;this&lt;/a&gt; to find the cause, but was unsuccessful.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What causes this error and how can I rectify this?&lt;/p&gt;&#xA;" OwnerUserId="2368687" LastActivityDate="2018-03-07T11:38:03.987" Title="LiveListenerBus error in Spark" Tags="&lt;apache-spark&gt;&lt;spark-graphx&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49151248" PostTypeId="1" CreationDate="2018-03-07T11:50:32.063" Score="-1" ViewCount="16" Body="&lt;p&gt;I'm working on an application which uses Apache Spark for data transformation. I want to implement a schema validation functionality on top of it an in order to do so I have to check each value in each column.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I had in mind is to just do a &lt;code&gt;foreach&lt;/code&gt; and check the values in the &lt;code&gt;Row&lt;/code&gt; one by one like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val myStatObject = MyStatObject()&#xA;datasource.foreach { row -&amp;gt;&#xA;    myStatObject.gatherStats(row)   &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;MyStatObject&lt;/code&gt; will validate each value in a &lt;code&gt;Row&lt;/code&gt; (for example it can check whether a value is a proper &lt;code&gt;URL&lt;/code&gt; or not).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a better way to do so? Is there some built-in functionality in Spark for this?&lt;/p&gt;&#xA;" OwnerUserId="9316458" LastActivityDate="2018-03-07T11:50:32.063" Title="What is the fastest way to gather statistics about Apache Spark Dataset columns?" Tags="&lt;validation&gt;&lt;apache-spark&gt;" AnswerCount="0" CommentCount="1" ClosedDate="2018-03-07T18:25:55.720" />
  <row Id="49151525" PostTypeId="1" CreationDate="2018-03-07T12:05:22.440" Score="0" ViewCount="11" Body="&lt;p&gt;I'm trying to launch a spark job with sparklauncher and i'm getting this error, there is no new job in yarn -list or UI and no record in yarn log.But the job can be launched with launcher() and finished with success.I try to use &lt;code&gt;SPARK_PRINT_LAUNCH_COMMAND&lt;/code&gt; to find out the command that is used by sparklauncher and run it directly, it works successfully and i get the results.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is the info that sparklaunch shows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;INFO: Exception in thread &quot;main&quot; java.net.ConnectException: Connection refused&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.OutputRedirector redirect&#xA;INFO:   at java.net.PlainSocketImpl.socketConnect(Native Method)&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.OutputRedirector redirect&#xA;INFO:   at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.OutputRedirector redirect&#xA;INFO:   at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.OutputRedirector redirect&#xA;INFO:   at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.OutputRedirector redirect&#xA;INFO:   at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.OutputRedirector redirect&#xA;INFO:   at java.net.Socket.connect(Socket.java:589)&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.OutputRedirector redirect&#xA;INFO:   at java.net.Socket.connect(Socket.java:538)&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.OutputRedirector redirect&#xA;INFO:   at java.net.Socket.&amp;lt;init&amp;gt;(Socket.java:434)&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.OutputRedirector redirect&#xA;INFO:   at java.net.Socket.&amp;lt;init&amp;gt;(Socket.java:244)&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.OutputRedirector redirect&#xA;INFO:   at org.apache.spark.launcher.LauncherBackend.connect(LauncherBackend.scala:48)&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.OutputRedirector redirect&#xA;INFO:   at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:147)&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.OutputRedirector redirect&#xA;INFO:   at org.apache.spark.deploy.yarn.Client.run(Client.scala:1146)&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.OutputRedirector redirect&#xA;INFO:   at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1518)&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.OutputRedirector redirect&#xA;INFO:   at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879)&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.OutputRedirector redirect&#xA;INFO:   at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197)&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.OutputRedirector redirect&#xA;INFO:   at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227)&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.OutputRedirector redirect&#xA;INFO:   at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136)&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.OutputRedirector redirect&#xA;INFO:   at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.OutputRedirector redirect&#xA;INFO: 18/03/07 17:34:23 INFO util.ShutdownHookManager: Shutdown hook called&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.OutputRedirector redirect&#xA;INFO: 18/03/07 17:34:23 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-7c170234-8885-41c3-8214-95863039a838&#xA;Spark job's state changed to: FAILED - FAILED&#xA;null&#xA;Mar 07, 2018 5:34:23 PM org.apache.spark.launcher.AbstractAppHandle setState&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the code that launches the driver:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;public static String submit(String mainClass, String... appArgs) throws IOException, InterruptedException{&#xA;    final CountDownLatch countDownLatch = new CountDownLatch(1);&#xA;    SparkAppListener sparkAppListener = new SparkAppListener(countDownLatch);&#xA;    SparkAppHandle appHandle = new SparkLauncher().setMaster(&quot;yarn&quot;)&#xA;            .setDeployMode(&quot;cluster&quot;)&#xA;            .setAppName(&quot;test&quot;)&#xA;            .setConf(&quot;spark.driver.maxResultSize&quot;, &quot;22g&quot;)&#xA;            .addJar(&quot;/home/hadoop/libs/spark_lib.jar&quot;)&#xA;            .setAppResource(SparkEngine.class.getProtectionDomain().getCodeSource().getLocation().getPath())&#xA;            .setSparkHome(&quot;/usr/local/spark-2.3.0-bin-2.6.0-cdh5.8.0&quot;)&#xA;            .addSparkArg(&quot;--num-executors&quot;, &quot;11&quot;)&#xA;            .addSparkArg(&quot;--driver-memory&quot;, &quot;2g&quot;)&#xA;            .addSparkArg(&quot;--executor-memory&quot;, &quot;2g&quot;)&#xA;            .addSparkArg(&quot;--executor-cores&quot;, &quot;2&quot;)&#xA;            .addSparkArg(&quot;--principal&quot;, &quot;hadoop&quot;)&#xA;            .addSparkArg(&quot;--keytab&quot;, &quot;/home/hadoop/hadoop.keytab&quot;)&#xA;            .setMainClass(mainClass)&#xA;            .addAppArgs(appArgs)&#xA;            .setVerbose(true)&#xA;            .startApplication(sparkAppListener);&#xA;    Thread sparkAppListenerThread = new Thread(sparkAppListener);&#xA;    sparkAppListenerThread.start();&#xA;    countDownLatch.await(20, TimeUnit.MINUTES);&#xA;    SparkAppHandle.State state = appHandle.getState();&#xA;    if (state.name().equals(&quot;FINISHED&quot;)) {&#xA;        return appHandle.getAppId();&#xA;    } else {&#xA;        return null;&#xA;    }&#xA;}&#xA;&#xA;private static class SparkAppListener implements SparkAppHandle.Listener, Runnable {&#xA;    private final CountDownLatch countDownLatch;&#xA;    SparkAppListener(CountDownLatch countDownLatch) {&#xA;        this.countDownLatch = countDownLatch;&#xA;    }&#xA;    @Override&#xA;    public void stateChanged(SparkAppHandle handle) {&#xA;        String sparkAppId = handle.getAppId();&#xA;        SparkAppHandle.State appState = handle.getState();&#xA;        if (sparkAppId != null) {&#xA;            System.out.println(&quot;Spark job with app id: &quot; + sparkAppId + &quot;,\t State changed to: &quot; + appState + &quot; - &quot;&#xA;                    + appState.name());&#xA;        } else {&#xA;            System.out.println(&quot;Spark job's state changed to: &quot; + appState + &quot; - &quot; + appState.name());&#xA;        }&#xA;        if (appState.isFinal()) {&#xA;            countDownLatch.countDown();&#xA;        }&#xA;    }&#xA;    @Override&#xA;    public void infoChanged(SparkAppHandle handle) {}&#xA;    @Override&#xA;    public void run() {}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="9456046" LastActivityDate="2018-03-07T12:05:22.440" Title="SparkLauncher failed with Connection refused" Tags="&lt;apache-spark&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49151668" PostTypeId="1" CreationDate="2018-03-07T12:12:13.320" Score="0" ViewCount="27" Body="&lt;p&gt;I have this mistake with spark streaming, I need to store a global variable (like a counter) to use in the various executors, which only increments in the driver. &lt;em&gt;Spark accumulators&lt;/em&gt; can not be used because they have local context for every executors. I have also tried to use &lt;em&gt;spark caching&lt;/em&gt;, but it allows only storing entire data sets, not just an integer variable. &lt;em&gt;BlockManager&lt;/em&gt; could be a good solution but from the documentation provided by spark it is difficult to understand anything. &#xA;I hope you can help to find a solution. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;N.B: I use spark with python&lt;/p&gt;&#xA;" OwnerUserId="1690863" LastActivityDate="2018-03-07T12:12:13.320" Title="How to utilize a global variable in spark, caching or BlockManager?" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;caching&gt;&lt;global-variables&gt;&lt;spark-streaming&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49151722" PostTypeId="1" AcceptedAnswerId="49152239" CreationDate="2018-03-07T12:15:00.110" Score="1" ViewCount="37" Body="&lt;p&gt;I am new to spark and Cassandra. I Use this code but it give me error. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val dfprev = df.select(col = &quot;se&quot;,&quot;hu&quot;)&#xA;val a = dfprev.select(&quot;se&quot;)&#xA;val b = dfprev.select(&quot;hu&quot;)&#xA;val collection = sc.parallelize(Seq(a,b))&#xA;collection.saveToCassandra(&quot;keyspace&quot;, &quot;table&quot;, SomeColumns(&quot;se&quot;,&quot;hu&quot;))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I enter this code on &lt;code&gt;savetocassandra&lt;/code&gt;, it give me error and the error is: &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;java.lang.IllegalArgumentException: Multiple constructors with the same number of parameters not allowed.&#xA;    at com.datastax.spark.connector.util.Reflect$.methodSymbol(Reflect.scala:16)&#xA;    at com.datastax.spark.connector.util.ReflectionUtil$.constructorParams(ReflectionUtil.scala:63)&#xA;    at com.datastax.spark.connector.mapper.DefaultColumnMapper.(DefaultColumnMapper.scala:45) at com.datastax.spark.connector.mapper.LowPriorityColumnMapper$class.defaultColumnMapper(ColumnMapper.scala:51)&#xA;    at&#xA;  om.datastax.spark.connector.mapper.ColumnMapper$.defaultColumnMapper(ColumnMapper.scala:55)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="6450118" LastEditorUserId="5414105" LastEditDate="2018-03-07T12:47:10.583" LastActivityDate="2018-03-07T12:47:10.583" Title="How to write file to cassandra from spark" Tags="&lt;scala&gt;&lt;hadoop&gt;&lt;apache-spark&gt;&lt;cassandra&gt;&lt;apache-spark-sql&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="0" />
  <row Id="49151938" PostTypeId="1" CreationDate="2018-03-07T12:26:29.030" Score="0" ViewCount="25" Body="&lt;p&gt;I am working with AWS Glue and PySpark ETL scripts, and want to use auxiliary libraries such as &lt;code&gt;google_cloud_bigquery&lt;/code&gt; as a part of my PySpark scripts.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The &lt;a href=&quot;https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-python-libraries.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;documentation states this should be possible&lt;/a&gt;. &lt;a href=&quot;https://stackoverflow.com/questions/46329561/aws-glue-python&quot;&gt;This previous Stack Overflow discussion&lt;/a&gt;, especially one comment in one of the answers seems to provide additional proof. However, &lt;em&gt;how&lt;/em&gt; to do it is unclear to me.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So the goal is to turn the &lt;code&gt;pip install&lt;/code&gt;ed packages into one or more zip files, in order to be able to just host the packages on S3 and point to them like so: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;s3://bucket/prefix/lib_A.zip,s3://bucket_B/prefix/lib_X.zip&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How &lt;em&gt;that&lt;/em&gt; should be done is not clearly stated anywhere I've looked.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;i.e. how do I &lt;code&gt;pip install&lt;/code&gt; a package and then turn it into a zip file&lt;/strong&gt; that I can upload to S3 so PySpark can use it with such an S3 URL?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;By using the command &lt;code&gt;pip download&lt;/code&gt; I have been able to fetch the libs, but they are not .zip files by default but instead either .whl files or .tar.gz&lt;/p&gt;&#xA;&#xA;&lt;p&gt;..so not sure what to do to turn them into zip files that AWS Glue can digest. Maybe with .tar.gz I could just &lt;code&gt;tar -xf&lt;/code&gt; them and then &lt;code&gt;zip&lt;/code&gt; them back up, but how about whl files?&lt;/p&gt;&#xA;" OwnerUserId="1525876" LastActivityDate="2018-03-08T14:50:39.677" Title="How to turn pip / pypi installed python packages into zip files to be used in AWS Glue" Tags="&lt;python&gt;&lt;amazon-web-services&gt;&lt;amazon-s3&gt;&lt;pyspark&gt;&lt;aws-glue&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="49152180" PostTypeId="1" CreationDate="2018-03-07T12:38:29.177" Score="0" ViewCount="16" Body="&lt;p&gt;I use Spark 1.6.0 to write a wide table (400+ columns) as a parquet file.&#xA;The resources of my executors is limited to 16Gb.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I read the table with &lt;code&gt;distribute by &amp;lt;partition_cols&amp;gt;&lt;/code&gt; and then use &lt;code&gt;df.write.distributeBy(&amp;lt;partition_cols&amp;gt;).parquet(&amp;lt;path&amp;gt;)&lt;/code&gt;.&lt;br&gt;&#xA;This gives me one parquet file per partition. Thats exactly what I want.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is, that the executors run out of memory when the partition size gets too big. I tried to address this problem, by reducing the &lt;code&gt;parquet.block.size&lt;/code&gt;from 128Mb to 64Mb. This had no effect. When I examined an output parquet file, I saw that the row group size is only about 2Mb. &#xA;For another parquet file, that was created from a narrow table with the same configuration, the row group size is 100Mb. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can somebody explain to me how parquet files are written, and the effect of the &lt;code&gt;parquet.block.size&lt;/code&gt;?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;" OwnerUserId="6162321" LastActivityDate="2018-03-07T12:38:29.177" Title="Spark parquet row group size for wide tables" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;hadoop2&gt;&lt;parquet&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49152501" PostTypeId="1" CreationDate="2018-03-07T12:54:40.620" Score="0" ViewCount="8" Body="&lt;p&gt;I am trying to connect ElasticSerach 5.6.6 from Spark 1.6.&#xA;Following are configured in my code. Is there anything else I am missing ?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  &quot;xpack.security.user&quot;&#xA;  &quot;xpack.security.transport.ssl.enabled&quot;&#xA;  &quot;xpack.security.transport.ssl.keystore.path&quot;&#xA;  &quot;xpack.security.transport.ssl.keystore.password&quot;&#xA;  &quot;xpack.security.transport.ssl.truststore.path&quot;&#xA;  &quot;xpack.security.transport.ssl.truststore.password&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Getting follwoung issues:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;2018-03-07 07:33:01,509 WARN  [task-result-getter-2] scheduler.TaskSetManager(70): Lost task 2.0 in stage 6.0 (TID 150, ip-172-30-4-42.ec2.internal, executor 3): org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only'&#xA;at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:250)&#xA;at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:546)&#xA;at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58)&#xA;at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102)&#xA;at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:102)&#xA;at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)&#xA;at org.apache.spark.scheduler.Task.run(Task.scala:89)&#xA;at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:242)&#xA;at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&#xA;at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&#xA;at java.lang.Thread.run(Thread.java:748)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Caused by: org.elasticsearch.hadoop.rest.EsHadoopTransportException: javax.net.ssl.SSLException: java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty&#xA;    at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:124)&#xA;    at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)&#xA;    at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:424)&#xA;    at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:428)&#xA;    at org.elasticsearch.hadoop.rest.RestClient.get(RestClient.java:154)&#xA;    at org.elasticsearch.hadoop.rest.RestClient.remoteEsVersion(RestClient.java:609)&#xA;    at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:243)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Caused by: org.elasticsearch.hadoop.rest.EsHadoopTransportException: javax.net.ssl.SSLException: java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty&#xA;at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:124)&#xA;at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:444)&#xA;at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:424)&#xA;at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:428)&#xA;at org.elasticsearch.hadoop.rest.RestClient.get(RestClient.java:154)&#xA;at org.elasticsearch.hadoop.rest.RestClient.remoteEsVersion(RestClient.java:609)&#xA;at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:243)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="7811886" LastActivityDate="2018-03-08T12:26:46.170" Title="Elasticsearch SSL trustAnchor issue" Tags="&lt;apache-spark&gt;&lt;elasticsearch&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49152747" PostTypeId="1" CreationDate="2018-03-07T13:08:05.643" Score="0" ViewCount="12" Body="&lt;p&gt;How do we split spark data frame into test, train data frames by stratified split. I searched on web and got answers with rdds but not with data frames. And I want the split to be done on the dataframe before applying VectorAssembler().&lt;/p&gt;&#xA;" OwnerUserId="9186358" LastActivityDate="2018-03-07T13:08:05.643" Title="stratified test train split of spark dataframe using scala" Tags="&lt;scala&gt;&lt;spark-dataframe&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49152771" PostTypeId="1" CreationDate="2018-03-07T13:09:32.093" Score="0" ViewCount="35" Body="&lt;p&gt;Trying to catch up with the Spark 2.3 documentation on how to deploy jobs on a Kubernetes 1.9.3 cluster : &lt;a href=&quot;http://spark.apache.org/docs/latest/running-on-kubernetes.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://spark.apache.org/docs/latest/running-on-kubernetes.html&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The Kubernetes 1.9.3 cluster is operating properly on offline bare-metal servers and was installed with &lt;code&gt;kubeadm&lt;/code&gt;. The following command was used to submit the job (&lt;code&gt;SparkPi&lt;/code&gt; example job):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;/opt/spark/bin/spark-submit --master k8s://https://k8s-master:6443 --deploy-mode cluster --name spark-pi --class org.apache.spark.examples.SparkPi --conf spark.executor.instances=2 --conf spark.kubernetes.container.image=spark:v2.3.0 local:///opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the stacktrace that we all love:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;++ id -u&#xA;+ myuid=0&#xA;++ id -g&#xA;+ mygid=0&#xA;++ getent passwd 0&#xA;+ uidentry=root:x:0:0:root:/root:/bin/ash&#xA;+ '[' -z root:x:0:0:root:/root:/bin/ash ']'&#xA;+ SPARK_K8S_CMD=driver&#xA;+ '[' -z driver ']'&#xA;+ shift 1&#xA;+ SPARK_CLASSPATH=':/opt/spark/jars/*'&#xA;+ env&#xA;+ grep SPARK_JAVA_OPT_&#xA;+ sed 's/[^=]*=\(.*\)/\1/g'&#xA;+ readarray -t SPARK_JAVA_OPTS&#xA;+ '[' -n /opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar:/opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar ']'&#xA;+ SPARK_CLASSPATH=':/opt/spark/jars/*:/opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar:/opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar'&#xA;+ '[' -n '' ']'&#xA;+ case &quot;$SPARK_K8S_CMD&quot; in&#xA;+ CMD=(${JAVA_HOME}/bin/java &quot;${SPARK_JAVA_OPTS[@]}&quot; -cp &quot;$SPARK_CLASSPATH&quot; -Xms$SPARK_DRIVER_MEMORY -Xmx$SPARK_DRIVER_MEMORY -Dspark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS $SPARK_DRIVER_CLASS $SPARK_DRIVER_ARGS)&#xA;+ exec /sbin/tini -s -- /usr/lib/jvm/java-1.8-openjdk/bin/java -Dspark.kubernetes.driver.pod.name=spark-pi-b6f8a60df70a3b9d869c4e305518f43a-driver -Dspark.driver.port=7078 -Dspark.submit.deployMode=cluster -Dspark.master=k8s://https://k8s-master:6443 -Dspark.kubernetes.executor.podNamePrefix=spark-pi-b6f8a60df70a3b9d869c4e305518f43a -Dspark.driver.blockManager.port=7079 -Dspark.app.id=spark-7077ad8f86114551b0ae04ae63a74d5a -Dspark.driver.host=spark-pi-b6f8a60df70a3b9d869c4e305518f43a-driver-svc.default.svc -Dspark.app.name=spark-pi -Dspark.kubernetes.container.image=spark:v2.3.0 -Dspark.jars=/opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar,/opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar -Dspark.executor.instances=2 -cp ':/opt/spark/jars/*:/opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar:/opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar' -Xms1g -Xmx1g -Dspark.driver.bindAddress=10.244.1.17 org.apache.spark.examples.SparkPi&#xA;2018-03-07 12:39:35 INFO  SparkContext:54 - Running Spark version 2.3.0&#xA;2018-03-07 12:39:36 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&#xA;2018-03-07 12:39:36 INFO  SparkContext:54 - Submitted application: Spark Pi&#xA;2018-03-07 12:39:36 INFO  SecurityManager:54 - Changing view acls to: root&#xA;2018-03-07 12:39:36 INFO  SecurityManager:54 - Changing modify acls to: root&#xA;2018-03-07 12:39:36 INFO  SecurityManager:54 - Changing view acls groups to: &#xA;2018-03-07 12:39:36 INFO  SecurityManager:54 - Changing modify acls groups to: &#xA;2018-03-07 12:39:36 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()&#xA;2018-03-07 12:39:36 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 7078.&#xA;2018-03-07 12:39:36 INFO  SparkEnv:54 - Registering MapOutputTracker&#xA;2018-03-07 12:39:36 INFO  SparkEnv:54 - Registering BlockManagerMaster&#xA;2018-03-07 12:39:36 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information&#xA;2018-03-07 12:39:36 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up&#xA;2018-03-07 12:39:36 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-7f5370ad-b495-4943-ad75-285b7ead3e5b&#xA;2018-03-07 12:39:36 INFO  MemoryStore:54 - MemoryStore started with capacity 408.9 MB&#xA;2018-03-07 12:39:36 INFO  SparkEnv:54 - Registering OutputCommitCoordinator&#xA;2018-03-07 12:39:36 INFO  log:192 - Logging initialized @1936ms&#xA;2018-03-07 12:39:36 INFO  Server:346 - jetty-9.3.z-SNAPSHOT&#xA;2018-03-07 12:39:36 INFO  Server:414 - Started @2019ms&#xA;2018-03-07 12:39:36 INFO  AbstractConnector:278 - Started ServerConnector@4215838f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}&#xA;2018-03-07 12:39:36 INFO  Utils:54 - Successfully started service 'SparkUI' on port 4040.&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5b6813df{/jobs,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@495083a0{/jobs/json,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5fd62371{/jobs/job,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2b62442c{/jobs/job/json,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@66629f63{/stages,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@841e575{/stages/json,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@27a5328c{/stages/stage,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6b5966e1{/stages/stage/json,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@65e61854{/stages/pool,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@1568159{/stages/pool/json,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4fcee388{/storage,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6f80fafe{/storage/json,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3af17be2{/storage/rdd,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@f9879ac{/storage/rdd/json,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@37f21974{/environment,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5f4d427e{/environment/json,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@6e521c1e{/executors,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@224b4d61{/executors/json,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@5d5d9e5{/executors/threadDump,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@303e3593{/executors/threadDump/json,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4ef27d66{/static,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@62dae245{/,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@4b6579e8{/api,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@3954d008{/jobs/job/kill,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  ContextHandler:781 - Started o.s.j.s.ServletContextHandler@2f94c4db{/stages/stage/kill,null,AVAILABLE,@Spark}&#xA;2018-03-07 12:39:36 INFO  SparkUI:54 - Bound SparkUI to 0.0.0.0, and started at http://spark-pi-b6f8a60df70a3b9d869c4e305518f43a-driver-svc.default.svc:4040&#xA;2018-03-07 12:39:36 INFO  SparkContext:54 - Added JAR /opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar at spark://spark-pi-b6f8a60df70a3b9d869c4e305518f43a-driver-svc.default.svc:7078/jars/spark-examples_2.11-2.3.0.jar with timestamp 1520426376949&#xA;2018-03-07 12:39:37 WARN  KubernetesClusterManager:66 - The executor's init-container config map is not specified. Executors will therefore not attempt to fetch remote or submitted dependencies.&#xA;2018-03-07 12:39:37 WARN  KubernetesClusterManager:66 - The executor's init-container config map key is not specified. Executors will therefore not attempt to fetch remote or submitted dependencies.&#xA;2018-03-07 12:39:42 ERROR SparkContext:91 - Error initializing SparkContext.&#xA;org.apache.spark.SparkException: External scheduler cannot be instantiated&#xA;    at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2747)&#xA;    at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:492)&#xA;    at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2486)&#xA;    at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:930)&#xA;    at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:921)&#xA;    at scala.Option.getOrElse(Option.scala:121)&#xA;    at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921)&#xA;    at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:31)&#xA;    at org.apache.spark.examples.SparkPi.main(SparkPi.scala)&#xA;Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Operation: [get]  for kind: [Pod]  with name: [spark-pi-b6f8a60df70a3b9d869c4e305518f43a-driver]  in namespace: [default]  failed.&#xA;    at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:62)&#xA;    at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:71)&#xA;    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.getMandatory(BaseOperation.java:228)&#xA;    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.get(BaseOperation.java:184)&#xA;    at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.&amp;lt;init&amp;gt;(KubernetesClusterSchedulerBackend.scala:70)&#xA;    at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterManager.createSchedulerBackend(KubernetesClusterManager.scala:120)&#xA;    at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2741)&#xA;    ... 8 more&#xA;Caused by: java.net.UnknownHostException: kubernetes.default.svc: Try again&#xA;    at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)&#xA;    at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928)&#xA;    at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323)&#xA;    at java.net.InetAddress.getAllByName0(InetAddress.java:1276)&#xA;    at java.net.InetAddress.getAllByName(InetAddress.java:1192)&#xA;    at java.net.InetAddress.getAllByName(InetAddress.java:1126)&#xA;    at okhttp3.Dns$1.lookup(Dns.java:39)&#xA;    at okhttp3.internal.connection.RouteSelector.resetNextInetSocketAddress(RouteSelector.java:171)&#xA;    at okhttp3.internal.connection.RouteSelector.nextProxy(RouteSelector.java:137)&#xA;    at okhttp3.internal.connection.RouteSelector.next(RouteSelector.java:82)&#xA;    at okhttp3.internal.connection.StreamAllocation.findConnection(StreamAllocation.java:171)&#xA;    at okhttp3.internal.connection.StreamAllocation.findHealthyConnection(StreamAllocation.java:121)&#xA;    at okhttp3.internal.connection.StreamAllocation.newStream(StreamAllocation.java:100)&#xA;    at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:42)&#xA;    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)&#xA;    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)&#xA;    at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:93)&#xA;    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)&#xA;    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)&#xA;    at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93)&#xA;    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)&#xA;    at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:120)&#xA;    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)&#xA;    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)&#xA;    at io.fabric8.kubernetes.client.utils.HttpClientUtils$2.intercept(HttpClientUtils.java:93)&#xA;    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)&#xA;    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)&#xA;    at okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:185)&#xA;    at okhttp3.RealCall.execute(RealCall.java:69)&#xA;    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:377)&#xA;    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:343)&#xA;    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleGet(OperationSupport.java:312)&#xA;    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleGet(OperationSupport.java:295)&#xA;    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleGet(BaseOperation.java:783)&#xA;    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.getMandatory(BaseOperation.java:217)&#xA;    ... 12 more&#xA;2018-03-07 12:39:42 INFO  AbstractConnector:318 - Stopped Spark@4215838f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}&#xA;2018-03-07 12:39:42 INFO  SparkUI:54 - Stopped Spark web UI at http://spark-pi-b6f8a60df70a3b9d869c4e305518f43a-driver-svc.default.svc:4040&#xA;2018-03-07 12:39:42 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!&#xA;2018-03-07 12:39:42 INFO  MemoryStore:54 - MemoryStore cleared&#xA;2018-03-07 12:39:42 INFO  BlockManager:54 - BlockManager stopped&#xA;2018-03-07 12:39:42 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped&#xA;2018-03-07 12:39:42 WARN  MetricsSystem:66 - Stopping a MetricsSystem that is not running&#xA;2018-03-07 12:39:42 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!&#xA;2018-03-07 12:39:42 INFO  SparkContext:54 - Successfully stopped SparkContext&#xA;Exception in thread &quot;main&quot; org.apache.spark.SparkException: External scheduler cannot be instantiated&#xA;    at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2747)&#xA;    at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:492)&#xA;    at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2486)&#xA;    at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:930)&#xA;    at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:921)&#xA;    at scala.Option.getOrElse(Option.scala:121)&#xA;    at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:921)&#xA;    at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:31)&#xA;    at org.apache.spark.examples.SparkPi.main(SparkPi.scala)&#xA;Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Operation: [get]  for kind: [Pod]  with name: [spark-pi-b6f8a60df70a3b9d869c4e305518f43a-driver]  in namespace: [default]  failed.&#xA;    at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:62)&#xA;    at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:71)&#xA;    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.getMandatory(BaseOperation.java:228)&#xA;    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.get(BaseOperation.java:184)&#xA;    at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.&amp;lt;init&amp;gt;(KubernetesClusterSchedulerBackend.scala:70)&#xA;    at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterManager.createSchedulerBackend(KubernetesClusterManager.scala:120)&#xA;    at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2741)&#xA;    ... 8 more&#xA;Caused by: java.net.UnknownHostException: kubernetes.default.svc: Try again&#xA;    at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)&#xA;    at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928)&#xA;    at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323)&#xA;    at java.net.InetAddress.getAllByName0(InetAddress.java:1276)&#xA;    at java.net.InetAddress.getAllByName(InetAddress.java:1192)&#xA;    at java.net.InetAddress.getAllByName(InetAddress.java:1126)&#xA;    at okhttp3.Dns$1.lookup(Dns.java:39)&#xA;    at okhttp3.internal.connection.RouteSelector.resetNextInetSocketAddress(RouteSelector.java:171)&#xA;    at okhttp3.internal.connection.RouteSelector.nextProxy(RouteSelector.java:137)&#xA;    at okhttp3.internal.connection.RouteSelector.next(RouteSelector.java:82)&#xA;    at okhttp3.internal.connection.StreamAllocation.findConnection(StreamAllocation.java:171)&#xA;    at okhttp3.internal.connection.StreamAllocation.findHealthyConnection(StreamAllocation.java:121)&#xA;    at okhttp3.internal.connection.StreamAllocation.newStream(StreamAllocation.java:100)&#xA;    at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:42)&#xA;    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)&#xA;    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)&#xA;    at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:93)&#xA;    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)&#xA;    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)&#xA;    at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93)&#xA;    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)&#xA;    at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:120)&#xA;    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)&#xA;    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)&#xA;    at io.fabric8.kubernetes.client.utils.HttpClientUtils$2.intercept(HttpClientUtils.java:93)&#xA;    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:92)&#xA;    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:67)&#xA;    at okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:185)&#xA;    at okhttp3.RealCall.execute(RealCall.java:69)&#xA;    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:377)&#xA;    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:343)&#xA;    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleGet(OperationSupport.java:312)&#xA;    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleGet(OperationSupport.java:295)&#xA;    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleGet(BaseOperation.java:783)&#xA;    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.getMandatory(BaseOperation.java:217)&#xA;    ... 12 more&#xA;2018-03-07 12:39:42 INFO  ShutdownHookManager:54 - Shutdown hook called&#xA;2018-03-07 12:39:42 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-64fe7ad8-669f-4591-a3f6-67440d450a44&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So apparently the Kubernetes Scheduler Backend cannot contact the pod because it is unable to resolve &lt;code&gt;kubernetes.default.svc&lt;/code&gt;. Hum.. why?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also configured RBAC with a &lt;code&gt;spark&lt;/code&gt; service account as mentionned in the documentation but the same problem occurs. (also tried on a different namespace, same problem)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here are the logs from &lt;code&gt;kube-dns&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;I0306 16:04:04.170889       1 dns.go:555] Could not find endpoints for service &quot;spark-pi-b9e8b4c66fe83c4d94a8d46abc2ee8f5-driver-svc&quot; in namespace &quot;default&quot;. DNS records will be created once endpoints show up.&#xA;I0306 16:04:29.751201       1 dns.go:555] Could not find endpoints for service &quot;spark-pi-0665ad323820371cb215063987a31e05-driver-svc&quot; in namespace &quot;default&quot;. DNS records will be created once endpoints show up.&#xA;I0306 16:06:26.414146       1 dns.go:555] Could not find endpoints for service &quot;spark-pi-2bf24282e8033fa9a59098616323e267-driver-svc&quot; in namespace &quot;default&quot;. DNS records will be created once endpoints show up.&#xA;I0307 08:16:17.404971       1 dns.go:555] Could not find endpoints for service &quot;spark-pi-3887031e031732108711154b2ec57d28-driver-svc&quot; in namespace &quot;default&quot;. DNS records will be created once endpoints show up.&#xA;I0307 08:17:11.682218       1 dns.go:555] Could not find endpoints for service &quot;spark-pi-3d84127226393fc99e2fe035db56bfb5-driver-svc&quot; in namespace &quot;default&quot;. DNS records will be created once endpoints show up. &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I really can't figure out why those errors come up.&lt;/p&gt;&#xA;" OwnerUserId="9456760" LastEditorUserId="9456760" LastEditDate="2018-03-08T08:06:56.003" LastActivityDate="2018-03-08T08:06:56.003" Title="Kubernetes 1.9 can't initialize SparkContext" Tags="&lt;apache-spark&gt;&lt;kubernetes&gt;" AnswerCount="0" CommentCount="0" FavoriteCount="1" />
  <row Id="49152806" PostTypeId="1" AcceptedAnswerId="49153627" CreationDate="2018-03-07T13:11:53.267" Score="0" ViewCount="29" Body="&lt;p&gt;I created a Hive table for which we added some description in the &quot;comment&quot; field for each variable as shown below:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark.sql(&quot;create table test_comment (col string comment 'col comment') comment 'hello world table comment ' &quot;)&#xA;spark.sql(&quot;describe test_comment&quot;).show()&#xA;+--------+---------+-----------+&#xA;|col_name|data_type|    comment|&#xA;+--------+---------+-----------+&#xA;|     col|   string|col comment|&#xA;+--------+---------+-----------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;All is good and we see the comment &quot;col comment&quot; in the commennt field of the variable &quot;col&quot;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now when I am creating a view on this table, the &quot;comment&quot; field is not propagated to the view and the &quot;comment&quot; column is empty:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark.sql(&quot;&quot;&quot;create view test_comment_view as select * from test_comment&quot;&quot;&quot;)&#xA;spark.sql(&quot;describe test_comment_view&quot;)&#xA;&#xA;+--------+---------+-------+&#xA;|col_name|data_type|comment|&#xA;+--------+---------+-------+&#xA;|     col|   string|   null|&#xA;+--------+---------+-------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there a way to keep the values of the comment field when created a view ? What is the reason of this &quot;feature&quot; ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am using:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hadoop 2.6.0-cdh5.8.0&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hive 1.1.0-cdh5.8.0&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Spark 2.1.0.cloudera1&lt;/p&gt;&#xA;" OwnerUserId="6430839" LastActivityDate="2018-03-07T14:09:11.107" Title="create view on Hive table: comment for each variable are lost" Tags="&lt;view&gt;&lt;hive&gt;&lt;pyspark&gt;&lt;comments&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49152833" PostTypeId="1" CreationDate="2018-03-07T13:12:58.023" Score="0" ViewCount="16" Body="&lt;p&gt;Iv'e been reading a bit about the parquet format and how spark integrates with it. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Being a columnar storage, &lt;a href=&quot;https://db-blog.web.cern.ch/blog/luca-canali/2017-06-diving-spark-and-parquet-workloads-example&quot; rel=&quot;nofollow noreferrer&quot;&gt;parquet really shines&lt;/a&gt; whenever spark can collaborate with the underlying storage in order to perform projections without having to load all the data as well as instructing the storage to load specific column chunks based on various statistics (when a filter is involved).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I saw a &lt;a href=&quot;https://www.youtube.com/watch?v=_0Wpwj_gvzg&amp;amp;t=1405s&quot; rel=&quot;nofollow noreferrer&quot;&gt;lecture on youtube&lt;/a&gt; (21:54) cautioning that object stores do not support pushdown filters (specifically Amazon S3 was given as an example). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How does the Azure Blob Storage fare regarding this (when we read the session parquets)?&lt;/p&gt;&#xA;" OwnerUserId="180650" LastActivityDate="2018-03-07T13:36:56.627" Title="Does Azure blob store support for parquet column projection and pushdown filters/predicates" Tags="&lt;apache-spark&gt;&lt;parquet&gt;&lt;azure-blob-storage&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49152967" PostTypeId="1" CreationDate="2018-03-07T13:19:03.043" Score="2" ViewCount="40" Body="&lt;p&gt;I try to create a simple program using Spark in Java and I get this error:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Error:(10, 57) java: incompatible types: org.apache.spark.SparkConf cannot be converted to org.apache.spark.SparkContext&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;My code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;package com.example.lab;&#xA;import org.apache.spark.SparkConf;&#xA;import org.apache.spark.api.java.JavaSparkContext;&#xA;&#xA;public class Lab {&#xA;    public static void main(String[] args) {&#xA;&#xA;            SparkConf config = new SparkConf();&#xA;            config.setMaster(&quot;local[*]&quot;).setAppName(&quot;Lab&quot;);&#xA;            JavaSparkContext jsc = new JavaSparkContext(config);&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have a Windows 8.1 PC, running Java 1.8 and Spark v 2.3.0.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Why do I get this error?&lt;/p&gt;&#xA;" OwnerUserId="3858890" LastActivityDate="2018-03-07T13:33:57.203" Title="JAVA: SparkConf cannot be converted to SparkContext" Tags="&lt;java&gt;&lt;hadoop&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49153021" PostTypeId="1" CreationDate="2018-03-07T13:22:40.277" Score="0" ViewCount="14" Body="&lt;p&gt;I'm struggling trying to configure ssl communication between mesos 1.6.0 and Spark 2.2.1.&#xA;My mesos cluster components talk each other using ssl and now I'm trying, with no luck, to deploy spark jobs to the master.&#xA;I configured mesos components using this variables in /etc/default/mesos&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;LIBPROCESS_SSL_CA_DIR=/etc/pki/tls/certs/&#xA;LIBPROCESS_SSL_CA_FILE=/etc/pki/tls/certs/localhost.crt&#xA;LIBPROCESS_SSL_KEY_FILE=/etc/pki/tls/private/localhost.key&#xA;LIBPROCESS_SSL_ENABLED=true&#xA;LIBPROCESS_SSL_CERT_FILE=/etc/pki/tls/certs/localhost.crt&#xA;LIBPROCESS_SSL_SUPPORT_DOWNGRADE=false&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and spark with  this parameters:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark.ssl.enabled                  true&#xA;spark.ssl.protocol                 TLSv1.2&#xA;spark.ssl.keyStore                 /opt/software/Spark/2.2.1/keystore.jks&#xA;spark.ssl.keyStorePassword         password&#xA;spark.ssl.trustStore /opt/software/Spark/2.2.1/server.truststore&#xA;spark.ssl.trustStorePassword       password&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But, infortunately spark refuse to talk with mesos using ssl so in the logs I found this entries&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;W0307 14:14:39.847388 29661 process.cpp:894] Failed to accept socket: Failed accept: connection error: error:1407609C:SSL routines:SSL23_GET_CLIENT_HELLO:http request&#xA;W0307 14:14:41.552497 29661 process.cpp:894] Failed to accept socket: Failed accept: connection error: error:1407609C:SSL routines:SSL23_GET_CLIENT_HELLO:http request&#xA;W0307 14:14:41.572708 29661 process.cpp:894] Failed to accept socket: Failed accept: connection error: error:1407609C:SSL routines:SSL23_GET_CLIENT_HELLO:http request&#xA;W0307 14:14:45.633244 29661 process.cpp:894] Failed to accept socket: Failed accept: connection error: error:1407609C:SSL routines:SSL23_GET_CLIENT_HELLO:http request&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and the client (pyspark in this example) remains stuck on &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;18/03/07 13:14:38 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).&#xA;I0307 14:14:39.843199 20640 sched.cpp:232] Version: 1.6.0&#xA;I0307 14:14:39.845962 20627 sched.cpp:336] New master detected at master@10.110.193.52:5050&#xA;**I0307 14:14:39.846292 20627 sched.cpp:351] No credentials provided. Attempting to register without authentication**&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If I set &lt;code&gt;LIBPROCESS_SSL_SUPPORT_DOWNGRADE=true&lt;/code&gt; the connection is estabilished and the job starts.&#xA;Any help about this problem will be appreciated !&lt;/p&gt;&#xA;" OwnerUserId="7424721" LastEditorUserId="7424721" LastEditDate="2018-03-08T14:34:42.277" LastActivityDate="2018-03-08T14:34:42.277" Title="Problems deploying spark jobs on mesos using ssl" Tags="&lt;apache-spark&gt;&lt;ssl&gt;&lt;mesos&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49153302" PostTypeId="1" CreationDate="2018-03-07T13:38:04.187" Score="0" ViewCount="17" Body="&lt;p&gt;I have a requirement to work with a positional file which has 150 columns.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have tried to solve this by using substring ,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val inputFile = sc.textFile(&quot;hdsf://....&quot;)&#xA;&#xA;    inputFile.map(x =&amp;gt;(x.substring(0,1),x.substring(1,11),x.substring(11,18),x.substring(18,33),x.substring(33,35),x.substring(35,36),x.substring(36,42),x.substring(42,97),x.substring(97,103),x.substring(103,106),x.substring(106,109),x.substring(109,121),x.substring(121,129),x.substring(129,149),x.substring(149,151),x.substring(151,181),x.substring(181,191),x.substring(191,193),x.substring(193,204),x.substring(204,207),x.substring(207,208),x.substring(208,219),x.substring(219,229),x.substring(229,231),x.substring(231,235),x.substring(235,243),x.substring(243,245),x.substring(245,247),x.substring(247,257),x.substring(257,270),x.substring(270,271),x.substring(271,280),x.substring(280,289),x.substring(289,298),x.substring(298,307),x.substring(307,316),x.substring(316,325),x.substring(325,334),x.substring(334,343),x.substring(343,352),x.substring(352,361),x.substring(361,370),x.substring(370,379),x.substring(379,388),x.substring(388,397),x.substring(397,406),x.substring(406,415),x.substring(415,424),x.substring(424,433),x.substring(433,442),x.substring(442,457),x.substring(457,482),x.substring(482,483),x.substring(483,491),x.substring(491,492),x.substring(492,512),x.substring(512,527),x.substring(527,530),x.substring(530,531),x.substring(531,546),x.substring(546,548),x.substring(548,563),x.substring(563,588),x.substring(588,603),x.substring(603,609),x.substring(609,615),x.substring(615,630),x.substring(630,632),x.substring(632,634),x.substring(634,645),x.substring(645,647),x.substring(647,667),x.substring(667,692),x.substring(692,707),x.substring(707,708),x.substring(708,716),x.substring(716,756),x.substring(756,796),x.substring(796,811),x.substring(811,813),x.substring(813,828),x.substring(828,830),x.substring(830,831),x.substring(831,833),x.substring(833,841),x.substring(841,842),x.substring(842,844),x.substring(844,845),x.substring(845,846),x.substring(846,847),x.substring(847,848),x.substring(848,849),x.substring(849,851),x.substring(851,852),x.substring(852,853),x.substring(853,854),x.substring(854,869),x.substring(869,879),x.substring(879,889),x.substring(889,899),x.substring(899,900),x.substring(900,901),x.substring(901,909),x.substring(909,923),x.substring(923,983),x.substring(983,988),x.substring(988,994),x.substring(994,1003),x.substring(1003,1018),x.substring(1018,1033),x.substring(1033,1042),x.substring(1042,1050),x.substring(1050,1056),x.substring(1056,1071),x.substring(1071,1074),x.substring(1074,1075),x.substring(1075,1077),x.substring(1077,1078),x.substring(1078,1118),x.substring(1118,1119),x.substring(1119,1120),x.substring(1120,1129),x.substring(1129,1138),x.substring(1138,1146),x.substring(1146,1147),x.substring(1147,1148),x.substring(1148,1149),x.substring(1149,1150),x.substring(1150,1151),x.substring(1151,1152),x.substring(1152,1153),x.substring(1153,1154),x.substring(1154,1155),x.substring(1155,1156),x.substring(1156,1157),x.substring(1157,1165),x.substring(1165,1174),x.substring(1174,1184),x.substring(1184,1190),x.substring(1190,1196),x.substring(1196,1206),x.substring(1206,1216),x.substring(1216,1218),x.substring(1218,1227),x.substring(1227,1228),x.substring(1228,1237),x.substring(1237,1238),x.substring(1238,1247),x.substring(1247,1248),x.substring(1248,1548),x.substring(1548,1598))).take(2)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;but I am getting error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;console&amp;gt;:1: error: too many elements for tuple: 151, allowed: 22&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;can any one give hints on how can I use structfiled concept here to convert the positioned file to pipe delimited.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am using scala 2.11 version&lt;/p&gt;&#xA;" OwnerUserId="6561443" LastActivityDate="2018-03-07T13:38:04.187" Title="convert the positioned file have 150 columns to pipe dlimited" Tags="&lt;apache-spark&gt;" AnswerCount="0" CommentCount="1" ClosedDate="2018-03-07T14:57:30.230" />
  <row Id="49153795" PostTypeId="1" CreationDate="2018-03-07T14:02:39.437" Score="0" ViewCount="24" Body="&lt;p&gt;My team is in the process of finding a good unit testing framework for our spark/scala application. So far, the most popular ones we found were: Spark Testing Base (&lt;a href=&quot;https://github.com/holdenk/spark-testing-base&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://github.com/holdenk/spark-testing-base&lt;/a&gt;) and ScalaTest (&lt;a href=&quot;http://www.scalatest.org/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.scalatest.org/&lt;/a&gt;). I was wondering if anyone had any inputs on these and their &lt;strong&gt;pros/cons&lt;/strong&gt;. Thanks! &lt;/p&gt;&#xA;" OwnerUserId="5505587" LastEditorUserId="5505587" LastEditDate="2018-03-07T14:14:39.070" LastActivityDate="2018-03-07T14:14:39.070" Title="Unit Testing for Spark/Scala: SparkTestingBase vs ScalaTest" Tags="&lt;scala&gt;&lt;unit-testing&gt;&lt;apache-spark&gt;" AnswerCount="0" CommentCount="1" ClosedDate="2018-03-07T16:15:55.340" />
  <row Id="49154059" PostTypeId="1" CreationDate="2018-03-07T14:16:35.797" Score="1" ViewCount="31" Body="&lt;p&gt;I am having JSON objects which take the following form:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;{&#xA;    &quot;docId&quot; : &quot;1&quot;,&#xA;    &quot;links&quot; : {&#xA;        &quot;a link&quot;: [&quot;endpoint 1&quot;, &quot;endpoint 2&quot;],&#xA;        &quot;another link&quot;: [&quot;endpoint 3&quot;] &#xA;    },&#xA;    &quot;authors&quot; : [&quot;Thomas&quot;, &quot;Peter&quot;],&#xA;    &quot;text&quot;: &quot;This is the document text!&quot;    &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In order to load it with &lt;code&gt;SparkSession.read.json&lt;/code&gt; I am writing all those JSON objects into one file &lt;code&gt;concatenated.txt&lt;/code&gt; where each line represents one entire document:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;{&quot;docId&quot;: &quot;1&quot;, ...}&#xA;{&quot;docId&quot;: &quot;2&quot;, ...}&#xA;{&quot;docId&quot;: &quot;3&quot;, ...}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The problem is if I run&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df_data = spark.read.json('concatenated.txt')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;it does not work because I am running into a &lt;code&gt;java.lang.OutOfMemoryError&lt;/code&gt;. I don't understand the exact problem but I guess it's just not able to infer the correct types. For that reason I defined the following schema as a work around. This is working so far:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;schema = StructType([&#xA;        StructField(&quot;docId&quot;, StringType(), True),&#xA;        StructField(&quot;links&quot;, StringType(), True),&#xA;        StructField(&quot;authors&quot;, StringType(), True),&#xA;        StructField(&quot;text&quot;, StringType(), True)                              &#xA;])&#xA;&#xA;df_data = spark.read.json('concatenated.txt', schema=schema)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;but of course, this requires me to do someting like:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;import json &#xA;# ..&#xA;df_data.flatMap(lambda x: json.loads(x.authors))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;in order to have the actual underlying object.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question would be how I can load the JSON data such that all the complex objects are actually constructed on load. So &lt;code&gt;authors&lt;/code&gt; should e.g. always contain a &lt;code&gt;list&lt;/code&gt; (or &lt;code&gt;None&lt;/code&gt;), whereas &lt;code&gt;links&lt;/code&gt; is a &lt;code&gt;dict&lt;/code&gt; which values are also of the type &lt;code&gt;list&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;Sample data: &lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;{ &quot;docId&quot; : &quot;1&quot;, &quot;links&quot; : { &quot;a link&quot;: [&quot;endpoint 1&quot;, &quot;endpoint 2&quot;], &quot;another link&quot;: [&quot;endpoint 3&quot;] }, &quot;authors&quot; : [&quot;Thomas&quot;, &quot;Peter&quot;], &quot;text&quot;: &quot;This is the document text!&quot; }&#xA;{ &quot;docId&quot; : &quot;2&quot;, &quot;links&quot; : { &quot;a link&quot;: [&quot;endpoint 1&quot;, &quot;endpoint 2&quot;], &quot;another link&quot;: [&quot;endpoint 3&quot;] }, &quot;authors&quot; : [&quot;Thomas&quot;, &quot;Peter&quot;], &quot;text&quot;: &quot;This is the document text!&quot; }&#xA;{ &quot;docId&quot; : &quot;3&quot;, &quot;links&quot; : { &quot;a link&quot;: [&quot;endpoint 1&quot;, &quot;endpoint 2&quot;], &quot;another link&quot;: [&quot;endpoint 3&quot;] }, &quot;authors&quot; : [&quot;Thomas&quot;, &quot;Peter&quot;], &quot;text&quot;: &quot;This is the document text!&quot; }&#xA;{ &quot;docId&quot; : &quot;4&quot;, &quot;links&quot; : { &quot;a link&quot;: [&quot;endpoint 1&quot;, &quot;endpoint 2&quot;], &quot;another link&quot;: [&quot;endpoint 3&quot;] }, &quot;authors&quot; : [&quot;Thomas&quot;, &quot;Peter&quot;], &quot;text&quot;: &quot;This is the document text!&quot; }&#xA;{ &quot;docId&quot; : &quot;5&quot;, &quot;links&quot; : { &quot;a link&quot;: [&quot;endpoint 1&quot;, &quot;endpoint 2&quot;], &quot;another link&quot;: [&quot;endpoint 3&quot;] }, &quot;authors&quot; : [&quot;Thomas&quot;, &quot;Peter&quot;], &quot;text&quot;: &quot;This is the document text!&quot; }&#xA;{ &quot;docId&quot; : &quot;6&quot;, &quot;links&quot; : { &quot;a link&quot;: [&quot;endpoint 1&quot;, &quot;endpoint 2&quot;], &quot;another link&quot;: [&quot;endpoint 3&quot;] }, &quot;authors&quot; : [&quot;Thomas&quot;, &quot;Peter&quot;], &quot;text&quot;: &quot;This is the document text!&quot; }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="826983" LastEditorUserId="826983" LastEditDate="2018-03-07T15:45:00.677" LastActivityDate="2018-03-07T16:11:24.830" Title="How to load JSON objects with nested lists and dictionaries?" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="8" />
  <row Id="49154726" PostTypeId="1" CreationDate="2018-03-07T14:49:33.387" Score="0" ViewCount="8" Body="&lt;p&gt;I have a pyspark program that takes 2 hours to run in &quot;local mode&quot;. I am running it in a EC2 instance in AWS. Does it run faster in &quot;stand alone mode&quot;? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;In local mode, even I set the driver memory to 30 GB, it only uses around 15 GB while it runs. All CPU cores are 100% though. If I run in stand along mode, does it require to open some ports? I might not be able to do that since it is managed by someone else. I have searched but have not find any info about performance difference between local and stand alone mode. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;" OwnerUserId="5236286" LastActivityDate="2018-03-07T14:49:33.387" Title="pyspark performance in local mode and stand alone mode" Tags="&lt;pyspark&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49155378" PostTypeId="1" CreationDate="2018-03-07T15:18:51.737" Score="-2" ViewCount="31" Body="&lt;p&gt;I am trying to use col, mean and avg functions to replace null value but I could not found the col, mean, avg in pyspark.sql.functions. I am using spark 2.2. &#xA;My code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark.sql.functions import col, mean, avg, when&#xA;&#xA;gbl_top= hiveContext.sql(&quot;SELECT a.*, ntile(10) OVER (ORDER BY a.margin) as decile from apj_xxx a&quot;)&#xA;&#xA;remove_na_gbl_top= gbl_top.na.drop()&#xA;imputeDF = gbl_top&#xA;    for y in imputeDF.columns:&#xA;        meanValue = remove_na_gbl_top.agg(avg(y).first()[0])&#xA;        print (y, meanValue)&#xA;        imputeDF = imputeDF.na.fill(meanValue, [y])&#xA;     gbl_top.describe(['total_rev']).show()&#xA;     imputeDF.describe(['total_rev']).show()&#xA;&#xA;Error:&#xA; meanValue = remove_na_gbl_top.agg(avg(y).first()[0])&#xA;TypeError: 'Column' object is not callable&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Please help me out.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Ouput of  gbl_top.show(2, False):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;|12.044372913 |12.884042122     |593975.9366|sls_5          |GENERAL SERVICES|STATE, LOCAL &amp;amp; EDUCA|PUBSEC &amp;amp; ED         |Before_1970  |0      |0     |0       |0          |1     |0      |0    |310            |3           |0           |0           |36          |11              |9           |9           |36       |1       |18.0         |0.0          |0.0          |1.0285714286 |3.6              |4.5          |3.5          |1.0       |10.596295153|12.29689848   |0.0       |12.072313261    |8.3262336343  |11.028238608  |0.0         |11.809683204|11.205711927 |10.514569629 |11.368023901|10.748296936|11.366534401|11.660771126|11.680578876|11.981620079|3.6109179126|39985.4191|219014.6528|0.0  |174959.1059|4129.83  |134547.93|73548.3528|36847.4793|86509.7447|46549.6821|86380.9828|115932.3952|118251.6689|159789.6946|1       |1     |1     |0      |0    |0      |1    |1    |1.2518048242|0.4440882217  |0.219255676|10943.715519|36     |0                |0                |0    |0      |0    |0    |1  |61588.0028|0.0       |0    |0      |0    |0    |0    |0      |0    |35    |36            |0             |0             |36            |36                |36            |28            |0                 |36                |36                |0                 |0                     |0                 |8                 |979    |3                 |5              |3                 |1  |1  |1  |750022675|179183861  |345        |0        |33116.45    |&#xA;|10.842471632 |12.88491179      |411476.9351|sls_5          |UNCLASSIFIED    |RETAIL &amp;amp; WHOLESALE  |MFG &amp;amp; DIST          |Before_1970  |0      |0     |0       |0          |1     |0      |0    |51             |1           |0           |1           |36          |0               |0           |0           |36       |1       |0.0          |0.0          |0.0          |1.0285714286 |0.0              |0.0          |0.0          |1.0       |9.493625846 |12.526609606  |0.0       |11.684734037    |8.957182102   |8.9562645632  |9.8122777965|12.801574389|10.379252367 |10.270017773 |0.0         |0.0         |0.0         |12.298448023|11.456258337|11.29568185 |3.6109179126|13273.8407|275572.4851|0.0  |118744.05  |7762.45  |362787.17|32183.89  |28853.4   |0.0       |0.0       |0.0       |219354.29  |94489.8551 |80472.39   |1       |1     |1     |0      |1    |0      |1    |1    |2.3207266815|0.301138906   |0.0        |10953.237086|36     |0                |0                |0    |0      |0    |0    |0  |7755.33   |18255.5244|0    |0      |0    |0    |0    |0      |0    |35    |21            |0             |21            |36            |0                 |0             |0             |15                |36                |15                |0                 |36                    |36                |36                |49418  |2                 |4              |3                 |1  |1  |1  |704524917|23432412960|213        |0        |78061.51    |&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="4937851" LastEditorUserId="4937851" LastEditDate="2018-03-07T16:55:17.420" LastActivityDate="2018-03-07T16:55:17.420" Title="SQL functions in pyspark - col, mean, avg" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;&lt;apache-spark-sql&gt;" AnswerCount="0" CommentCount="8" />
  <row Id="49155388" PostTypeId="1" CreationDate="2018-03-07T15:19:16.943" Score="0" ViewCount="21" Body="&lt;p&gt;I have a python script which is dependent on another file, which is also essential for other scripts, so i have zipped it and shipped it to run as a spark-submit job, but unfortunately it seems not to be working, here is my code snippet and the error i'm getting all the time&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark import SparkConf, SparkContext&#xA;from pyspark.sql.session import SparkSession&#xA;def main(spark):&#xA;    employee = spark.read.json(&quot;/storage/hadoop/hadoop-3.0.0/bin/employees.json&quot;)&#xA;    # employee = spark.read.json(&quot;/storage/hadoop/hadoop-3.0.0/bin/employee.json&quot;)&#xA;    employee.printSchema()&#xA;    employee.show()&#xA;    people = spark.read.json(&quot;/storage/hadoop/hadoop-3.0.0/bin/people.json&quot;)&#xA;    people.printSchema()&#xA;    people.show()&#xA;    employee.createOrReplaceTempView(&quot;employee&quot;)&#xA;    people.createOrReplaceTempView(&quot;people&quot;)&#xA;    newDataFrame = employee.join(people,(employee.name==people.name),how=&quot;inner&quot;)&#xA;    newDataFrame.distinct().show()&#xA;    return &quot;Hello I'm Done Processing the Operation&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;which is the external dependencies called by other modules as well, and here is another script which is trying to execute the file&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark import SparkConf, SparkContext&#xA;from pyspark.sql.session import SparkSession&#xA;&#xA;def sampleTest(output):&#xA;    print output&#xA;&#xA;if __name__ == &quot;__main__&quot;:&#xA;    #Application Name for the Spark RDD using Python&#xA;# APP_NAME = &quot;Spark Application&quot;&#xA;     spark = SparkSession \&#xA;    .builder \&#xA;    .appName(&quot;Spark Application&quot;) \&#xA;    .config(&quot;spark.master&quot;, &quot;spark://192.168.2.3:7077&quot;) \&#xA;    .getOrCreate()&#xA;&#xA;    # .config() \&#xA;import SparkFileMerge    &#xA;abc = SparkFileMerge.main(spark)&#xA;sampleTest(abc)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;now when i'm executing the command&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;./spark-submit --py-files /home/varun/SparkPythonJob.zip /home/varun/main.py&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;it is giving me the following error. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;/home/varun/main.py&quot;, line 18, in &amp;lt;module&amp;gt;&#xA;    from SparkFileMerge import SparkFileMerge&#xA;ImportError: No module named SparkFileMerge&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;any help will be highly appreciated.&lt;/p&gt;&#xA;" OwnerUserId="7160479" LastActivityDate="2018-03-08T09:53:15.273" Title="Failed to Import External Dependency in Spark" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;spark-dataframe&gt;&lt;spark-submit&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49156100" PostTypeId="1" CreationDate="2018-03-07T15:53:06.163" Score="0" ViewCount="12" Body="&#xA;&#xA;&lt;p&gt;I have a Spark DataFrame with column titles &lt;code&gt;['tsmID','sigID','Timestamp', 'Value']&lt;/code&gt;. I would like to create a user defined function, such that it compares the measurement in the &lt;code&gt;Value&lt;/code&gt;column to an item looked up in a dictionary passed into the function. The comparison should return a &lt;code&gt;BooleanType()&lt;/code&gt; and store it in a new column called &lt;code&gt;Constraint?&lt;/code&gt;. My code is as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;def checkConstraints(meas, cm, sigID):&#xA;    entry = cm[str(sigID)]&#xA;    hi = entry['Upper Bound']&#xA;    lo = entry['Lower Bound']&#xA;    if meas &amp;gt; hi or meas &amp;lt; lo:&#xA;        return True&#xA;    else:&#xA;        return False&#xA;&#xA;if __name__ == &quot;__main__&quot;:&#xA;    sc = SparkContext(appName=&quot;RTSGtemporalParallel&quot;)&#xA;    sqlContext = SQLContext(sc)&#xA;    df = sqlContext.read.load(sys.argv[1],format='com.databricks.spark.csv',header='true',inferSchema='true')&#xA;    cm = createConstraintMap(sys.argv[2])&#xA;    df = df.withColumn('Constraint?',constraintChecker(df['Value'], cm, df['SignalID']))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;After executing this, however, I receive the following error:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;py4j.protocol.Py4JError: An error occurred while calling z:org.apache.spark.sql.functions.col. Trace:&#xA;py4j.Py4JException: Method col([class java.util.HashMap]) does not exist&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;From reading the error, I assume this has something to do with passing in the column as an argument. Is there a work-around to achieving the same desired result?&lt;/p&gt;&#xA;" OwnerUserId="6650960" LastEditorUserId="5858851" LastEditDate="2018-03-07T15:55:36.113" LastActivityDate="2018-03-07T15:55:36.113" Title="How to reference DataFrame row value in Spark SQL user-defined function?" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;&lt;apache-spark-sql&gt;&lt;pyspark-sql&gt;" AnswerCount="0" CommentCount="1" ClosedDate="2018-03-07T15:56:20.650" />
  <row Id="49156151" PostTypeId="1" CreationDate="2018-03-07T15:55:33.610" Score="1" ViewCount="19" Body="&lt;p&gt;This is Apache Spark on Windows.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the need of &lt;code&gt;\tmp\hive&lt;/code&gt; while changing the permission with &lt;code&gt;winutils.exe&lt;/code&gt;?  Can I change this path to a random temporary directory?&lt;/p&gt;&#xA;" OwnerUserId="4319700" LastEditorUserId="1305344" LastEditDate="2018-03-08T15:32:11.087" LastActivityDate="2018-03-08T15:32:11.087" Title="What is \tmp\hive for in Spark SQL (esp. while setting it up with winutils.exe)?" Tags="&lt;windows&gt;&lt;apache-spark&gt;&lt;hive&gt;&lt;apache-spark-sql&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49156187" PostTypeId="1" AcceptedAnswerId="49165097" CreationDate="2018-03-07T15:57:22.887" Score="0" ViewCount="40" Body="&lt;p&gt;I have loaded the following data into DataFrame as below&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;S.NO  routename        status      tripcount&#xA;&#xA;1      East           STARTED      1&#xA;2      West           STARTED      1&#xA;4      East           ARRIVED      2&#xA;5      East           ARRIVED      3&#xA;6      East           STARTED      4 &#xA;7      East           STARTED      5 &#xA;8      West           ARRIVED      2&#xA;9      East           ARRIVED      6&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to take out only the following rows&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1, 2, 4, 6, 8, 9&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically STARTED - ARRIVED base rest of them I want to skip. Now I have loaded &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;dataframe_mysql.select(&quot;routename&quot;).distinct().show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With this should I have to loop inside lambda expression or is there any other inbuilt method will help me to get the result.&lt;/p&gt;&#xA;" OwnerUserId="903521" LastActivityDate="2018-03-08T09:47:16.157" Title="Extracting group data PySpark SQL" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="6" />
  <row Id="49156515" PostTypeId="1" CreationDate="2018-03-07T16:12:21.560" Score="1" ViewCount="30" Body="&lt;h1&gt;Framing the question&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;This question stems from the following problem:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to test a spark structured streaming [2.2.X or 2.3.x] application that reads its input from kafka (without a &lt;code&gt;from beginning&lt;/code&gt; flag).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The app essentially reads like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val sparkSession = SparkSession.builder.getOrCreate()&#xA;val lines = sparkSession&#xA;                .readStream&#xA;                .format(&quot;kafka&quot;)&#xA;                .option(&quot;kafka.bootstrap.servers&quot;,&quot;localhost:9092&quot;)&#xA;                .option(&quot;subscribe&quot;, &quot;test&quot;)&#xA;                .load()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Once the app is started and running, it may take an arbitrary amount of time for it to start listening to the kafka topic.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;How can I post the input data to kafka, after waiting the least time possible?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Naive solution&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;A simple solution to the problem would be to wait a large arbitrary amount of time after starting the app:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;startApplication()&#xA;Thread.sleep(10*1000)&#xA;postInputDataToKafka()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is problematic on 2 accounts:&lt;br&gt;&#xA; - Not all environments are equal, and some may take longer than you expected&lt;br&gt;&#xA; - It's wasteful&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Complex solution&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;Another option would be to use a &lt;em&gt;global supervisor&lt;/em&gt;, meaning, to have some process that coordinates the test.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Meaning, the same process that starts the application waits to receive a signal from it that it's ready to listen. After this signal is received, it then starts posting the input data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This approach requires the application to send such a signal, my question is how to do so.&lt;/p&gt;&#xA;" OwnerUserId="3517025" LastEditorUserId="3517025" LastEditDate="2018-03-07T19:33:50.850" LastActivityDate="2018-03-08T13:18:36.133" Title="How can a spark structured streaming [2.2.x or 2.3.x] application signal that it is ready to consume from a kafka topic" Tags="&lt;apache-spark&gt;&lt;apache-kafka&gt;&lt;structured-streaming&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="49157052" PostTypeId="1" CreationDate="2018-03-07T16:41:16.187" Score="0" ViewCount="47" Body="&lt;p&gt;I want to perform a transformation on my DataFrame &lt;code&gt;df&lt;/code&gt; so that I only have each key once and only once in the final DataFrame.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For machine learning purposes, I don't want to have a bias in my dataset. This should never occur, but the data I get from my data source contains this &quot;weirdness&quot;. So if I have lines with the same keys, I want to be able to chose either a combination of the two (like mean value) or a string concatenation (labels for example) or a random values set.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Say my DataFrame &lt;code&gt;df&lt;/code&gt; looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+---+----+-----------+---------+&#xA;|ID1| ID2|       VAL1|     VAL2|&#xA;+---+----+-----------+---------+&#xA;|  A|   U|     PIERRE|        1|&#xA;|  A|   U|     THOMAS|        2|&#xA;|  A|   U|    MICHAEL|        3|&#xA;|  A|   V|        TOM|        2|&#xA;|  A|   V|       JACK|        3|&#xA;|  A|   W|     MICHEL|        2|&#xA;|  A|   W|     JULIEN|        3|&#xA;+---+----+-----------+---------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want my final DataFrame &lt;code&gt;out&lt;/code&gt; to only keep one set of values per key, randomly. It could be another type of aggregation (say the concatenation of all values as a string) but I just don't want to build an Integer value from it, rather build new entries.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Eg. a final output could be (keeping only the first row per key):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+---+----+-----------+---------+&#xA;|ID1| ID2|       VAL1|     VAL2|&#xA;+---+----+-----------+---------+&#xA;|  A|   U|     PIERRE|        1|&#xA;|  A|   V|        TOM|        2|&#xA;|  A|   W|     MICHEL|        2|&#xA;+---+----+-----------+---------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Another final output could be (keeping a random row per key):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+---+----+-----------+---------+&#xA;|ID1| ID2|       VAL1|     VAL2|&#xA;+---+----+-----------+---------+&#xA;|  A|   U|    MICHAEL|        3|&#xA;|  A|   V|       JACK|        3|&#xA;|  A|   W|     MICHEL|        2|&#xA;+---+----+-----------+---------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Or, building a new set of values:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+---+----+--------------------------+----------+&#xA;|ID1| ID2|                      VAL1|      VAL2|&#xA;+---+----+--------------------------+----------+&#xA;|  A|   U| (PIERRE, THOMAS, MICHAEL)| (1, 2, 3)|&#xA;|  A|   V|               (TOM, JACK)|    (2, 3)|&#xA;|  A|   W|          (MICHEL, JULIEN)|    (2, 3)|&#xA;+---+----+--------------------------+----------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The answer should use Spark with Scala. I also want to underline that the actual schema is way more complicated than that and I would like to reach a generic solution. Also, I &lt;strong&gt;do not&lt;/strong&gt; want to fetch only unique values from one column but filter out lines that have same keys. Thanks!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt; This is what I tried to do (but &lt;code&gt;Row.get(colname)&lt;/code&gt; throws a &lt;code&gt;NoSuchElementException: key not found...&lt;/code&gt;):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  def myDropDuplicatesRandom(df: DataFrame, colnames: Seq[String]): DataFrame = {&#xA;    val fields_map: Map[String, (Int, DataType)] =&#xA;      df.schema.fieldNames.map(fname =&amp;gt; {&#xA;        val findex = df.schema.fieldIndex(fname)&#xA;        val ftype = df.schema.fields(findex).dataType&#xA;        (fname, (findex, ftype))&#xA;      }).toMap[String, (Int, DataType)]&#xA;&#xA;    df.sparkSession.createDataFrame(&#xA;      df.rdd&#xA;        .map[(String, Row)](r =&amp;gt; (colnames.map(colname =&amp;gt; r.get(fields_map(colname)._1).toString.replace(&quot;`&quot;, &quot;&quot;)).reduceLeft((x, y) =&amp;gt; &quot;&quot; + x + y), r))&#xA;        .groupByKey()&#xA;        .map{case (x: String, y: Iterable[Row]) =&amp;gt; Utils.randomElement(y)}&#xA;    , df.schema)&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="7115301" LastEditorUserId="7115301" LastEditDate="2018-03-08T10:27:16.310" LastActivityDate="2018-03-08T10:27:16.310" Title="Homemade DataFrame aggregation/dropDuplicates Spark" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;spark-dataframe&gt;&lt;rdd&gt;" AnswerCount="2" CommentCount="11" />
  <row Id="49157068" PostTypeId="1" AcceptedAnswerId="49157129" CreationDate="2018-03-07T16:41:53.093" Score="2" ViewCount="43" Body="&lt;p&gt;I have a scala spark DataFrame:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df.select($&quot;row_id&quot;, $&quot;array_of_data&quot;).show&#xA;+----------+--------------------+&#xA;| row_id   |      array_of_data |&#xA;+----------+--------------------+&#xA;|       0  | [123, ABC, G12]    |&#xA;|       1  | [100, 410]         |&#xA;|       2  | [500, 300,  ...]   |&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to explode those arrays so that each element is in a different row, but I also want to mark which row corresponds to the first element of the array:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+----------+--------------------+----------+----------+&#xA;| row_id   |      array_of_data | exploded | is_first |&#xA;+----------+--------------------+----------+----------+&#xA;|       0  | [123, ABC, G12]    | 123      |    Yes   |&#xA;|       0  | [123, ABC, G12]    | ABC      |    No    |&#xA;|       0  | [123, ABC, G12]    | G12      |    No    |&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;To achieve this, I use the &lt;a href=&quot;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$&quot; rel=&quot;nofollow noreferrer&quot;&gt;explode function&lt;/a&gt;, and hope that the first row corresponds to the first data element:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;var exploded_df = df.withColumn(&quot;exploded&quot;, explode($&quot;array_of_data&quot;))&#xA;&#xA;val window = Window.partitionBy(&quot;row_id&quot;).orderBy(&quot;row_id&quot;)&#xA;// Create an internal rank variable to figure out the first element&#xA;exploded_df = exploded_df.withColumn(&quot;_rank&quot;, row_number().over(window))&#xA;exploded_df = exploded_df.withColumn(&quot;is_first&quot;,&#xA;                                     when(($&quot;_rank&quot; === 1), &quot;Yes&quot;).otherwise(&quot;No&quot;)&#xA;                                    )&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This appears to work for my purposes and produces the desired output, but can I trust that this will always work? I can't find anywhere in the explode documentation that promises this behavior, and it seems unwise to trust the order of rows in a spark dataframe.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only other solution I could think of would be to create a new column for every element in &lt;code&gt;array_of_data&lt;/code&gt; and then match when &lt;code&gt;exploded&lt;/code&gt; is equal to the value from the first column, but I don't have a guarantee that there wont be duplicate values in the array.&lt;/p&gt;&#xA;" OwnerUserId="1748679" LastActivityDate="2018-03-07T17:22:48.417" Title="Does the SparkSQL Dataframe function explode preserve order?" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49157593" PostTypeId="1" CreationDate="2018-03-07T17:09:45.537" Score="-2" ViewCount="30" Body="&#xA;&#xA;&lt;p&gt;Code:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;new_words = words_data.withColumn(&#xA;    'finewords',&#xA;    [x for x in words_data['Words'] if x.startswith('http')!=True]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;Error:&#xA;    239     def __iter__(self):&#xA;--&amp;gt; 240         raise TypeError(&quot;Column is not iterable&quot;)&#xA;    241 &#xA;    242     # string methods&#xA;&#xA;TypeError: Column is not iterable&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="9453140" LastEditorUserId="5858851" LastEditDate="2018-03-07T17:53:46.120" LastActivityDate="2018-03-07T17:53:46.120" Title="PySpark 'TypeError: Column is not iterable' when trying to filter list of words using startswith()" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="9" />
  <row Id="49157944" PostTypeId="1" CreationDate="2018-03-07T17:29:57.617" Score="0" ViewCount="26" Body="&lt;p&gt;I am new Spark and Kafka. I have done Spark and Kafka setup done in Windows system. Both are working good. I have followed below mentioned tutorial and executing the Scala code in Spark-shell and getting below mentioned error.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anybody help me on how to listen Kafka stream using Spark in Scala.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Spark : 2.2&#xA;Kafka : 2.12&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;http://www.godatafy.com/poc/streaming-with-spark-kafka/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.godatafy.com/poc/streaming-with-spark-kafka/&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;spark-shell -jars ..\jars\spark-streaming-kafka_2.11-1.6.3.jar&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;scala&amp;gt; import org.apache.spark.SparkConf&#xA;import org.apache.spark.SparkConf&#xA;&#xA;scala&amp;gt; import org.apache.spark.streaming.StreamingContext&#xA;import org.apache.spark.streaming.StreamingContext&#xA;&#xA;scala&amp;gt; import org.apache.spark.streaming.Seconds&#xA;import org.apache.spark.streaming.Seconds&#xA;&#xA;scala&amp;gt; import org.apache.spark.streaming.kafka.KafkaUtils&#xA;import org.apache.spark.streaming.kafka.KafkaUtils&#xA;&#xA;scala&amp;gt; sc.stop&#xA;&#xA;scala&amp;gt; val sparkConf = new SparkConf().setAppName(&quot;KafkaWordCount&quot;).setMaster(&quot;local[2]&quot;)&#xA;sparkConf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@427c2c96&#xA;&#xA;scala&amp;gt; val ssc = new StreamingContext(sparkConf, Seconds(2))&#xA;ssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@1fd73dcb&#xA;&#xA;scala&amp;gt; val lines = KafkaUtils.createStream(ssc, &quot;localhost:2181&quot;, &quot;spark-streaming-consumer-group&quot;, Map(&quot;test&quot; -&amp;gt; 5))&#xA;error: missing or invalid dependency detected while loading class file 'KafkaUtils.class'.&#xA;Could not access term kafka in package &amp;lt;root&amp;gt;,&#xA;because it (or its dependencies) are missing. Check your build definition for&#xA;missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.)&#xA;A full rebuild may help if 'KafkaUtils.class' was compiled against an incompatible version of &amp;lt;root&amp;gt;.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="3714465" LastActivityDate="2018-03-07T18:24:38.803" Title="Spark Kafka Stream error" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;apache-kafka&gt;&lt;spark-streaming&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="49158612" PostTypeId="1" AcceptedAnswerId="49158999" CreationDate="2018-03-07T18:09:19.053" Score="1" ViewCount="25" Body="&lt;p&gt;It looks like I am again stuck on the running a packaged spark app jar using spark submit. Following is my pom file:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&#xA;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&amp;gt;&#xA;    &amp;lt;parent&amp;gt;&#xA;        &amp;lt;artifactId&amp;gt;oneview-forecaster&amp;lt;/artifactId&amp;gt;&#xA;        &amp;lt;groupId&amp;gt;com.dataxu.oneview.forecast&amp;lt;/groupId&amp;gt;&#xA;        &amp;lt;version&amp;gt;1.0.0-SNAPSHOT&amp;lt;/version&amp;gt;&#xA;    &amp;lt;/parent&amp;gt;&#xA;    &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;forecaster&amp;lt;/artifactId&amp;gt;&#xA;&#xA;&amp;lt;dependencies&amp;gt;&#xA;    &amp;lt;dependency&amp;gt;&#xA;        &amp;lt;groupId&amp;gt;com.fasterxml.jackson.core&amp;lt;/groupId&amp;gt;&#xA;        &amp;lt;artifactId&amp;gt;jackson-databind&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;/dependency&amp;gt;&#xA;    &amp;lt;dependency&amp;gt;&#xA;        &amp;lt;groupId&amp;gt;com.fasterxml.jackson.module&amp;lt;/groupId&amp;gt;&#xA;        &amp;lt;artifactId&amp;gt;jackson-module-scala_${scala.binary.version}&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;/dependency&amp;gt;&#xA;    &amp;lt;dependency&amp;gt;&#xA;        &amp;lt;groupId&amp;gt;org.scala-lang&amp;lt;/groupId&amp;gt;&#xA;        &amp;lt;artifactId&amp;gt;scala-library&amp;lt;/artifactId&amp;gt;&#xA;        &amp;lt;version&amp;gt;${scala.version}&amp;lt;/version&amp;gt;&#xA;    &amp;lt;/dependency&amp;gt;&#xA;    &amp;lt;dependency&amp;gt;&#xA;        &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&#xA;        &amp;lt;artifactId&amp;gt;spark-streaming_${scala.binary.version}&amp;lt;/artifactId&amp;gt;&#xA;        &amp;lt;version&amp;gt;${spark.version}&amp;lt;/version&amp;gt;&#xA;        &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;&#xA;    &amp;lt;/dependency&amp;gt;&#xA;    &amp;lt;dependency&amp;gt;&#xA;        &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&#xA;        &amp;lt;artifactId&amp;gt;spark-sql_${scala.binary.version}&amp;lt;/artifactId&amp;gt;&#xA;        &amp;lt;version&amp;gt;${spark.version}&amp;lt;/version&amp;gt;&#xA;        &amp;lt;!--&amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;--&amp;gt;&#xA;    &amp;lt;/dependency&amp;gt;&#xA;    &amp;lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-hive --&amp;gt;&#xA;    &amp;lt;dependency&amp;gt;&#xA;        &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&#xA;        &amp;lt;artifactId&amp;gt;spark-hive_2.11&amp;lt;/artifactId&amp;gt;&#xA;        &amp;lt;version&amp;gt;2.2.0&amp;lt;/version&amp;gt;&#xA;        &amp;lt;!--&amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;--&amp;gt;&#xA;    &amp;lt;/dependency&amp;gt;&#xA;    &amp;lt;dependency&amp;gt;&#xA;        &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;&#xA;        &amp;lt;artifactId&amp;gt;hadoop-aws&amp;lt;/artifactId&amp;gt;&#xA;        &amp;lt;version&amp;gt;2.8.3&amp;lt;/version&amp;gt;&#xA;        &amp;lt;!--&amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;--&amp;gt;&#xA;    &amp;lt;/dependency&amp;gt;&#xA;    &amp;lt;dependency&amp;gt;&#xA;        &amp;lt;groupId&amp;gt;com.amazonaws&amp;lt;/groupId&amp;gt;&#xA;        &amp;lt;artifactId&amp;gt;aws-java-sdk&amp;lt;/artifactId&amp;gt;&#xA;        &amp;lt;version&amp;gt;1.10.60&amp;lt;/version&amp;gt;&#xA;    &amp;lt;/dependency&amp;gt;&#xA;    &amp;lt;!-- https://mvnrepository.com/artifact/joda-time/joda-time --&amp;gt;&#xA;    &amp;lt;dependency&amp;gt;&#xA;        &amp;lt;groupId&amp;gt;joda-time&amp;lt;/groupId&amp;gt;&#xA;        &amp;lt;artifactId&amp;gt;joda-time&amp;lt;/artifactId&amp;gt;&#xA;        &amp;lt;version&amp;gt;2.9.9&amp;lt;/version&amp;gt;&#xA;    &amp;lt;/dependency&amp;gt;&#xA;    &amp;lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&amp;gt;&#xA;    &amp;lt;dependency&amp;gt;&#xA;        &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;&#xA;        &amp;lt;artifactId&amp;gt;hadoop-common&amp;lt;/artifactId&amp;gt;&#xA;        &amp;lt;version&amp;gt;2.8.0&amp;lt;/version&amp;gt;&#xA;        &amp;lt;!--&amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;--&amp;gt;&#xA;    &amp;lt;/dependency&amp;gt;&#xA;&amp;lt;/dependencies&amp;gt;&#xA;&#xA;&amp;lt;build&amp;gt;&#xA;    &amp;lt;sourceDirectory&amp;gt;src/main/scala&amp;lt;/sourceDirectory&amp;gt;&#xA;    &amp;lt;testSourceDirectory&amp;gt;src/test/scala&amp;lt;/testSourceDirectory&amp;gt;&#xA;    &amp;lt;plugins&amp;gt;&#xA;        &amp;lt;plugin&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;net.alchim31.maven&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;scala-maven-plugin&amp;lt;/artifactId&amp;gt;&#xA;            &amp;lt;version&amp;gt;${scala-maven-plugin.version}&amp;lt;/version&amp;gt;&#xA;            &amp;lt;executions&amp;gt;&#xA;                &amp;lt;execution&amp;gt;&#xA;                    &amp;lt;goals&amp;gt;&#xA;                        &amp;lt;goal&amp;gt;compile&amp;lt;/goal&amp;gt;&#xA;                        &amp;lt;goal&amp;gt;testCompile&amp;lt;/goal&amp;gt;&#xA;                    &amp;lt;/goals&amp;gt;&#xA;                &amp;lt;/execution&amp;gt;&#xA;            &amp;lt;/executions&amp;gt;&#xA;        &amp;lt;/plugin&amp;gt;&#xA;        &amp;lt;plugin&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;maven-assembly-plugin&amp;lt;/artifactId&amp;gt;&#xA;            &amp;lt;configuration&amp;gt;&#xA;                &amp;lt;archive&amp;gt;&#xA;                    &amp;lt;manifest&amp;gt;&#xA;                        &amp;lt;mainClass&amp;gt;com.dataxu.oneview.forecaster.App&amp;lt;/mainClass&amp;gt;&#xA;                    &amp;lt;/manifest&amp;gt;&#xA;                &amp;lt;/archive&amp;gt;&#xA;                &amp;lt;descriptorRefs&amp;gt;&#xA;                    &amp;lt;descriptorRef&amp;gt;jar-with-dependencies&amp;lt;/descriptorRef&amp;gt;&#xA;                &amp;lt;/descriptorRefs&amp;gt;&#xA;            &amp;lt;/configuration&amp;gt;&#xA;            &amp;lt;executions&amp;gt;&#xA;                &amp;lt;execution&amp;gt;&#xA;                    &amp;lt;id&amp;gt;make-assembly&amp;lt;/id&amp;gt;&#xA;                    &amp;lt;phase&amp;gt;package&amp;lt;/phase&amp;gt;&#xA;                    &amp;lt;goals&amp;gt;&#xA;                        &amp;lt;goal&amp;gt;single&amp;lt;/goal&amp;gt;&#xA;                    &amp;lt;/goals&amp;gt;&#xA;                &amp;lt;/execution&amp;gt;&#xA;            &amp;lt;/executions&amp;gt;&#xA;        &amp;lt;/plugin&amp;gt;&#xA;    &amp;lt;/plugins&amp;gt;&#xA;&amp;lt;/build&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Following is a simple snippet of code which fetches data from s3 location and prints it:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def getS3Data(path: String): Map[String, Any] = {&#xA;    println(&quot;spark session start.........&quot;)&#xA;    val spark =  getSparkSession()&#xA;&#xA;    val configTxt = spark.sparkContext.textFile(path)&#xA;        .collect().reduce(_ + _)&#xA;&#xA;    val mapper = new ObjectMapper&#xA;    mapper.registerModule(DefaultScalaModule)&#xA;    mapper.readValue(configTxt, classOf[Map[String, String]])&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I run it from intellij, everything works fine. the log is clear and looks good. However, when I package it using mvn package and try to run it using spark submit, I end up getting the following error at the &lt;code&gt;.collect.reduce(_ + _)&lt;/code&gt;. Following is the error I encounter:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; &quot;main&quot; java.lang.NoSuchMethodError: org.apache.hadoop.conf.Configuration.reloadExistingConfigurations()V&#xA;at org.apache.hadoop.fs.s3a.S3AFileSystem.addDeprecatedKeys(S3AFileSystem.java:181)&#xA;at org.apache.hadoop.fs.s3a.S3AFileSystem.&amp;lt;clinit&amp;gt;(S3AFileSystem.java:185)&#xA;at java.lang.Class.forName0(Native Method)&#xA;at java.lang.Class.forName(Class.java:348)&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am not understanding which dependency was not packaged or what might be the issue as I did set the versions correctly expecting the hadoop aws should have all of them.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help will be appreciated.&lt;/p&gt;&#xA;" OwnerUserId="4435214" LastActivityDate="2018-03-07T18:34:16.523" Title="java.lang.NoSuchMethodError: org.apache.hadoop.conf.Configuration.reloadExistingConfigurations()V" Tags="&lt;maven&gt;&lt;hadoop&gt;&lt;apache-spark&gt;&lt;intellij-idea&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49158779" PostTypeId="1" CreationDate="2018-03-07T18:19:59.910" Score="0" ViewCount="20" Body="&lt;p&gt;I have created a table in Spark by using the below commands in Spark &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; **&#xA;&#xA;case class trip(trip_id: String  , duration : String  , start_date : String , start_station : String  , start_terminal : String , end_date: String&#xA;     , end_station: String , end_terminal : String , bike : String , subscriber_type : String , zipcode : String )&#xA;&#xA; val trip_data = sc.textFile(&quot;/user/sankha087_gmail_com/trip_data.csv&quot;)&#xA;&#xA;     val tripDF = trip_data.map(x=&amp;gt; x.split(&quot;,&quot;)).filter(x=&amp;gt; (x(1)!= &quot;Duration&quot;)).map(x=&amp;gt; trip(x(0),x(1),x(2),x(3),x(4),x(5),x(6),x(7),x(8),x(9),&#xA;x(10))).toDF() &#xA;&#xA;tripDF.registerTempTable(&quot;tripdatas&quot;)&#xA;&#xA;sqlContext.sql(&quot;select * from tripdatas&quot;).show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;**&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I am running the above query (i.e. select *) , then I am getting desired result , but say if I run the below query , then I am getting the below exception :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sqlContext.sql(&quot;select count(1) from tripdatas&quot;).show() &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;&lt;em&gt;18/03/07 17:59:55 ERROR scheduler.TaskSetManager: Task 1 in stage 2.0 failed 4 times; aborting job&lt;br&gt;&#xA;org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.&#xA;0 (TID 6, datanode1-cloudera.mettl.com, executor 1): java.lang.ArrayIndexOutOfBoundsException: 10&lt;br&gt;&#xA;        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$3.apply(:31)&lt;br&gt;&#xA;        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$3.apply(:31)&lt;/em&gt;&lt;/strong&gt;   &lt;/p&gt;&#xA;" OwnerUserId="1670805" LastActivityDate="2018-03-07T19:57:13.020" Title="arrayindexoutofbound exception while executing sql query on apache spark" Tags="&lt;apache-spark&gt;&lt;apache-spark-sql&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49159012" PostTypeId="1" CreationDate="2018-03-07T18:35:05.830" Score="-1" ViewCount="26" Body="&lt;p&gt;Currently I have a Spark job that fetches messages from a Kafka topic and store this data into ElasticSearch. The problem that I am facing is that this data is coming in a two-way operation, meaning that I have a message with part of the data and after (unspecified amount of time), I will receive the other part of the data and I have to merge them together before storing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently I am using upsert with this goal, but I suppose that it would be much faster to aggregate this data in-memory using Spark, am I right? If so, how can I keep this data around in between fetches?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;That's a very generic question from someone starting with Spark, so I am trying to get the concept right.&lt;/p&gt;&#xA;" OwnerUserId="3995424" LastActivityDate="2018-03-07T18:35:05.830" Title="Aggregate Records by key using Spark before storing" Tags="&lt;apache-spark&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="49159559" PostTypeId="1" CreationDate="2018-03-07T19:10:26.990" Score="0" ViewCount="15" Body="&lt;p&gt;I am starting with pySpark. I have installed it in anadonda in Win10. I have copied an example and when I execute the code, I am getting this error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;File &quot;.\testingSpark.py&quot;, line 7, in &amp;lt;module&amp;gt;&#xA;spark = SparkSession.builder.master(&quot;local&quot;).getOrCreate()&#xA;File &quot;D:\Windows\Anaconda3\lib\site-packages\pyspark\sql\session.py&quot;, line 173, in getOrCreate&#xA;sc = SparkContext.getOrCreate(sparkConf)&#xA;File &quot;D:\Windows\Anaconda3\lib\site-packages\pyspark\context.py&quot;, line 331, in getOrCreate&#xA;SparkContext(conf=conf or SparkConf())&#xA;File &quot;D:\Windows\Anaconda3\lib\site-packages\pyspark\context.py&quot;, line 118, in __init__&#xA;conf, jsc, profiler_cls)&#xA;File &quot;D:\Windows\Anaconda3\lib\site-packages\pyspark\context.py&quot;, line 188, in _do_init&#xA;self._javaAccumulator = self._jvm.PythonAccumulatorV2(host, port)&#xA;TypeError: 'JavaPackage' object is not callable&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have read about it, but I can find nothing to fix this error. Please, help me!&lt;/p&gt;&#xA;" OwnerUserId="7824761" LastEditorUserId="7824761" LastEditDate="2018-03-07T19:26:02.067" LastActivityDate="2018-03-07T19:26:02.067" Title="'JavaPackage' object is not callable pyspark 2.3.0 Anaconda Win10" Tags="&lt;pyspark&gt;&lt;anaconda&gt;&lt;py4j&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49160329" PostTypeId="1" CreationDate="2018-03-07T20:04:03.780" Score="0" ViewCount="22" Body="&#xA;&#xA;&lt;p&gt;I'm trying to get http response codes for 25k links using PySpark (Databricks), but it is taking forever: 14 hours(!)&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;from pyspark.sql.functions import udf&#xA;&#xA;def response_api_call(url):&#xA;    try: &#xA;        r = requests.head(url, timeout = 30)&#xA;        response = r.status_code&#xA;        return response&#xA;    except:&#xA;        return &quot;Error&quot;&#xA;&#xA;response_api_call = udf(response_api_call, StringType())&#xA;&#xA;links_df = links_df.withColumn('responseCode', response_api_call(links_df.link))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there something wrong with my code, or is there an easy way to parallelize it/improve performance?&lt;/p&gt;&#xA;" OwnerUserId="8127354" LastEditorUserId="5858851" LastEditDate="2018-03-07T20:27:29.473" LastActivityDate="2018-03-07T20:27:29.473" Title="Improving HTTP requests performance - PySpark on Databricks (Requests module)" Tags="&lt;python&gt;&lt;pyspark&gt;&lt;http-headers&gt;&lt;python-requests&gt;&lt;databricks&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49160574" PostTypeId="1" AcceptedAnswerId="49170762" CreationDate="2018-03-07T20:22:47.477" Score="0" ViewCount="23" Body="&lt;p&gt;I have two columns in a spark sql dataframe with each entry in either column as an Array of strings. &#xA;&lt;a href=&quot;https://i.stack.imgur.com/r8wKh.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/r8wKh.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to merge the arrays in each row to make a single array in a new column. My code is as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def concat_array(firstarray: Array[String], &#xA;                 secondarray: Array[String]) : Array[String] = &#xA;                                     { (firstarray ++ secondarray).toArray }&#xA;val concatUDF = udf(concat_array _)&#xA;val concatFrame = ngramDataFrame.withColumn(&quot;full_array&quot;, concatUDF($&quot;filtered_words&quot;, $&quot;ngrams_array&quot;))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I can successfully use the concat_array function on two arrays. However when I run the above code, I get the following exception:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 12, localhost): org.apache.spark.SparkException: Failed to execute user defined function(anonfun$1: (array, array) =&gt; array) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370) at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:389) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47) at org.apache.spark.scheduler.Task.run(Task.scala:86) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.lang.ClassCastException: scala.collection.mutable.WrappedArray$ofRef cannot be cast to [Ljava.lang.String; at $line80.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(:76) ... 13 more Driver stacktrace:&lt;/p&gt;&#xA;" OwnerUserId="6381208" LastActivityDate="2018-03-08T10:24:02.800" Title="Merge two spark sql columns of type Array[string] into a new Array[string] column" Tags="&lt;scala&gt;&lt;apache-spark-sql&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49160594" PostTypeId="1" CreationDate="2018-03-07T20:24:05.843" Score="0" ViewCount="17" Body="&lt;p&gt;I got question. How I can copy dataframe without unload it again to redshift ?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val companiesData = spark.read.format(&quot;com.databricks.spark.redshift&quot;)&#xA;.option(&quot;url&quot;,&quot;jdbc:redshift://xxxx:5439/cf?user=&quot;+user+&quot;&amp;amp;password=&quot;+password)&#xA;.option(&quot;query&quot;,&quot;select * from cf_core.company&quot;)&#xA;//.option(&quot;dbtable&quot;,schema+&quot;.&quot;+table)&#xA;.option(&quot;aws_iam_role&quot;,&quot;arn:aws:iam::xxxxxx:role/somerole&quot;)&#xA;.option(&quot;tempdir&quot;,&quot;s3a://xxxxx/Spark&quot;)&#xA;.load()&#xA;&#xA;import class.companiesData&#xA;class test {&#xA;val secondDF = filteredDF(companiesData)&#xA;&#xA; def filteredDF(df: Dataframe): Dataframe {&#xA;   val result = df.select(&quot;companynumber&quot;)&#xA;  result&#xA; }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In this case this will unload data twice. First select * from table and second it will unload by select only companynumber. How I can unload data once and operate on this many times ? This is serious problem for me. Thanks for help&lt;/p&gt;&#xA;" OwnerUserId="6253052" LastEditorUserId="6253052" LastEditDate="2018-03-07T20:49:59.590" LastActivityDate="2018-03-08T00:17:34.007" Title="Spark - use dataframe many times without many unloads" Tags="&lt;scala&gt;&lt;amazon-web-services&gt;&lt;apache-spark&gt;&lt;amazon-redshift&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="49160736" PostTypeId="1" CreationDate="2018-03-07T20:34:39.783" Score="0" ViewCount="18" Body="&lt;p&gt;I have two dataframe, a small one with IDs and a large one (6 billion rows with id and trx_id). I want all the transactions from the large table that have the same customer ID as the small one. For example:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;df1:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+------+&#xA;|userid|&#xA;+------+&#xA;|   348|&#xA;|   567|&#xA;|   595|&#xA;+------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;df2:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+------+----------+&#xA;|userid|  trx_id  |&#xA;+------+----------+&#xA;|   348|       287|&#xA;|   595|       288|&#xA;|   348|       311|&#xA;|   276|       094|&#xA;|   595|       288|&#xA;|   148|       512|&#xA;|   122|       514|&#xA;|   595|       679|&#xA;|   567|       870|&#xA;|   595|       889|&#xA;+------+----------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Result I want:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+------+----------+&#xA;|userid|  trx_id  |&#xA;+------+----------+&#xA;|   348|       287|&#xA;|   595|       288|&#xA;|   348|       311|&#xA;|   595|       288|&#xA;|   595|       679|&#xA;|   567|       870|&#xA;|   595|       889|&#xA;+------+----------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Should I use a join or filter? If so which command?&lt;/p&gt;&#xA;" OwnerUserId="1871528" LastEditorUserId="7579547" LastEditDate="2018-03-08T03:45:25.867" LastActivityDate="2018-03-08T03:45:25.867" Title="spark filter columns from one column by column of another table" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;&lt;apache-spark-sql&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="49160909" PostTypeId="1" CreationDate="2018-03-07T20:46:58.730" Score="0" ViewCount="10" Body="&lt;p&gt;I am able to connect to oracle DB through spark and access single table at a time, but is it possible to hit a query to oracle DB consisting of multiple joins within tables and get an output.&lt;/p&gt;&#xA;" OwnerUserId="8434529" LastActivityDate="2018-03-07T20:46:58.730" Title="Is it possible to hit oracle database with multiple joins within the oracle tables" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="0" CommentCount="0" ClosedDate="2018-03-07T21:17:55.553" />
  <row Id="49160971" PostTypeId="1" CreationDate="2018-03-07T20:50:54.273" Score="2" ViewCount="11" Body="&lt;p&gt;I shut down a Spark StreamingContext with the following code. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Essentially a thread monitors for a boolean switch and then calls StreamingContext.stop(true,true)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Everything seems to process and all my data appears to have been collected. However, I get the following exception on shutdown.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can I ignore? It looks like there is potential for data loss.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;18/03/07 11:46:40 WARN ReceivedBlockTracker: Exception thrown while&#xA;  writing record: BatchAllocationEvent(1520452000000&#xA;  ms,AllocatedBlocks(Map(0 -&gt; ArrayBuffer()))) to the WriteAheadLog.&#xA;  java.lang.IllegalStateException: close() was called on&#xA;  BatchedWriteAheadLog before write request with time 1520452000001&#xA;  could be fulfilled.&#xA;          at org.apache.spark.streaming.util.BatchedWriteAheadLog.write(BatchedWriteAheadLog.scala:86)&#xA;          at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.writeToLog(ReceivedBlockTracker.scala:234)&#xA;          at org.apache.spark.streaming.scheduler.ReceivedBlockTracker.allocateBlocksToBatch(ReceivedBlockTracker.scala:118)&#xA;          at org.apache.spark.streaming.scheduler.ReceiverTracker.allocateBlocksToBatch(ReceiverTracker.scala:213)&#xA;          at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:248)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;The Thread&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;var stopScc=false&#xA;&#xA;private def stopSccThread(): Unit = {&#xA;val thread = new Thread {&#xA;  override def run {&#xA;&#xA;    var continueRun=true&#xA;    while (continueRun) {&#xA;      logger.debug(&quot;Checking status&quot;)&#xA;      if (stopScc == true) {&#xA;        getSparkStreamingContext(fieldVariables).stop(true, true)&#xA;        logger.info(&quot;Called Stop on Streaming Context&quot;)&#xA;        continueRun=false&#xA;&#xA;&#xA;      }&#xA;      Thread.sleep(50)&#xA;    }&#xA;  }&#xA;}&#xA;thread.start&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;em&gt;The Stream&lt;/em&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;@throws(classOf[IKodaMLException])&#xA;def startStream(ip: String, port: Int): Unit = {&#xA;&#xA;try {&#xA;  val ssc = getSparkStreamingContext(fieldVariables)&#xA;  ssc.checkpoint(&quot;./ikoda/cp&quot;)&#xA;&#xA;  val lines = ssc.socketTextStream(ip, port, StorageLevel.MEMORY_AND_DISK_SER)&#xA;  lines.print&#xA;&#xA;&#xA;  val lmap = lines.map {&#xA;    l =&amp;gt;&#xA;&#xA;      if (l.contains(&quot;IKODA_END_STREAM&quot;)) {&#xA;        stopScc = true&#xA;      }&#xA;      l&#xA;  }&#xA;&#xA;&#xA;  lmap.foreachRDD {&#xA;    r =&amp;gt;&#xA;      if (r.count() &amp;gt; 0) {&#xA;        logger.info(s&quot;RECEIVED: ${r.toString()} first: ${r.first().toString}&quot;)&#xA;        r.saveAsTextFile(&quot;./ikoda/test/test&quot;)&#xA;      }&#xA;      else {&#xA;        logger.info(&quot;Empty RDD. No data received&quot;)&#xA;      }&#xA;  }&#xA;  ssc.start()&#xA;&#xA;  ssc.awaitTermination()&#xA;}&#xA;catch {&#xA;  case e: Exception =&amp;gt;&#xA;    logger.error(e.getMessage, e)&#xA;    throw new IKodaMLException(e.getMessage, e)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="5082504" LastActivityDate="2018-03-07T20:50:54.273" Title="Spark Streaming: Exception thrown while &gt; writing record: BatchAllocationEvent" Tags="&lt;apache-spark&gt;&lt;spark-streaming&gt;" AnswerCount="0" CommentCount="0" FavoriteCount="1" />
  <row Id="49161356" PostTypeId="1" CreationDate="2018-03-07T21:18:11.763" Score="-1" ViewCount="14" Body="&lt;p&gt;Assuming I have the following class structure:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;trait Tag {&#xA;    def id: Int&#xA;}&#xA;case class StringTag(id: Int, value: String) extends Tag&#xA;case class ListTag(id: Int, values: Array[String]) extends Tag&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and a list of tags I'd like to convert to a &lt;code&gt;DataFrame&lt;/code&gt; as below:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val tags: Seq[Tag] = Seq(StringTag(8, &quot;FIX 4.2&quot;), ListTag(18, Array(&quot;G&quot;, &quot;M&quot;)))&#xA;Seq(tags).toDF(&quot;tags&quot;).show(false)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How do I supply an &lt;code&gt;Encoder&lt;/code&gt;? The code as it is gives an error &lt;code&gt;No Encoder found for testpkg.Tag&lt;/code&gt; - which makes sense, as &lt;code&gt;Tag&lt;/code&gt; is a trait, and is not implicitly encodable. Each concrete type is a case class, though, and should have an implicit &lt;code&gt;Encoder&lt;/code&gt; - but how would I go about conveying that information to Spark? Is it even possible?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The actual type system is more complex, but I simplified it to just two, figuring that if I can make the polymorphism to work with these two, I can later expand it to more complex types too.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT: I'm not interested in Kryo serialization for encoders, unless there's really no other option&lt;/p&gt;&#xA;&#xA;&lt;p&gt;EDIT2: another answer has been linked in the meantime, suggestion that there really is no other option but Kryo/Java serialization&lt;/p&gt;&#xA;" OwnerUserId="1825027" LastEditorUserId="1825027" LastEditDate="2018-03-07T21:22:57.170" LastActivityDate="2018-03-07T21:22:57.170" Title="Spark encoder for polymorphic types?" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;dataframe&gt;&lt;spark-dataframe&gt;" AnswerCount="0" CommentCount="0" ClosedDate="2018-03-07T21:20:17.487" />
  <row Id="49161383" PostTypeId="1" CreationDate="2018-03-07T21:20:13.107" Score="2" ViewCount="31" Body="&lt;p&gt;My XML file is&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&#xA;&amp;lt;Root1&amp;gt;&#xA;&amp;lt;Root2&amp;gt;&#xA;    &amp;lt;RowType1&amp;gt;&#xA;    &amp;lt;InsideRowType1&amp;gt;XXX&amp;lt;/InsideRowType1&amp;gt;&#xA;    &amp;lt;/RowType1&amp;gt; &#xA;    &amp;lt;RowType2&amp;gt;&#xA;        &amp;lt;InsideRowType2&amp;gt;&#xA;            &amp;lt;InnerElement&amp;gt;XXX&amp;lt;/InnerElement&amp;gt;&#xA;        &amp;lt;/InsideRowType2&amp;gt;&#xA;    &amp;lt;/RowType2&amp;gt;&#xA;    &amp;lt;RowType2&amp;gt;&#xA;        &amp;lt;InsideRowType2&amp;gt;XXX&amp;lt;/InsideRowType2&amp;gt;&#xA;    &amp;lt;/RowType2&amp;gt;&#xA;    &amp;lt;RowType2&amp;gt;&#xA;        &amp;lt;InsideRowType2&amp;gt;XXX&amp;lt;/InsideRowType2&amp;gt;&#xA;    &amp;lt;/RowType2&amp;gt;&#xA;&amp;lt;/Root2&amp;gt;&#xA;&amp;lt;/Root1&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I tried below code to parse it&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Encoder&amp;lt;POSLog&amp;gt; pOSLogEncoder = Encoders.kryo(POSLog.class);&#xA;Dataset&amp;lt;POSLog&amp;gt; df  = sparkSession.read().format(&quot;com.databricks.spark.xml&quot;).schema(**StructType Schema**).load(**File Path**).toDF().sqlContext().sql(&quot;set spark.sql.caseSensitive=false&quot;).as(pOSLogEncoder);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But it's throwing some error, then tried this code&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Dataset&amp;lt;Row&amp;gt; df  = sparkSession.read()&#xA;                .format(&quot;com.databricks.spark.xml&quot;)&#xA;                .schema(**StructType Schema**)&#xA;                .load(**File Path**)&#xA;                .toDF();&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But it returns null in &lt;code&gt;df.show()&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please help me out in parsing my xml. Also, please suggest if we can have other optimized way to parse it.&lt;/p&gt;&#xA;" OwnerUserId="2531361" LastEditorUserId="2531361" LastEditDate="2018-03-08T15:14:16.053" LastActivityDate="2018-03-08T15:14:16.053" Title="Spark with Java: I want to read complex xml file (mentioned below) and parse it POJO" Tags="&lt;java&gt;&lt;xml&gt;&lt;apache-spark&gt;" AnswerCount="0" CommentCount="0" FavoriteCount="1" />
  <row Id="49162097" PostTypeId="1" AcceptedAnswerId="49183949" CreationDate="2018-03-07T22:16:24.693" Score="1" ViewCount="28" Body="&lt;p&gt;DataFrame saveAsTable is persisting all the column values properly but insertInto function is not storing all the columns especially json data is truncated and sub-sequent column in not stored hive table.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Our Environment&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Spark 2.2.0 &lt;/li&gt;&#xA;&lt;li&gt;EMR 5.10.0 &lt;/li&gt;&#xA;&lt;li&gt;Scala 2.11.8&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;The sample data is &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; a8f11f90-20c9-11e8-b93e-2fc569d27605   efe5bdb3-baac-5d8e-6cae57771c13 Unknown E657F298-2D96-4C7D-8516-E228153FE010    NonDemarcated       {&quot;org-id&quot;:&quot;efe5bdb3-baac-5d8e-6cae57771c13&quot;,&quot;nodeid&quot;:&quot;N02c00056&quot;,&quot;parkingzoneid&quot;:&quot;E657F298-2D96-4C7D-8516-E228153FE010&quot;,&quot;site-id&quot;:&quot;a8f11f90-20c9-11e8-b93e-2fc569d27605&quot;,&quot;channel&quot;:1,&quot;type&quot;:&quot;Park&quot;,&quot;active&quot;:true,&quot;tag&quot;:&quot;&quot;,&quot;configured_date&quot;:&quot;2017-10-23&#xA; 23:29:11.20&quot;,&quot;vs&quot;:[5.0,1.7999999523162842,1.5]}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;DF SaveAsTable&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val spark = SparkSession.builder().appName(&quot;Spark SQL Test&quot;).&#xA;config(&quot;hive.exec.dynamic.partition&quot;, &quot;true&quot;).&#xA;config(&quot;hive.exec.dynamic.partition.mode&quot;, &quot;nonstrict&quot;).&#xA;enableHiveSupport().getOrCreate()&#xA;&#xA;val zoneStatus = spark.table(&quot;zone_status&quot;)&#xA;&#xA;zoneStatus.select(col(&quot;site-id&quot;),col(&quot;org-id&quot;), col(&quot;groupid&quot;), col(&quot;zid&quot;), col(&quot;type&quot;), lit(0), col(&quot;config&quot;), unix_timestamp().alias(&quot;ts&quot;)).&#xA;write.mode(SaveMode.Overwrite).saveAsTable(&quot;dwh_zone_status&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Stored data properly in result table:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;a8f11f90-20c9-11e8-b93e-2fc569d27605    efe5bdb3-baac-5d8e-6cae57771c13 Unknown E657F298-2D96-4C7D-8516-E228153FE010    NonDemarcated   0   {&quot;org-id&quot;:&quot;efe5bdb3-baac-5d8e-6cae57771c13&quot;,&quot;nodeid&quot;:&quot;N02c00056&quot;,&quot;parkingzoneid&quot;:&quot;E657F298-2D96-4C7D-8516-E228153FE010&quot;,&quot;site-id&quot;:&quot;a8f11f90-20c9-11e8-b93e-2fc569d27605&quot;,&quot;channel&quot;:1,&quot;type&quot;:&quot;Park&quot;,&quot;active&quot;:true,&quot;tag&quot;:&quot;&quot;,&quot;configured_date&quot;:&quot;2017-10-23 23:29:11.20&quot;,&quot;vs&quot;:[5.0,1.7999999523162842,1.5]} 1520453589&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;DF insertInto&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;zoneStatus.&#xA;  select(col(&quot;site-id&quot;),col(&quot;org-id&quot;), col(&quot;groupid&quot;), col(&quot;zid&quot;), col(&quot;type&quot;), lit(0), col(&quot;config&quot;), unix_timestamp().alias(&quot;ts&quot;)).&#xA;  write.mode(SaveMode.Overwrite).insertInto(&quot;zone_status_insert&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But insertInto is not persisting all the contents. The json string is storing partially and sub-sequent column is not stored.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;a8f11f90-20c9-11e8-b93e-2fc569d27605    efe5bdb3-baac-5d8e-6cae57771c13 Unknown E657F298-2D96-4C7D-8516-E228153FE010    NonDemarcated   0   {&quot;org-id&quot;:&quot;efe5bdb3-baac-5d8e-6cae57771c13&quot;  NULL&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We are using insertInto functions in our projects and recently encountered when parsing json data to pull other metrics. We noticed that the config content is not stored fully. Planning to change to saveAsTable but we can avoid the code change, if any workaround available to add in spark configuration.&lt;/p&gt;&#xA;" OwnerUserId="9231454" LastEditorUserId="865401" LastEditDate="2018-03-07T22:44:04.197" LastActivityDate="2018-03-08T23:02:59.137" Title="DF insertInto is not persisting all columns for mixed structured data ( json, string)" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;spark-dataframe&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49162854" PostTypeId="1" AcceptedAnswerId="49166009" CreationDate="2018-03-07T23:17:44.060" Score="0" ViewCount="54" Body="&lt;p&gt;my &quot;asdasd.csv&quot; file has the following structure.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; Index,Arrival_Time,Creation_Time,x,y,z,User,Model,Device,gt&#xA;0,1424696633908,1424696631913248572,-5.958191,0.6880646,8.135345,a,nexus4,nexus4_1,stand&#xA;1,1424696633909,1424696631918283972,-5.95224,0.6702118,8.136536,a,nexus4,nexus4_1,stand&#xA;2,1424696633918,1424696631923288855,-5.9950867,0.6535491999999999,8.204376,a,nexus4,nexus4_1,stand&#xA;3,1424696633919,1424696631928385290,-5.9427185,0.6761626999999999,8.128204,a,nexus4,nexus4_1,stand&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Ok, I get the following {key,value} tuple to operate with it.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#                                 x           y        z&#xA;[(('a', 'nexus4', 'stand'), ((-5.958191, 0.6880646, 8.135345)))]&#xA;#           part A (key)               part B (value) &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My code for calculating the mean is the following, I have to calculate the mean from each column, X, Y Z for each Key.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;rdd_ori = sc.textFile(&quot;asdasd.csv&quot;) \&#xA;        .map(lambda x: ((x.split(&quot;,&quot;)[6], x.split(&quot;,&quot;)[7], x.split(&quot;,&quot;)[9]),(float(x.split(&quot;,&quot;)[3]),float(x.split(&quot;,&quot;)[4]),float(x.split(&quot;,&quot;)[5]))))&#xA;&#xA;meanRDD = rdd_ori.mapValues(lambda x: (x,1)) \&#xA;            .reduceByKey(lambda a, b: (a[0][0] + b[0][0], a[0][1] + b[0][1], a[0][2] + b[0][2], a[1] + b[1]))\&#xA;            .mapValues(lambda a : (a[0]/a[3], a[1]/a[3],a[2]/a[3]))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My problem I that I tried that code and it works fine on other PC with the same MV I'm using for developing it (PySpark Py3)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is an example, that this code is correct: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/9W3lz.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/9W3lz.jpg&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But I don't know why I'm getting this error, important part is in &lt;strong&gt;Strong&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;blockquote&gt;&#xA;    &lt;p&gt;--------------------------------------------------------------------------- Py4JJavaError                             Traceback (most recent call&#xA;    last)  in ()&#xA;          9 #sum_1 = count_.reduceByKey(lambda x, y: (x[0][0]+y[0][0],x&lt;a href=&quot;https://i.stack.imgur.com/9W3lz.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;0&lt;/a&gt;+y&lt;a href=&quot;https://i.stack.imgur.com/9W3lz.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;0&lt;/a&gt;,x[0][2]+y[0][2]))&#xA;         10 &#xA;    ---&gt; 11 print(meanRDD.take(1))&lt;/p&gt;&#xA;  &lt;/blockquote&gt;&#xA;  &#xA;  &lt;p&gt;/opt/spark/current/python/pyspark/rdd.py in take(self, num)    1341&lt;br&gt;&#xA;  1342             p = range(partsScanned, min(partsScanned +&#xA;  numPartsToTry, totalParts))&#xA;  -&gt; 1343             res = self.context.runJob(self, takeUpToNumLeft, p)    1344     1345             items += res&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/opt/spark/current/python/pyspark/context.py in runJob(self, rdd,&#xA;  partitionFunc, partitions, allowLocal)&#xA;      990         # SparkContext#runJob.&#xA;      991         mappedRDD = rdd.mapPartitions(partitionFunc)&#xA;  --&gt; 992         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)&#xA;      993         return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))&#xA;      994 &lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/opt/spark/current/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py&#xA;  in &lt;strong&gt;call&lt;/strong&gt;(self, *args)    1131         answer =&#xA;  self.gateway_client.send_command(command)    1132         return_value&#xA;  = get_return_value(&#xA;  -&gt; 1133             answer, self.gateway_client, self.target_id, self.name)    1134     1135         for temp_arg in temp_args:&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/opt/spark/current/python/pyspark/sql/utils.py in deco(*a, **kw)&#xA;       61     def deco(*a, **kw):&#xA;       62         try:&#xA;  ---&gt; 63             return f(*a, **kw)&#xA;       64         except py4j.protocol.Py4JJavaError as e:&#xA;       65             s = e.java_exception.toString()&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;/opt/spark/current/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in&#xA;  get_return_value(answer, gateway_client, target_id, name)&#xA;      317                 raise Py4JJavaError(&#xA;      318                     &quot;An error occurred while calling {0}{1}{2}.\n&quot;.&#xA;  --&gt; 319                     format(target_id, &quot;.&quot;, name), value)&#xA;      320             else:&#xA;      321                 raise Py4JError(&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;Py4JJavaError: An error occurred while calling&#xA;  z:org.apache.spark.api.python.PythonRDD.runJob. :&#xA;  org.apache.spark.SparkException: Job aborted due to stage failure:&#xA;  Task 0 in stage 127.0 failed 1 times, most recent failure: Lost task&#xA;  0.0 in stage 127.0 (TID 102, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent&#xA;  call last):   File&#xA;  &quot;/opt/spark/current/python/lib/pyspark.zip/pyspark/worker.py&quot;, line&#xA;  177, in main&#xA;      process()   File &quot;/opt/spark/current/python/lib/pyspark.zip/pyspark/worker.py&quot;, line&#xA;  172, in process&#xA;      serializer.dump_stream(func(split_index, iterator), outfile)   File &quot;/opt/spark/current/python/pyspark/rdd.py&quot;, line 2423, in&#xA;  pipeline_func&#xA;      return func(split, prev_func(split, iterator))   File &quot;/opt/spark/current/python/pyspark/rdd.py&quot;, line 2423, in&#xA;  pipeline_func&#xA;      return func(split, prev_func(split, iterator))   File &quot;/opt/spark/current/python/pyspark/rdd.py&quot;, line 346, in func&#xA;      return f(iterator)   File &quot;/opt/spark/current/python/pyspark/rdd.py&quot;, line 1842, in&#xA;  combineLocally&#xA;      merger.mergeValues(iterator)   File &quot;/opt/spark/current/python/lib/pyspark.zip/pyspark/shuffle.py&quot;, line&#xA;  238, in mergeValues&#xA;      &lt;strong&gt;d[k] = comb(d[k], v) if k in d else creator(v)   File &quot;&quot;, line 3, in  TypeError:&#xA;  'float' object is not subscriptable&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="6297869" LastEditorUserId="5880706" LastEditDate="2018-03-08T10:59:52.750" LastActivityDate="2018-03-08T10:59:52.750" Title="Pyspark - TypeError: 'float' object is not subscriptable when calculating mean using reduceByKey" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49162954" PostTypeId="1" CreationDate="2018-03-07T23:26:21.397" Score="0" ViewCount="8" Body="&lt;p&gt;I want to split and save my file as xml on the basis of column name, but below code is not working. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;File are getting split automatically.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df.write().partitionBy(columnName).format(&quot;xml&quot;).save(filePath);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2531361" LastActivityDate="2018-03-07T23:26:21.397" Title="Spark Java : file partition by column name is not working on saving file as xml format" Tags="&lt;java&gt;&lt;xml&gt;&lt;apache-spark&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49163141" PostTypeId="1" CreationDate="2018-03-07T23:47:46.477" Score="1" ViewCount="63" Body="&lt;p&gt;I am fetching a column from a Dataframe. The column is of &lt;code&gt;string&lt;/code&gt; type.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;x = &quot;[{somevalues, id:1, name:'xyz'}, {address:Some Value}, {somevalue}]&quot;&lt;/code&gt; &amp;amp; so on..&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The data is stored as a string. It can be easily represented as a list.&#xA;I want the output to be: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;LIST of [&#xA;{somevalues, id:1, name:'xyz'}, &#xA;{address:Some Value}, &#xA;{somevalue}&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How can I achieve this using Spark's API? I know that with Python I can use the &lt;code&gt;eval(x)&lt;/code&gt; function and it will return the list or I can use the &lt;code&gt;x.split()&lt;/code&gt; function, which will also return a list. However, in this approach, it needs to iterate for each record.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also, I want to use &lt;code&gt;mapPartition&lt;/code&gt;; that is the reason why I need my string column to be in a list so that I can pass it to &lt;code&gt;mapPartition&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there an efficient way where I can also convert my string data using spark API or would &lt;code&gt;mapPartitions&lt;/code&gt; be even better as I will be looping every partition rather than every record?&lt;/p&gt;&#xA;" OwnerUserId="4265823" LastEditorUserId="4799172" LastEditDate="2018-03-07T23:57:18.293" LastActivityDate="2018-03-10T04:16:27.697" Title="How to convert a String into a List using spark function Pyspark" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;apache-spark-sql&gt;&lt;spark-dataframe&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49163164" PostTypeId="1" CreationDate="2018-03-07T23:49:37.197" Score="1" ViewCount="11" Body="&lt;p&gt;I am trying to run a test file in PyCharm with unittest. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;It errors with this message: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;FileNotFoundError: [Errno 2] No such file or directory: 'myfolder/opt/spark/./bin/spark-submit': 'myfolder/opt/spark/./bin/spark-submit'&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think it has something to do with my .bash_profile which a coworker helped configure. It looks like this: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;export SPARK_HOME=myfolder/opt/spark&#xA;export PATH=$SPARK_HOME/bin:$PATH&#xA;export PATH=$HOME/.node_modules_global/bin:$PATH&#xA;# added by Anaconda3 5.1.0 installer&#xA;export PATH=&quot;/Users/myname/anaconda3/bin:$PATH&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Important to note, the path &lt;code&gt;myfolder/opt/spark/./bin/spark-submit&lt;/code&gt; does not exist, but &lt;code&gt;myfolder/opt/spark/bin/spark-submit&lt;/code&gt; does. I have no idea why the extra &lt;code&gt;./&lt;/code&gt; is being added to the path.&lt;/p&gt;&#xA;" OwnerUserId="6660836" LastActivityDate="2018-03-07T23:49:37.197" Title="PyCharm: Can't find spark-submit" Tags="&lt;bash&gt;&lt;pyspark&gt;&lt;pycharm&gt;&lt;conda&gt;&lt;python-unittest&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49163812" PostTypeId="1" AcceptedAnswerId="49165265" CreationDate="2018-03-08T01:07:01.807" Score="0" ViewCount="25" Body="&lt;p&gt;I have a Spark &lt;code&gt;DataFrame&lt;/code&gt; called &lt;code&gt;nfe&lt;/code&gt;, which contains a column called &lt;code&gt;NFE_CNPJ_EMITENTE&lt;/code&gt; that is currently formatted as a string (although it is completely numeric). This column should have entries that are either 11 or 14 characters in length, but there are entries of 9, 10, 12, and 13 character length for which I need to add leading zeros. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I cannot use the &lt;code&gt;sprintf&lt;/code&gt; function as I do in R:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;nfe$CNPJ_EMITENTE &amp;lt;- ifelse(nfe$length_emit == 9, sprintf(&quot;%00s&quot;, nfe$NFE_CNPJ_EMITENTE), nfe$NFE_CNPJ_EMITENTE)&#xA;&#xA;# Error in sprintf(&quot;%00s&quot;, nfe$NFE_CNPJ_EMITENTE) : unsupported type&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there a simple way to add 2 leading zeros for 9 and 12 length entries and 1 leading zero for 11 and 13 length entries?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;" OwnerUserId="9153761" LastActivityDate="2018-03-08T18:21:55.607" Title="Adding leading zeros in SparkR" Tags="&lt;r&gt;&lt;apache-spark&gt;&lt;sparkr&gt;&lt;leading-zero&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49164031" PostTypeId="1" CreationDate="2018-03-08T01:39:22.590" Score="-1" ViewCount="30" Body="&lt;p&gt;The Spark I connected to, is not built on my local computer but a remote one. Everytime when I connect to it &lt;a href=&quot;http://xx.xxx.xxx.xxx:10000/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://xx.xxx.xxx.xxx:10000/&lt;/a&gt;, the error says:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[IPKernelApp] WARNING | Unknown error in handling PYTHONSTARTUP file /usr/local/spark/python/pyspark/shell.py:&#xA;18/03/07 08:52:53 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Anyways, I still keep trying to run it on Jupyter notebook:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark.conf import SparkConf&#xA;SparkSession.builder.config(conf=SparkConf())&#xA;&#xA;dir(spark)  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I ran it yesterday, it shows directory. when I did it today, it says:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;NameError: name 'spark' is not defined&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any suggestion is appreciated!&lt;/p&gt;&#xA;" OwnerUserId="8968910" LastEditorUserId="7045987" LastEditDate="2018-03-08T06:41:22.630" LastActivityDate="2018-03-08T06:45:53.270" Title="Pyspark running error" Tags="&lt;pyspark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49164332" PostTypeId="1" CreationDate="2018-03-08T02:20:31.437" Score="0" ViewCount="11" Body="&lt;p&gt;I am trying to read data from kafka and save to parquet file on hdfs. &#xA;My code is similar to following, that the difference is I am writing in Java.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val df = spark&#xA;      .readStream&#xA;      .format(&quot;kafka&quot;)&#xA;      .option(&quot;kafka.bootstrap.servers&quot;, &quot;host1:port1,host2:port2&quot;)&#xA;      .option(&quot;subscribe&quot;, &quot;topic1&quot;)&#xA;      .load()&#xA;&#xA;df.selectExpr(&quot;CAST(key AS STRING)&quot;,&quot;CAST(value AS STRING)&quot;).writeStream.format(&quot;parquet&quot;).option(&quot;path&quot;,outputPath).option(&quot;checkpointLocation&quot;, &quot;/tmp/sparkcheckpoint1/&quot;).outputMode(&quot;append&quot;).start().awaiteTermination()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However it threw &lt;code&gt;Uri without authority: hdfs:/data/_spark_metadata&lt;/code&gt; exception, where &lt;code&gt;hdfs:///data&lt;/code&gt; is the output path. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I change the code to &lt;code&gt;spark.read&lt;/code&gt; and &lt;code&gt;df.write&lt;/code&gt; to write out parquet file once, there is no any exception, so I guess it is not related to my hdfs config. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone help me? &lt;/p&gt;&#xA;" OwnerUserId="2559256" LastEditorUserId="6378311" LastEditDate="2018-03-08T09:45:10.043" LastActivityDate="2018-03-08T09:45:10.043" Title="Uri without authority: hdfs:/data/_spark_metadata error when use spark stream write parquet file on hdfs" Tags="&lt;apache-spark&gt;&lt;hdfs&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49164434" PostTypeId="1" CreationDate="2018-03-08T02:30:59.743" Score="0" ViewCount="13" Body="&lt;p&gt;when I run the below line of code &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt;model_pca &amp;lt;- tbl(sc, &quot;flights&quot;) %&amp;gt;% select(air_time,distance,dep_time) %&amp;gt;% &#xA;ml_pca()    &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;results into a stage failure but I cannot understand the reason&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;org.apache.spark.SparkException: Failed to execute user defined function($anonfun$3: (struct&amp;lt;air_time:double,distance:double,dep_time_double_vector_assembler_801c364a4ab0:double&amp;gt;) =&amp;gt; vector)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It says user defined function but I am not using any user defined function, have I missed any package installation or is it because I am using spark 2.2.0 with hadoop 2.7&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Below is the full error message:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Error: org.apache.spark.SparkException: Job aborted due to stage failure: &#xA;Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in &#xA;stage 12.0 (TID 12, localhost, executor driver): &#xA;org.apache.spark.SparkException: Failed to execute user defined &#xA;function($anonfun$3:&#xA; (struct&amp;lt;air_time:double,distance:double,dep_time_double_vector_assembler_801c364a4ab0:double&amp;gt;) =&amp;gt; vector)&#xA;    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)&#xA;        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)&#xA;        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:395)&#xA;        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)&#xA;        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)&#xA;    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)&#xA;        at scala.collection.Iterator$class.foreach(Iterator.scala:893)&#xA;        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)&#xA;        at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)&#xA;        at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)&#xA;        at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)&#xA;        at scala.collection.AbstractIterator.aggregate(Iterator.scala:1336)&#xA;        at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136)&#xA;        at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136)&#xA;        at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137)&#xA;        at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137)&#xA;        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)&#xA;        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797)&#xA;        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)&#xA;        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)&#xA;        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)&#xA;        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)&#xA;        at org.apache.spark.scheduler.Task.run(Task.scala:108)&#xA;        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)&#xA;        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)&#xA;        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)&#xA;        at java.lang.Thread.run(Unknown Source)&#xA;    Caused by: org.apache.spark.SparkException: Values to assemble cannot be null.&#xA;        at org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:160)&#xA;    at org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:143)&#xA;        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)&#xA;        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)&#xA;        at org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:143)&#xA;        at org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:99)&#xA;        at org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:98)&#xA;    ... 27 more&#xA;&#xA;Driver stacktrace:&#xA;    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)&#xA;        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)&#xA;    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)&#xA;        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&#xA;        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)&#xA;        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)&#xA;        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)&#xA;    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)&#xA;        at scala.Option.foreach(Option.scala:257)&#xA;        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)&#xA;        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)&#xA;        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)&#xA;        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)&#xA;        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)&#xA;        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)&#xA;        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)&#xA;        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119)&#xA;        at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)&#xA;        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&#xA;    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)&#xA;        at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)&#xA;        at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)&#xA;        at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151)&#xA;        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&#xA;    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)&#xA;        at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)&#xA;        at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128)&#xA;        at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeColumnSummaryStatistics(RowMatrix.scala:419)&#xA;        at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:334)&#xA;        at org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponentsAndExplainedVariance(RowMatrix.scala:387)&#xA;        at org.apache.spark.mllib.feature.PCA.fit(PCA.scala:48)&#xA;        at org.apache.spark.ml.feature.PCA.fit(PCA.scala:99)&#xA;        at org.apache.spark.ml.feature.PCA.fit(PCA.scala:70)&#xA;        at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:153)&#xA;        at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:149)&#xA;        at scala.collection.Iterator$class.foreach(Iterator.scala:893)&#xA;    at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)&#xA;    at scala.collection.IterableViewLike$Transformed$class.foreach(IterableViewLike.scala:44)&#xA;    at scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:37)&#xA;        at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:149)&#xA;        at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:96)&#xA;        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#xA;        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)&#xA;        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)&#xA;        at java.lang.reflect.Method.invoke(Unknown Source)&#xA;        at sparklyr.Invoke$.invoke(invoke.scala:102)&#xA;    at sparklyr.StreamHandler$.handleMethodCall(stream.scala:97)&#xA;        at sparklyr.StreamHandler$.read(stream.scala:62)&#xA;    at sparklyr.BackendHandler.channelRead0(handler.scala:52)&#xA;    at sparklyr.BackendHandler.channelRead0(handler.scala:14)&#xA;    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)&#xA;    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)&#xA;    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)&#xA;    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)&#xA;    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)&#xA;    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)&#xA;    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)&#xA;    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)&#xA;    at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:293)&#xA;    at io.netty.handler.co&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="9459769" LastEditorUserId="1402846" LastEditDate="2018-03-08T02:41:37.137" LastActivityDate="2018-03-08T04:21:36.943" Title="Spark Stage error when performing PCA analysis in R" Tags="&lt;r&gt;&lt;apache-spark&gt;&lt;pca&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49164683" PostTypeId="1" CreationDate="2018-03-08T03:04:15.600" Score="1" ViewCount="24" Body="&lt;p&gt;I have to pass record to an UDF which calls an API but as we want to do it  parallely,we are  using spark and thats why UDF is being developed, the problem here is that that UDF needs to take only 100 records at a time not more than that, it can't handle more than 100 records parallely, so how to ensure that only 100 record pass to it in one go please note we don't want to use count() function on whole record.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am attaching the UDF code here,it's a generic UDF which returns array of struct.moreover if we pass 100 records in batchsize variable each time then,if suppose there are 198 records then if as we dont want to use count() we will not be knowing that its last batchsize is going to be 98.so how to handle that thing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Guys... I have a generic UDF in  which call is made for an API but before calling it creates batch of 100 firstly then only call restapi.. So the argument UDF takes are x1:string, x2:string, batchsize:integer(currently the batchsize is 100)..so in UDF until and unless the batchsize is not 100 the call will not happen.. And for each record it will return null. &#xA;So till 99th record it will return. Null but at 100th record the call will happen&#xA;[So, now the problem part:as we are taking batchsize 100 and call will take place only at 100th record. So, in condition like if we have suppose 198 record in file then 100 record will get the output but, other 98 will only return null as they will not get processed..&#xA;So please help a way around, and UDF take one record at a time, but it keep on collecting till 100th record.. I hope this clears up&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;div class=&quot;snippet&quot; data-lang=&quot;js&quot; data-hide=&quot;false&quot; data-console=&quot;true&quot; data-babel=&quot;false&quot;&gt;&#xD;&#xA;&lt;div class=&quot;snippet-code&quot;&gt;&#xD;&#xA;&lt;pre class=&quot;snippet-code-js lang-js prettyprint-override&quot;&gt;&lt;code&gt;public class Standardize_Address extends GenericUDF {&#xD;&#xA;&#xD;&#xA;	private static final Logger logger = LoggerFactory.getLogger(Standardize_Address.class);&#xD;&#xA;	private int counter = 0;&#xD;&#xA;	Client client = null;&#xD;&#xA;	private Batch batch = new Batch();&#xD;&#xA;&#xD;&#xA;	public Standardize_Address() {&#xD;&#xA;&#xD;&#xA;		client = new ClientBuilder().withUrl(&quot;https://ss-staging-public.beringmedia.com/street-address&quot;).build();&#xD;&#xA;	}&#xD;&#xA;&#xD;&#xA;	// StringObjectInspector streeti;&#xD;&#xA;	PrimitiveObjectInspector streeti;&#xD;&#xA;	PrimitiveObjectInspector cityi;&#xD;&#xA;	PrimitiveObjectInspector zipi;&#xD;&#xA;	PrimitiveObjectInspector statei;&#xD;&#xA;	PrimitiveObjectInspector batchsizei;&#xD;&#xA;&#xD;&#xA;	private ArrayList ret;&#xD;&#xA;&#xD;&#xA;	@Override&#xD;&#xA;	public String getDisplayString(String[] argument) {&#xD;&#xA;		return &quot;My display string&quot;;&#xD;&#xA;	}&#xD;&#xA;&#xD;&#xA;	@Override&#xD;&#xA;	public ObjectInspector initialize(ObjectInspector[] args) throws UDFArgumentException {&#xD;&#xA;		&#xD;&#xA;		System.out.println(&quot;under initialize&quot;);&#xD;&#xA;&#xD;&#xA;		if (args[0] == null) {&#xD;&#xA;			throw new UDFArgumentTypeException(0, &quot;NO Street is mentioned&quot;);&#xD;&#xA;		}&#xD;&#xA;		if (args[1] == null) {&#xD;&#xA;			throw new UDFArgumentTypeException(0, &quot;No Zip is mentioned&quot;);&#xD;&#xA;		}&#xD;&#xA;		if (args[2] == null) {&#xD;&#xA;			throw new UDFArgumentTypeException(0, &quot;No city is mentioned&quot;);&#xD;&#xA;		}&#xD;&#xA;		if (args[3] == null) {&#xD;&#xA;			throw new UDFArgumentTypeException(0, &quot;No State is mentioned&quot;);&#xD;&#xA;		}&#xD;&#xA;		if (args[4] == null) {&#xD;&#xA;			throw new UDFArgumentTypeException(0, &quot;No batch size is mentioned&quot;);&#xD;&#xA;		}&#xD;&#xA;&#xD;&#xA;		/// streeti =args[0];&#xD;&#xA;		 streeti = (PrimitiveObjectInspector)args[0];&#xD;&#xA;		// this.streetvalue = (StringObjectInspector) streeti;&#xD;&#xA;		 cityi = (PrimitiveObjectInspector)args[1];&#xD;&#xA;		 zipi = (PrimitiveObjectInspector)args[2];&#xD;&#xA;		 statei = (PrimitiveObjectInspector)args[3];&#xD;&#xA;		batchsizei = (PrimitiveObjectInspector)args[4];&#xD;&#xA;&#xD;&#xA;		ret = new ArrayList();&#xD;&#xA;&#xD;&#xA;		ArrayList structFieldNames = new ArrayList();&#xD;&#xA;		ArrayList structFieldObjectInspectors = new ArrayList();&#xD;&#xA;&#xD;&#xA;		structFieldNames.add(&quot;Street&quot;);&#xD;&#xA;		structFieldNames.add(&quot;city&quot;);&#xD;&#xA;		structFieldNames.add(&quot;zip&quot;);&#xD;&#xA;		structFieldNames.add(&quot;state&quot;);&#xD;&#xA;&#xD;&#xA;		structFieldObjectInspectors.add(PrimitiveObjectInspectorFactory.writableStringObjectInspector);&#xD;&#xA;		structFieldObjectInspectors.add(PrimitiveObjectInspectorFactory.writableStringObjectInspector);&#xD;&#xA;		structFieldObjectInspectors.add(PrimitiveObjectInspectorFactory.writableStringObjectInspector);&#xD;&#xA;		structFieldObjectInspectors.add(PrimitiveObjectInspectorFactory.writableStringObjectInspector);&#xD;&#xA;&#xD;&#xA;		StructObjectInspector si2 = ObjectInspectorFactory.getStandardStructObjectInspector(structFieldNames,&#xD;&#xA;				structFieldObjectInspectors);&#xD;&#xA;&#xD;&#xA;		ListObjectInspector li2;&#xD;&#xA;		li2 = ObjectInspectorFactory.getStandardListObjectInspector(si2);&#xD;&#xA;		return li2;&#xD;&#xA;	}&#xD;&#xA;&#xD;&#xA;	@Override&#xD;&#xA;	public Object evaluate(DeferredObject[] args) throws HiveException {&#xD;&#xA;&#xD;&#xA;		ret.clear();&#xD;&#xA;		System.out.println(&quot;under evaluate&quot;);&#xD;&#xA;&#xD;&#xA;		// String street1 = streetvalue.getPrimitiveJavaObject(args[0].get());&#xD;&#xA;&#xD;&#xA;		Object oin = args[4].get();&#xD;&#xA;		System.out.println(&quot;under typecasting&quot;);&#xD;&#xA;		int batchsize = (Integer) batchsizei.getPrimitiveJavaObject(oin);&#xD;&#xA;		System.out.println(&quot;batchsize&quot;);&#xD;&#xA;		Object oin1 = args[0].get();&#xD;&#xA;		String street1 = (String) streeti.getPrimitiveJavaObject(oin1);&#xD;&#xA;		Object oin2 = args[1].get();&#xD;&#xA;		String zip1 = (String) zipi.getPrimitiveJavaObject(oin2);&#xD;&#xA;		Object oin3 = args[2].get();&#xD;&#xA;		String city1 = (String) cityi.getPrimitiveJavaObject(oin3);&#xD;&#xA;		Object oin4 = args[3].get();&#xD;&#xA;		String state1 = (String) statei.getPrimitiveJavaObject(oin4);&#xD;&#xA;&#xD;&#xA;		logger.info(&quot;address passed, street=&quot; + street1 + &quot;,zip=&quot; + zip1 + &quot;,city=&quot; + city1 + &quot;,state=&quot; + state1);&#xD;&#xA;		counter++;&#xD;&#xA;&#xD;&#xA;		try {&#xD;&#xA;			System.out.println(&quot;under try&quot;);&#xD;&#xA;			Lookup lookup = new Lookup();&#xD;&#xA;			lookup.setStreet(street1);&#xD;&#xA;			lookup.setCity(city1);&#xD;&#xA;			lookup.setState(state1);&#xD;&#xA;			lookup.setZipCode(zip1);&#xD;&#xA;			lookup.setMaxCandidates(1);&#xD;&#xA;			batch.add(lookup);&#xD;&#xA;		} catch (BatchFullException ex) {&#xD;&#xA;			logger.error(ex.getMessage(), ex);&#xD;&#xA;		} catch (Exception e) {&#xD;&#xA;			logger.error(e.getMessage(), e);&#xD;&#xA;		}&#xD;&#xA;&#xD;&#xA;		/* batch.add(lookup); */&#xD;&#xA;		if (counter == batchsize) {&#xD;&#xA;			System.out.println(&quot;under if&quot;);&#xD;&#xA;			try {&#xD;&#xA;				logger.info(&quot;batch input street &quot; + batch.get(0).getStreet());&#xD;&#xA;				try {&#xD;&#xA;					client.send(batch);&#xD;&#xA;				} catch (Exception e) {&#xD;&#xA;					logger.error(e.getMessage(), e);&#xD;&#xA;					logger.warn(&quot;skipping current batch, continuing with the next batch&quot;);&#xD;&#xA;					batch.clear();&#xD;&#xA;					counter = 0;&#xD;&#xA;					return null;&#xD;&#xA;				}&#xD;&#xA;&#xD;&#xA;				Vector&amp;lt;Lookup&amp;gt; lookups = batch.getAllLookups();&#xD;&#xA;&#xD;&#xA;				for (int i = 0; i &amp;lt; batch.size(); i++) {&#xD;&#xA;					// ListObjectInspector candidates;&#xD;&#xA;					ArrayList&amp;lt;Candidate&amp;gt; candidates = lookups.get(i).getResult();&#xD;&#xA;&#xD;&#xA;					if (candidates.isEmpty()) {&#xD;&#xA;						logger.warn(&quot;Address &quot; + i + &quot; is invalid.\n&quot;);&#xD;&#xA;						continue;&#xD;&#xA;					}&#xD;&#xA;&#xD;&#xA;					logger.info(&quot;Address &quot; + i + &quot; is valid. (There is at least one candidate)&quot;);&#xD;&#xA;&#xD;&#xA;					for (Candidate candidate : candidates) {&#xD;&#xA;						final Components components = candidate.getComponents();&#xD;&#xA;						final Metadata metadata = candidate.getMetadata();&#xD;&#xA;&#xD;&#xA;						logger.info(&quot;\nCandidate &quot; + candidate.getCandidateIndex() + &quot;:&quot;);&#xD;&#xA;						logger.info(&quot;Delivery line 1: &quot; + candidate.getDeliveryLine1());&#xD;&#xA;						logger.info(&quot;Last line:       &quot; + candidate.getLastLine());&#xD;&#xA;						logger.info(&quot;ZIP Code:        &quot; + components.getZipCode() + &quot;-&quot; + components.getPlus4Code());&#xD;&#xA;						logger.info(&quot;County:          &quot; + metadata.getCountyName());&#xD;&#xA;						logger.info(&quot;Latitude:        &quot; + metadata.getLatitude());&#xD;&#xA;						logger.info(&quot;Longitude:       &quot; + metadata.getLongitude());&#xD;&#xA;					}&#xD;&#xA;&#xD;&#xA;					Object[] e;&#xD;&#xA;					e = new Object[4];&#xD;&#xA;&#xD;&#xA;					e[0] = new Text(candidates.get(i).getComponents().getStreetName());&#xD;&#xA;					e[1] = new Text(candidates.get(i).getComponents().getCityName());&#xD;&#xA;					e[2] = new Text(candidates.get(i).getComponents().getZipCode());&#xD;&#xA;					e[3] = new Text(candidates.get(i).getComponents().getState());&#xD;&#xA;&#xD;&#xA;					ret.add(e);&#xD;&#xA;				}&#xD;&#xA;				counter = 0;&#xD;&#xA;				batch.clear();&#xD;&#xA;			} catch (Exception e) {&#xD;&#xA;				logger.error(e.getMessage(), e);&#xD;&#xA;			}&#xD;&#xA;			return ret;&#xD;&#xA;&#xD;&#xA;		} else {&#xD;&#xA;			return null;&#xD;&#xA;		}&#xD;&#xA;&#xD;&#xA;	}&#xD;&#xA;&#xD;&#xA;}&lt;/code&gt;&lt;/pre&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;/p&gt;&#xA;" OwnerUserId="9065316" LastEditorUserId="9065316" LastEditDate="2018-03-09T02:24:31.947" LastActivityDate="2018-03-09T02:24:31.947" Title="100 records at a time to udf" Tags="&lt;java&gt;&lt;apache-spark&gt;&lt;bigdata&gt;&lt;spark-dataframe&gt;" AnswerCount="0" CommentCount="5" FavoriteCount="1" />
  <row Id="49165329" PostTypeId="1" AcceptedAnswerId="49166790" CreationDate="2018-03-08T04:15:41.073" Score="1" ViewCount="33" Body="&lt;p&gt;There is a json data source. Here is an example of one row:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{&#xA;  &quot;PrimaryAcctNumber&quot;: &quot;account1&quot;,&#xA;  &quot;AdditionalData&quot;: [&#xA;    {&#xA;      &quot;Addrs&quot;: [&#xA;        &quot;an address for account1&quot;,&#xA;        &quot;the longest address in the address list for account1&quot;,&#xA;        &quot;another address for account1&quot;&#xA;      ],&#xA;      &quot;AccountNumber&quot;: &quot;Account1&quot;,&#xA;      &quot;IP&quot;: 2368971684&#xA;    },&#xA;    {&#xA;      &quot;Addrs&quot;: [&#xA;        &quot;an address for account2&quot;,&#xA;        &quot;the longest address in the address list for account2&quot;,&#xA;        &quot;another address for account2&quot;&#xA;      ],&#xA;      &quot;AccountNumber&quot;: &quot;Account2&quot;,&#xA;      &quot;IP&quot;: 9864766814&#xA;    }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So when load it to spark DataFrame, the schema is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;root&#xA; |-- PrimaryAcctNumber: string (nullable = true)&#xA; |-- AdditionalData: array (nullable = true)&#xA; |    |-- element: struct (containsNull = true)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to use spark to create a new column called &lt;code&gt;LongestAddressOfPrimaryAccount&lt;/code&gt; based on colomn &lt;code&gt;AdditionalData (ArrayType[StructType])&lt;/code&gt; using the following logic: &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Iterate AdditionalData&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;If &lt;code&gt;AccountNumber&lt;/code&gt; property equals &lt;code&gt;PrimaryAcctNumber&lt;/code&gt; of the row, the value of &lt;code&gt;LongestAddressOfPrimaryAccount&lt;/code&gt; will be the longest string in &lt;code&gt;Addrs&lt;/code&gt; array&lt;/li&gt;&#xA;&lt;li&gt;If no &lt;code&gt;AccountNumber&lt;/code&gt; property equals &lt;code&gt;PrimaryAcctNumber&lt;/code&gt;, the value will be &quot;N/A&quot;&lt;/li&gt;&#xA;&lt;/ul&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;So for the given row above, the expected output is:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{&#xA;  &quot;PrimaryAcctNumber&quot;: &quot;account1&quot;,&#xA;  &quot;AdditionalData&quot;: [&#xA;    {&#xA;      &quot;Addrs&quot;: [&#xA;        &quot;an address for account1&quot;,&#xA;        &quot;the longest address in the address list for account1&quot;,&#xA;        &quot;another address for account1&quot;&#xA;      ],&#xA;      &quot;AccountNumber&quot;: &quot;Account1&quot;,&#xA;      &quot;IP&quot;: 2368971684&#xA;    },&#xA;    {&#xA;      &quot;Addrs&quot;: [&#xA;        &quot;an address for account2&quot;,&#xA;        &quot;the longest address in the address list for account2&quot;,&#xA;        &quot;another address for account2&quot;&#xA;      ],&#xA;      &quot;AccountNumber&quot;: &quot;Account2&quot;,&#xA;      &quot;IP&quot;: 9864766814&#xA;    }&#xA;  ],&#xA;  &quot;LongestAddressOfPrimaryAccount&quot;: &quot;the longest address in the address list for account1&quot;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It is doable to use a UDF or a map function. But that is not the best practice for spark. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is it doable to just use spark functions? Something like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sourceDdf.withColumn(&quot;LongestAddressOfPrimaryAccount&quot;, coalesce(&#xA;  longest(&#xA;    get_field(iterate_array_for_match($&quot;AdditionalData&quot;, &quot;AccountNumber&quot;, $&quot;PrimaryAcctNumber&quot;), &quot;Addrs&quot;)&#xA;  )&#xA;  , lit(&quot;N/A&quot;)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="155547" LastActivityDate="2018-03-08T06:31:21.420" Title="How to process complex data in ArrayType using Spark function" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;apache-spark-dataset&gt;&lt;apache-spark-function&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49165645" PostTypeId="1" CreationDate="2018-03-08T04:51:21.620" Score="0" ViewCount="14" Body="&lt;p&gt;I have a problem while working with Spark. I am using JavaStreamingContext to recieve the eventMessages from a RabbitMQ. Then storing those events into Spark Store with StorageLevel.MEMORY_AND_DISK_2().&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;JavaReceiverInputDStream&amp;lt;String&amp;gt; eventsListDStream = getInputStream();//getInputStream is nothing a but a receiver.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;After this i want to process those DStreams. So i am converting them to a JSON Object and then start some process.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;eventsListDStream.foreachRDD((VoidFunction&amp;lt;JavaRDD&amp;lt;String&amp;gt;&amp;gt;) rdd -&amp;gt; {&#xA;        processInputEventsRDD(rdd);&#xA;    });&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now work on these RDD&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    protected void processInputEventsRDD(JavaRDD&amp;lt;String&amp;gt; eventsListRDD) throws RetrieveException, UpdateException, Exception {&#xA;    List&amp;lt;String&amp;gt; eventMsgs = eventsListRDD.collect();&#xA;    if (eventMsgs.isEmpty()){&#xA;        return;&#xA;    }&#xA;    logger.debug(CCFLogCtx.COMPUTEENGINE, &quot;Processing of Event messages started.&quot;);&#xA;    try{&#xA;        List&amp;lt;UUID&amp;gt; requestUUIDs = getAllRequestUUIDs(eventMsgs);&#xA;        List&amp;lt;ComputeRequest&amp;gt; computeRequests = getComputationDataFromCassandra(requestUUIDs);//query C* and populate computeRequest&#xA;        Map&amp;lt;String, List&amp;lt;ComputeRequest&amp;gt;&amp;gt; engineTypeMap = getEngineTypeMap(computeRequests);&#xA;&#xA;        //Parallelize logic here.&#xA;        for(String engineType : engineTypeMap.keySet()){&#xA;&#xA;        }&#xA;&#xA;        logger.debug(CCFLogCtx.COMPUTEENGINE, &quot;Current batch processing is done. Get ready for next batch.&quot;);&#xA;&#xA;    } catch (Exception e) {&#xA;        handleException(e);&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So now i want to perform some specific task on the basis of map key. For example my map key is of type A then perform the logic which is written in a class A. &#xA;If the map key is B then perform the logic which is written in a class B.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a method which will do these processing whose body looks like this :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;public void processJob(JavaSparkContext jsc, String engineBeanName, List&amp;lt;ComputeRequest&amp;gt; jobs) throws UpdateException{&#xA;    // Perform pre-processing operations&#xA;    preProcessing(jobs);&#xA;&#xA;    // Start job processing.&#xA;    Map&amp;lt;UUID, ComputeResult&amp;gt; jobResult = null;&#xA;    try {&#xA;        jobResult = processing(jsc, engineBeanName, jobs);&#xA;    } catch (Exception e) {&#xA;        logger.error(CCFLogCtx.COMPUTEENGINE,&quot;Some error encountered while executing job for engine=&quot; + engineBeanName + &quot;, InputMsg=&quot;+jobs+&quot;, ErrorMsg=&quot; + e.getMessage(),e);&#xA;    }&#xA;&#xA;    // Perform post processing operations.&#xA;    postProcessing(jobs, jobResult);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But i need to call this method processJobs in parallel where engineBeanName is the key of the map and jobs are the values correspond to that key. its different because based on each engineBean the input data's handling is different.&lt;/p&gt;&#xA;" OwnerUserId="6007504" LastActivityDate="2018-03-08T04:51:21.620" Title="Parallel processing a map in spark which will perform different type of operation based on key specified" Tags="&lt;java&gt;&lt;apache-spark&gt;&lt;parallel-processing&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49165696" PostTypeId="1" CreationDate="2018-03-08T04:57:11.173" Score="0" ViewCount="21" Body="&lt;p&gt;I have searched through every documentation and still didn't find why there is a prefix and what is c000 in the below file naming convention:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;file:/Users/stephen/p/spark/f1/part-00000-445036f9-7a40-4333-8405-8451faa44319-&#xA;c000.snappy.parquet&lt;/p&gt;&#xA;" OwnerUserId="4326922" LastActivityDate="2018-03-08T08:18:37.187" Title="Could anyone please explain what is c000 means in c000.snappy.parquet or c000.snappy.orc??" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;hive&gt;&lt;parquet&gt;&lt;orc&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49166137" PostTypeId="1" CreationDate="2018-03-08T05:37:03.980" Score="0" ViewCount="20" Body="&lt;p&gt;I have to parse large csvs approx 1gb, map the header to the database columns, and format every row. I.E the csv has &quot;Gender&quot; Male but my database only accepts &lt;code&gt;enum('M', 'F', 'U')&lt;/code&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since the files are so large I have to use node streams, to transform the file and then use &lt;code&gt;load data infile&lt;/code&gt; to upload it all at once. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I would like granular control over the inserts, which &lt;code&gt;load data infile&lt;/code&gt; doesn't provide. If a single line has incorrect data the whole upload fails. I am currently using &lt;a href=&quot;https://github.com/mysqljs/mysql&quot; rel=&quot;nofollow noreferrer&quot;&gt;mysqljs&lt;/a&gt;, which doesn't provide an api to check if the pool has reached queueLimit and therefore I can't pause the stream reliably. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am wondering if I can use apache kafka or spark to stream the instructions and it will be added to the database sequentially. I have skimmed through the docs and read some tutorials but none of them show how to connect them to the database. It is mostly consumer/producer examples. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know there are multiple ways of solving this problem but I am very much interested in a way to seamlessly integrate streams with databases. If streams can work with I.O why not databases? I am pretty sure big companies don't use &lt;code&gt;load data infile&lt;/code&gt; or add chunks of data to array repeatedly and insert to database. &lt;/p&gt;&#xA;" OwnerUserId="1391555" LastEditorUserId="1391555" LastEditDate="2018-03-08T14:55:18.723" LastActivityDate="2018-03-08T14:55:18.723" Title="Node Streams to Mysql" Tags="&lt;node.js&gt;&lt;apache-spark&gt;&lt;stream&gt;&lt;apache-kafka&gt;&lt;mysqljs&gt;" AnswerCount="0" CommentCount="8" />
  <row Id="49166173" PostTypeId="1" CreationDate="2018-03-08T05:39:36.363" Score="0" ViewCount="26" Body="&lt;p&gt;I am running Apache Spark 2.1.1 in Standalone Mode with client deploy mode.&#xA;&lt;strong&gt;I want to disable Spark web UI for master and all workers&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Refered to: &lt;a href=&quot;https://spark.apache.org/docs/latest/configuration.html#spark-ui&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://spark.apache.org/docs/latest/configuration.html#spark-ui&lt;/a&gt; and &#xA;Used following conf in $SPARK_HOME/conf/spark-defaults.conf:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark.ui.showConsoleProgress=false&#xA;spark.ui.enabled=false&#xA;spark.ui.killEnabled=false&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Yet I could still see web UI at default port 8080 (for Spark Master and 8081 for Spark Worker).&#xA;I have the same configuration in all the worker nodes.&lt;/p&gt;&#xA;" OwnerUserId="4006204" LastActivityDate="2018-03-08T05:39:36.363" Title="Apache Spark: Disable Spark Web UI in Spark Standalone mode" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49166258" PostTypeId="1" AcceptedAnswerId="49166276" CreationDate="2018-03-08T05:46:44.370" Score="0" ViewCount="30" Body="&lt;p&gt;I have the below dataframe, from which I need to keep null values not being dropped from key column. I know that if we pass one more column then we can avoid dropping null values, but my problem is from key column I need to drop only the values which are duplicating, I should not drop null values.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;id    pin    key  &#xA;-----------------&#xA;1       9       9&#xA;9       8       9&#xA;2       6       &#xA;6       3         &#xA;8       0       8&#xA;&#xA;df.dropDuplicates(&quot;key&quot;).show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am getting output as below,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;id    pin    key  &#xA;-----------------&#xA;1       9       9&#xA;2       6        &#xA;8       0       8&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am expecting   &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;id    pin    key  &#xA;-----------------&#xA;1       9       9&#xA;2       6       &#xA;6       3         &#xA;8       0       8&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If I use the above function it is dropping the null values also. Kindly provide me if I have any other alternative.&lt;/p&gt;&#xA;" OwnerUserId="8919697" LastEditorUserId="7579547" LastEditDate="2018-03-08T06:32:57.017" LastActivityDate="2018-03-08T10:57:22.507" Title="How to avoid dropping null values from dropduplicate function when passing single column" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49166975" PostTypeId="1" CreationDate="2018-03-08T06:45:12.313" Score="0" ViewCount="13" Body="&lt;p&gt;I have Installed spark using docker/kubetnetes and exposed the port 7077,8080.&#xA;I am using some different nodePort to access it from outside. Thing is that When I submit any job in spark,running job information and completed job information, I couldn't able to see it in spark master web console.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance!!&lt;/p&gt;&#xA;" OwnerUserId="7255439" LastActivityDate="2018-03-08T06:45:12.313" Title="Spark web UI not showing running jobs" Tags="&lt;apache-spark&gt;&lt;docker&gt;&lt;kubernetes&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49167158" PostTypeId="1" AcceptedAnswerId="49171593" CreationDate="2018-03-08T06:57:47.700" Score="0" ViewCount="46" Body="&lt;p&gt;I have a below dataset,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+---------+---------+----------+-----------+-----------+-----------+&#xA;| Column1 | Column2 | Column3  | Exspense1 | Exspense2 | Exspense3 |&#xA;+---------+---------+----------+-----------+-----------+-----------+&#xA;| null    | null    | null     | 175935.40 |   2557400 |         0 |&#xA;| null    | null    | 20160511 | 94598.40  |  13050360 |         0 |&#xA;| null    | null    | 20160512 | 81337.00  |  12523645 |         0 |&#xA;| null    | Item1   | null     | 24955.20  |   4206475 |         0 |&#xA;| null    | Item1   | 20160511 | 14143.30  |   2357534 |         0 |&#xA;| null    | Item1   | 20160512 | 10811.90  |   1848941 |         0 |&#xA;| null    | Item2   | null     | 26725.20  |   2188031 |         0 |&#xA;| null    | Item2   | 20160511 | 17807.50  |   1400011 |         0 |&#xA;| null    | Item2   | 20160512 | 8917.70   |    788020 |         0 |&#xA;| null    | Item3   | null     | 19234.30  |   2787529 |         0 |&#xA;| null    | Item3   | 20160511 | 8204.30   |   1162487 |         0 |&#xA;| null    | Item3   | 20160512 | 11030.00  |   1625042 |         0 |&#xA;| null    | Item4   | null     | 85239.20  |  13848186 |         0 |&#xA;| null    | Item4   | 20160511 | 47324.10  |   7157838 |         0 |&#xA;| null    | Item4   | 20160512 | 37915.10  |   6690348 |         0 |&#xA;| null    | Item5   | null     | 19781.50  |   2543784 |         0 |&#xA;| null    | Item5   | 20160511 | 7119.209  |     72490 |         0 |&#xA;| null    | Item5   | 20160512 | 12662.30  |   1571294 |         0 |&#xA;| Shop1   | null    | null     | 35.70     |     10577 |         0 |&#xA;| Shop1   | null    | 20160512 | 35.701    |      0577 |         0 |&#xA;| Shop1   | Item1   | null     | 34.40     |     10538 |         0 |&#xA;| Shop1   | Item1   | 20160512 | 34.401    |      0538 |         0 |&#xA;| Shop1   | Item3   | null     | 1.30      |        39 |         0 |&#xA;| Shop1   | Item3   | 20160512 | 1.30      |        39 |         0 |&#xA;| Shop2   | null    | null     | 10757.30  |   2163921 |         0 |&#xA;| Shop2   | null    | 20160511 | 6672.20   |   1286947 |         0 |&#xA;| Shop2   | null    | 20160512 | 4085.10   |    876974 |         0 |&#xA;| Shop2   | Item1   | null     | 1510.30   |    370818 |         0 |&#xA;| Shop2   | Item1   | 20160511 | 752.101   |     90052 |         0 |&#xA;| Shop2   | Item1   | 20160512 | 758.201   |     80766 |         0 |&#xA;+---------+---------+----------+-----------+-----------+-----------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm following a check foreg: &lt;code&gt;boolean sumCheck&lt;/code&gt; for each column below,&#xA;where I have to loop through each column. Now,&lt;/p&gt;&#xA;&#xA;&lt;p&gt;1.for Column1 if &lt;code&gt;sumCheck is true&lt;/code&gt; I have to filter lines where Column1 is not null and same row prevoius column is null, Since Column1 is first column so no filter,&lt;/p&gt;&#xA;&#xA;&lt;ol start=&quot;2&quot;&gt;&#xA;&lt;li&gt;For Column 2:&#xA;if check is true,&#xA;Then I have to filter rows where &lt;em&gt;Column2 is not null&lt;/em&gt; and &lt;em&gt;Column1 is null&lt;/em&gt;&#xA;that means I dont want the rows where &lt;code&gt;(Column2 is not null and Column1 is null)&lt;/code&gt;&#xA;I have to get below,&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;div class=&quot;snippet&quot; data-lang=&quot;js&quot; data-hide=&quot;false&quot; data-console=&quot;true&quot; data-babel=&quot;false&quot;&gt;&#xD;&#xA;&lt;div class=&quot;snippet-code&quot;&gt;&#xD;&#xA;&lt;pre class=&quot;snippet-code-html lang-html prettyprint-override&quot;&gt;&lt;code&gt;&amp;lt;table&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;th&amp;gt;Column1&amp;lt;/th&amp;gt;&amp;lt;th&amp;gt;Column2&amp;lt;/th&amp;gt;&amp;lt;th&amp;gt;Column3&amp;lt;/th&amp;gt;&amp;lt;th&amp;gt;Exspense1&amp;lt;/th&amp;gt;&amp;lt;th&amp;gt;Exspense2&amp;lt;/th&amp;gt;&amp;lt;th&amp;gt;Exspense3&amp;lt;/th&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;175935.40&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;2557400&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;20160511&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;94598.40&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;13050360&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;20160512&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;81337.00&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;12523645&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;35.70&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;10577&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;20160512&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;35.701&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0577&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;Item1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;34.40&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;10538&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;Item1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;20160512&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;34.401&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0538&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;Item3&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;1.30&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;39&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;Item3&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;20160512&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;1.30&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;39&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop2&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;10757.30&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;2163921&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop2&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;20160511&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;6672.20&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;1286947&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop2&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;20160512&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;4085.10&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;876974&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop2&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;Item1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;1510.30&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;370818&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop2&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;Item1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;20160511&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;752.101&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;90052&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop2&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;Item1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;20160512&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;758.201&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;80766&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;/code&gt;&lt;/pre&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;/p&gt;&#xA;&#xA;&lt;ol start=&quot;3&quot;&gt;&#xA;&lt;li&gt;For Column 3 if check is true the I have to filter the dataset so that where Column3 is &#xA;I have to remove rows where Column3 is not null and Column2 is null;&#xA;So that I get below ,&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;div class=&quot;snippet&quot; data-lang=&quot;js&quot; data-hide=&quot;false&quot; data-console=&quot;true&quot; data-babel=&quot;false&quot;&gt;&#xD;&#xA;&lt;div class=&quot;snippet-code&quot;&gt;&#xD;&#xA;&lt;pre class=&quot;snippet-code-html lang-html prettyprint-override&quot;&gt;&lt;code&gt;&amp;lt;table&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;th&amp;gt;Column1&amp;lt;/th&amp;gt;&amp;lt;th&amp;gt;Column2&amp;lt;/th&amp;gt;&amp;lt;th&amp;gt;Column3&amp;lt;/th&amp;gt;&amp;lt;th&amp;gt;Exspense1&amp;lt;/th&amp;gt;&amp;lt;th&amp;gt;Exspense2&amp;lt;/th&amp;gt;&amp;lt;th&amp;gt;Exspense3&amp;lt;/th&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;175935.40&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;2557400&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;35.70&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;10577&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;Item1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;34.40&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;10538&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;Item1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;20160512&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;34.401&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0538&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;Item3&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;1.30&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;39&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;Item3&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;20160512&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;1.30&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;39&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop2&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;10757.30&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;2163921&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop2&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;Item1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;null&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;1510.30&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;370818&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop2&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;Item1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;20160511&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;752.101&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;90052&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;tr&amp;gt;&amp;lt;td&amp;gt;Shop2&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;Item1&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;20160512&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;758.201&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;80766&amp;lt;/td&amp;gt;&amp;lt;td&amp;gt;0&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;&lt;/code&gt;&lt;/pre&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I currently I do below steps:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;for Each Column Size I loop and see the flag;&#xA;I start from Second Column:&#xA;for Second Col:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val exceptDf=dataset.filter(&quot;Column2 is not null and Column 1 is null&quot;);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;for Third Col: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val  exceptDf3=exceptDf.union(dataset.filter(&quot;Column3 is not null and Column 2 is null&quot;));&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and Finally i Do &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;dataset.except(exceptDf3);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Since I'm doing using &lt;code&gt;union except filter&lt;/code&gt; I just wanted to see if there's any method or &lt;code&gt;filter&lt;/code&gt; only that would avoid me from using the &lt;code&gt;unions&lt;/code&gt; and &lt;code&gt;exept&lt;/code&gt; functions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please help me in getting the desired results.&lt;/p&gt;&#xA;" OwnerUserId="9444445" LastEditorUserId="9444445" LastEditDate="2018-03-08T13:08:51.763" LastActivityDate="2018-03-08T13:08:51.763" Title="Where Condition in spark" Tags="&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;apache-spark-dataset&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="49167656" PostTypeId="1" CreationDate="2018-03-08T07:30:35.250" Score="1" ViewCount="33" Body="&lt;p&gt;I am trying to separate the website name from the URL. For example - if the URL is www.google.com, the output should be &quot;google&quot;. I tried the below code and everything works fine except the last line  - &quot;websites.collect()&quot;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I used a dataframe to store the website names and then converted it to a rdd and applied a split function on the values to get my required output. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The logic seems to be fine but I guess there is some error in my packages configuration and installation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The error is shown below:-&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;---------------------------------------------------------------------------&#xA;Py4JJavaError                             Traceback (most recent call last)&#xA;&amp;lt;ipython-input-11-a88287400951&amp;gt; in &amp;lt;module&amp;gt;()&#xA;----&amp;gt; 1 websites.collect()&#xA;&#xA;C:\ProgramData\Anaconda3\lib\site-packages\pyspark\rdd.py in collect(self)&#xA;    822         &quot;&quot;&quot;&#xA;    823         with SCCallSiteSync(self.context) as css:&#xA;--&amp;gt; 824             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())&#xA;    825         return list(_load_from_socket(port, self._jrdd_deserializer))&#xA;    826 &#xA;&#xA;C:\ProgramData\Anaconda3\lib\site-packages\py4j\java_gateway.py in __call__(self, *args)&#xA;   1158         answer = self.gateway_client.send_command(command)&#xA;   1159         return_value = get_return_value(&#xA;-&amp;gt; 1160             answer, self.gateway_client, self.target_id, self.name)&#xA;   1161 &#xA;   1162         for temp_arg in temp_args:&#xA;&#xA;C:\ProgramData\Anaconda3\lib\site-packages\pyspark\sql\utils.py in deco(*a, **kw)&#xA;     61     def deco(*a, **kw):&#xA;     62         try:&#xA;---&amp;gt; 63             return f(*a, **kw)&#xA;     64         except py4j.protocol.Py4JJavaError as e:&#xA;     65             s = e.java_exception.toString()&#xA;&#xA;C:\ProgramData\Anaconda3\lib\site-packages\py4j\protocol.py in get_return_value(answer, gateway_client, target_id, name)&#xA;    318                 raise Py4JJavaError(&#xA;    319                     &quot;An error occurred while calling {0}{1}{2}.\n&quot;.&#xA;--&amp;gt; 320                     format(target_id, &quot;.&quot;, name), value)&#xA;    321             else:&#xA;    322                 raise Py4JError(&#xA;&#xA;Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.&#xA;: java.lang.IllegalArgumentException&#xA;    at org.apache.xbean.asm5.ClassReader.&amp;lt;init&amp;gt;(Unknown Source)&#xA;    at org.apache.xbean.asm5.ClassReader.&amp;lt;init&amp;gt;(Unknown Source)&#xA;    at org.apache.xbean.asm5.ClassReader.&amp;lt;init&amp;gt;(Unknown Source)&#xA;    at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46)&#xA;    at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:449)&#xA;    at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:432)&#xA;    at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)&#xA;    at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)&#xA;    at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)&#xA;    at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)&#xA;    at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)&#xA;    at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)&#xA;    at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)&#xA;    at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:432)&#xA;    at org.apache.xbean.asm5.ClassReader.a(Unknown Source)&#xA;    at org.apache.xbean.asm5.ClassReader.b(Unknown Source)&#xA;    at org.apache.xbean.asm5.ClassReader.accept(Unknown Source)&#xA;    at org.apache.xbean.asm5.ClassReader.accept(Unknown Source)&#xA;    at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:262)&#xA;    at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:261)&#xA;    at scala.collection.immutable.List.foreach(List.scala:381)&#xA;    at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:261)&#xA;    at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159)&#xA;    at org.apache.spark.SparkContext.clean(SparkContext.scala:2292)&#xA;    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2066)&#xA;    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)&#xA;    at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)&#xA;    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&#xA;    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)&#xA;    at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)&#xA;    at org.apache.spark.rdd.RDD.collect(RDD.scala:938)&#xA;    at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)&#xA;    at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)&#xA;    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#xA;    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)&#xA;    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)&#xA;    at java.base/java.lang.reflect.Method.invoke(Unknown Source)&#xA;    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#xA;    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#xA;    at py4j.Gateway.invoke(Gateway.java:282)&#xA;    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#xA;    at py4j.commands.CallCommand.execute(CallCommand.java:79)&#xA;    at py4j.GatewayConnection.run(GatewayConnection.java:214)&#xA;    at java.base/java.lang.Thread.run(Unknown Source)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;CODE:-&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark import SparkConf, SparkContext&#xA;conf = (SparkConf()&#xA;         .setMaster(&quot;local[*]&quot;)&#xA;         .setAppName(&quot;Test&quot;)&#xA;         .set(&quot;spark.executor.memory&quot;, &quot;8g&quot;)&#xA;       )&#xA;&#xA;sc = SparkContext(conf = conf)&#xA;from pyspark.sql import SQLContext&#xA;sqlContext = SQLContext(sc) &#xA;&#xA;schemaWebsite = sc.parallelize([&#xA;    (0, &quot;www.google.com&quot;), (1, &quot;www.hackerrank.com&quot;),(2, &quot;www.walmart.com/in&quot;),&#xA;    (3, &quot;www.amazon.in&quot;),(4, &quot;www.ndtv.com&quot;)]).toDF([&quot;id&quot;, &quot;ev&quot;])&#xA;&#xA;websites = schemaWebsite.rdd.map(lambda x : x[1].split(&quot;.&quot;)[1])&#xA;websites.collect()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="6484734" LastEditorUserId="5088142" LastEditDate="2018-03-08T07:40:13.710" LastActivityDate="2018-03-08T07:53:10.183" Title="ERROR WHILE RUNNING collect() in PYSPARK" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;rdd&gt;" AnswerCount="0" CommentCount="4" />
  <row Id="49168338" PostTypeId="1" CreationDate="2018-03-08T08:13:02.250" Score="0" ViewCount="27" Body="&lt;p&gt;I have following DSE cluster configuration : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;6 nodes with 6 cores/16GB ram for each node.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My app is build using pyspark that read data from Cassandra DB.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We load on cassandra db 320.000.000 rows and run my python spark application with full memory and cores and have this error :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Lost task 97.0 in stage 299.0 (TID 14680, 11.218.78.15): java.io.IOException: No space left on device&#xA;at java.io.FileOutputStream.writeBytes(Native Method)&#xA;at java.io.FileOutputStream.write(FileOutputStream.java:326)&#xA;at org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)&#xA;at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)&#xA;at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)&#xA;at net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:205)&#xA;at net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:158)&#xA;at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)&#xA;at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)&#xA;at java.io.DataOutputStream.write(DataOutputStream.java:107)&#xA;at org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:562)&#xA;at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2.writeValue(UnsafeRowSerializer.scala:69)&#xA;at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:185)&#xA;at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:150)&#xA;at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)&#xA;at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)&#xA;at org.apache.spark.scheduler.Task.run(Task.scala:86)&#xA;at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)&#xA;at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;at java.lang.Thread.run(Thread.java:748)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Could you help me? I Have about 20GB on every node.&lt;/p&gt;&#xA;" OwnerUserId="5814847" LastActivityDate="2018-03-08T10:05:45.767" Title="Spark job performance issue" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;datastax-enterprise&gt;&lt;pyspark-sql&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="49168416" PostTypeId="1" CreationDate="2018-03-08T08:17:50.013" Score="0" ViewCount="7" Body="&lt;p&gt;I am using Zeppelin notebook to do some analysis using &lt;code&gt;spark.r&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I however cannot seem to get the default Zeppelin visualizations(graphs etc.) when am displaying data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For example if I was to do:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;query &amp;lt;- dbGetQuery(connection, statement= paste(&quot;&#xA;                    select col1, col2&#xA;                    from table&quot;))&#xA;&#xA;query&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It just gets displayed in a not so neat format.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I try with other interpreters like psql, the visualizations are there by default.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is this a common problem with spark.r or am I missing something?&lt;/p&gt;&#xA;" OwnerUserId="3026846" LastActivityDate="2018-03-08T08:17:50.013" Title="Zeppelin visuals using spark.r" Tags="&lt;r&gt;&lt;apache-spark&gt;&lt;apache-zeppelin&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49168960" PostTypeId="1" AcceptedAnswerId="49169839" CreationDate="2018-03-08T08:49:18.100" Score="0" ViewCount="27" Body="&lt;p&gt;I have directories in s3 in following format,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; &amp;lt;base-directory&amp;gt;/users/users=20180303/hour=0/&amp;lt;parquet files&amp;gt;&#xA; &amp;lt;base-directory&amp;gt;/users/users=20180303/hour=1/&amp;lt;parquet files&amp;gt;&#xA; ....&#xA; &amp;lt;base-directory&amp;gt;/users/users=20180302/hour=&amp;lt;0 to 23&amp;gt;/&amp;lt;parquet files&amp;gt;&#xA; &amp;lt;base-directory&amp;gt;/users/users=20180301/hour=&amp;lt;0 to 23&amp;gt;/&amp;lt;parquet files&amp;gt;&#xA; ....&#xA; &amp;lt;base-directory&amp;gt;/users/users=20180228/hour=&amp;lt;0 to 23&amp;gt;/&amp;lt;parquet files&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Basically I have hourly subdirectories in daily directories.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I want to process parquet files the last 30 days.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have tried following,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; val df = sqlContext.read.option(&quot;header&quot;, &quot;true&quot;)&#xA;    .parquet(&amp;lt;base-directory&amp;gt; + File.separator + &quot;users&quot; + File.separator)&#xA;    .where(col(&quot;users&quot;).between(startDate, endDate))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;where endDate and startDate are separated by 30 days and in yyyymmdd format.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Above solution is not giving correct subset of directories. What am I doing wrong ?&lt;/p&gt;&#xA;" OwnerUserId="1474659" LastActivityDate="2018-03-08T10:04:16.967" Title="How to read multiple directories in s3 in spark Scala?" Tags="&lt;apache-spark&gt;&lt;apache-spark-sql&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49169772" PostTypeId="1" CreationDate="2018-03-08T09:36:17.247" Score="-2" ViewCount="42" Body="&lt;p&gt;I am making a multi-nominal regression model in pyspark and after running my linear regression model it gives me this error &#xA;        &quot;IllegalArgumentException: u'requirement failed: Column label must be  of type NumericType but was actually of type StringType.&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please help me here as I have spent so much time to resolve this but couldn't ale to solve.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    lr_data=   loan_data.select('int_rate','loan_amnt','term','grade','sub_grade','emp_length','verification_status','home_ownership','annual_inc','purpose','addr_state','open_acc') &#xA;    lr_data.printSchema()&#xA;&#xA;    root&#xA;    |-- int_rate: string (nullable = true)&#xA;    |-- loan_amnt: integer (nullable = true)&#xA;    |-- term: string (nullable = true)&#xA;    |-- grade: string (nullable = true)&#xA;    |-- sub_grade: string (nullable = true)&#xA;    |-- emp_length: string (nullable = true)&#xA;    |-- verification_status: string (nullable = true)&#xA;    |-- home_ownership: string (nullable = true)&#xA;    |-- annual_inc: double (nullable = true)&#xA;    |-- purpose: string (nullable = true)&#xA;    |-- addr_state: string (nullable = true)&#xA;    |-- open_acc: string (nullable = true)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;here in the multinominol regression model my target variable should be int_rate(which is string type, probably that's why i am getting this error while running).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;but initially i tried using only two values in the regression model which are int_rate,loan_amnt.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;here is the code &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    input_data=lr_data.rdd.map(lambda x:(x[0], DenseVector(x[1:2])))&#xA;    data3= spark.createDataFrame(input_data,[&quot;label&quot;,&quot;features&quot;,])&#xA;    data3.printSchema()&#xA;&#xA;   root&#xA;   |-- label: string (nullable = true)&#xA;   |-- features: vector (nullable = true)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;IMP:NOTE I tried with taking other variables in DenseVector array but it was throwing me long error something like invalide literal for float(): 36 months&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;   usr/local/spark/python/pyspark/sql/session.pyc in createDataFrame(self,    data, schema, samplingRatio, verifySchema)&#xA;    580 &#xA;    581         if isinstance(data, RDD):&#xA;    582  rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)&#xA;   583         else:&#xA;   584             rdd, schema = self._createFromLocal(map(prepare, data), schema)&#xA;    if schema is None or isinstance(schema, (list, tuple)):&#xA;    380             struct = self._inferSchema(rdd, samplingRatio)&#xA;    381             converter = _create_converter(struct)&#xA;    382             rdd = rdd.map(converter)&#xA;&#xA;   /usr/local/spark/python/pyspark/sql/session.pyc in _inferSchema(self,   rdd, samplingRatio)&#xA;    349         :return: :class:`pyspark.sql.types.StructType`&#xA;    350         &quot;&quot;&quot;&#xA;    351         first = rdd.first()&#xA;    352         if not first:&#xA;    353             raise ValueError(&quot;The first row in RDD is empty, &quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Please tell me how to select more than 2 variable in this regression model as well. I guess i have to typecast every variable in my data set.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;   #spilt into two partition &#xA;   train_data, test_data = data3.randomSplit([.7,.3], seed = 1)&#xA;   lr = LinearRegression(labelCol=&quot;label&quot;, maxIter=100, regParam= 0.3, elasticNetParam = 0.8)&#xA;   linearModel = lr.fit(train_data)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now when i am running this linearmodel() I am getting this below error.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    IllegalArgumentException Traceback (most recent call  last)&#xA;   &amp;lt;ipython-input-20-5f84d575334f&amp;gt; in &amp;lt;module&amp;gt;()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;----&gt;    1 linearModel = lr.fit(train_data)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;     /usr/local/spark/python/pyspark/ml/base.pyc in fit(self,dataset,params) &#xA;      62                 return self.copy(params)._fit(dataset)&#xA;      63             else:&#xA;      64                 return self._fit(dataset)&#xA;      65         else:&#xA;      66             raise ValueError(&quot;Params must be either a param map  or a list/tuple of param maps, &quot;&#xA;&#xA;      /usr/local/spark/python/pyspark/ml/wrapper.pyc in _fit(self, dataset)&#xA;      263 &#xA;      264     def _fit(self, dataset):&#xA;      265         java_model = self._fit_java(dataset)&#xA;      266         return self._create_model(java_model)&#xA;      267 &#xA;&#xA;      /usr/local/spark/python/pyspark/ml/wrapper.pyc in _fit_java(self, dataset)&#xA;        260         &quot;&quot;&quot;&#xA;        261         self._transfer_params_to_java()&#xA;        262         return self._java_obj.fit(dataset._jdf)&#xA;        263 &#xA;        264     def _fit(self, dataset):&#xA;&#xA;       /usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args)&#xA;        1131         answer = self.gateway_client.send_command(command)&#xA;        1132         return_value = get_return_value(&#xA;        1133             answer, self.gateway_client, self.target_id, self.name)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;1134 &#xA;   1135         for temp_arg in temp_args:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;       /usr/local/spark/python/pyspark/sql/utils.pyc in deco(*a, **kw)&#xA;        77                 raise QueryExecutionException(s.split(': ', 1)[1], stackTrace)&#xA;        78             if  s.startswith('java.lang.IllegalArgumentException: '):&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;---&gt;        79                 raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)&#xA;            80             raise&#xA;            81     return deco&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;        IllegalArgumentException: u'requirement failed: Column label must be of type NumericType but was actually of type StringType.'&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Please help me, I have tried every method of casting string value to numeric but doesn't make any difference. As my int_rate which is target variable is string types by deafult but it takes value of numeric.one more is I have to select the whole lr data set in my regression model. How can i do this. &#xA;Thanks in advance :)&lt;/p&gt;&#xA;" OwnerUserId="9459842" LastEditorUserId="4685471" LastEditDate="2018-03-08T16:11:35.000" LastActivityDate="2018-03-08T16:11:35.000" Title="pyspark linear regression model gives error this column name must be numeric type but was actually string type" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49170282" PostTypeId="1" CreationDate="2018-03-08T09:59:41.413" Score="0" ViewCount="9" Body="&lt;p&gt;I have loaded a dataset which is just around ~ 20 GB in size - the cluster has ~ 1TB available so memory shouldn't be an issue imho.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;It is no problem for me to save the original data which consists only of strings:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;df_data.write.parquet(os.path.join(DATA_SET_BASE, 'concatenated.parquet'), mode='overwrite')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, as I transform the data:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;df_transformed = df_data.drop('bri').join(&#xA;    df_data[['docId', 'bri']].rdd\&#xA;        .map(lambda x: (x.docId, json.loads(x.bri)) &#xA;             if x.bri is not None else (x.docId, dict()))\&#xA;        .toDF()\&#xA;        .withColumnRenamed('_1', 'docId')\&#xA;        .withColumnRenamed('_2', 'bri'),&#xA;    ['dokumentId']&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and then save it:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;df_transformed.parquet(os.path.join(DATA_SET_BASE, 'concatenated.parquet'), mode='overwrite')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The log output will tell me that the memory limit was exceeded:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;18/03/08 10:23:09 WARN TaskSetManager: Lost task 17.0 in stage 18.3 (TID 2866, worker06.hadoop.know-center.at): ExecutorLostFailure (executor 40 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 15.2 GB of 13.5 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.&#xA;18/03/08 10:23:09 WARN TaskSetManager: Lost task 29.0 in stage 18.3 (TID 2878, worker06.hadoop.know-center.at): ExecutorLostFailure (executor 40 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 15.2 GB of 13.5 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.&#xA;18/03/08 10:23:09 WARN TaskSetManager: Lost task 65.0 in stage 18.3 (TID 2914, worker06.hadoop.know-center.at): ExecutorLostFailure (executor 40 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 15.2 GB of 13.5 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm not quite sure what the problem is. Even setting the executor's memory to 60GB RAM each does not solve the problem. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, obviously the problem comes with the transformation. Any idea what exactly causes this problem?&lt;/p&gt;&#xA;" OwnerUserId="826983" LastActivityDate="2018-03-08T09:59:41.413" Title="Losing executors when saving parquet file" Tags="&lt;pyspark&gt;&lt;parquet&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49170584" PostTypeId="1" CreationDate="2018-03-08T10:14:32.987" Score="0" ViewCount="9" Body="&lt;p&gt;I use Spark2.2 to read data in Elasticsearch2.3. The error description is in the image.&#xA;&lt;a href=&quot;https://i.stack.imgur.com/xgPoO.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;error image description&lt;/a&gt;&#xA;and my code is here &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val conf = new SparkConf().setAppName(&quot;esRDDtest&quot;)&#xA;conf.set(&quot;es.nodes&quot;,&quot;es.data.abc.com&quot;)&#xA;conf.set(&quot;es.port&quot;,&quot;80&quot;)&#xA;conf.set(&quot;es.index.auto.create&quot;, &quot;true&quot;)&#xA;conf.set(&quot;es.node.client.only&quot;,&quot;true&quot;)&#xA;conf.set(&quot;es.nodes.wan.only&quot;,&quot;true&quot;)&#xA;conf.set(&quot;es.mapping.date.rich&quot;,&quot;false&quot;)&#xA;//conf.set(&quot;es.batch.size.entries&quot;,20000)&#xA;//conf.set(&quot;es.index.read.missing.as.empty&quot;,&quot;true&quot;)&#xA;val sc = new SparkContext(conf)&#xA;val resource = &quot;log.abc_test_report_all&quot;&#xA;val query = &quot;?q=ip:100.101.142.54&quot;&#xA;val eslogs = sc.esRDD(resource,query)&#xA;println(eslogs.collect())&#xA;&#xA;&#xA;I wonder why the error it is and How to correct it .&#xA;I would appreciate it for any help&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="5531685" LastActivityDate="2018-03-08T10:14:32.987" Title="Spark read data in Elasticsearch" Tags="&lt;java&gt;&lt;apache-spark&gt;&lt;elasticsearch&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49170709" PostTypeId="1" CreationDate="2018-03-08T10:21:07.690" Score="0" ViewCount="27" Body="&lt;p&gt;&lt;strong&gt;BackGround:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Our project is build on PlayFrameWork. &lt;/li&gt;&#xA;&lt;li&gt;Front-end language: JavaScript&lt;/li&gt;&#xA;&lt;li&gt;Back-end language: Scala&lt;/li&gt;&#xA;&lt;li&gt;we are develope a web application,the server is a cluster.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Want to achieve:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;In the web UI, User first input some parameters which about query, and click the button such as &quot;submit&quot;.Then these parameters will be sent to backend. (This is easy，obviously)&lt;/li&gt;&#xA;&lt;li&gt;When backend get parameters, backend start reading and process the data which store in HDFS. Data processing include data-cleaning,filtering and other operations such as clustering algorithms,not just a spark-sql query. All These operations need to run on spark cluster&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;&lt;em&gt;We needn't manually pack a fat jar and submit it to cluster and send the result to front-end&lt;/em&gt;&lt;/strong&gt;（These are what bothering me!）&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What we have done:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;We build a spark-project separately in IDEA. When we get parameters, we manually assign these parameters to variables in spark-project.&lt;/li&gt;&#xA;&lt;li&gt;Then &lt;strong&gt;&quot;Build Artifacts&quot;-&gt;&quot;Bulid&quot;&lt;/strong&gt; to get a fat jar.&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Then submit by two approaches: &lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;&lt;strong&gt;&quot;spark-submit --class main.scala.Test --master yarn /path.jar&quot;&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;run scala code directly in IDEA on local mode (if change to Yarn, will throw Exceptions).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;When program execution finished, we get the processed_data and store it.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;Then read the processed_data's path and pass it to front-end.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;All are not user interactively submit. Very stupid!&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So if I am a user, I want to query or process data on cluster and get feedback on front-end conveniently.&lt;br&gt;&#xA;What should i do?&lt;br&gt;&#xA;Which tools or lib could use?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks!&lt;/p&gt;&#xA;" OwnerUserId="8740367" LastEditorUserId="8740367" LastEditDate="2018-03-08T12:46:05.327" LastActivityDate="2018-03-08T12:46:05.327" Title="How to interactive submit spark task in Web application's User interface?" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="49171071" PostTypeId="1" CreationDate="2018-03-08T10:39:31.180" Score="0" ViewCount="26" Body="&lt;p&gt;I have a piece of pyspark code the converts a dataframe into a physical table:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df.write.mode('overwrite).saveAsTable('sometablename')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In case the dataframe, df, contains columns which have spaces in their names it fails with the following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;18/03/08 10:33:29 ERROR CreateDataSourceTableAsSelectCommand: Failed to write to table pivot_up_spaces_Export_Data_4&#xA;org.apache.spark.sql.AnalysisException: Attribute name &quot;SUM_count_col umn&quot; contains invalid character(s) among &quot; ,;{}()\n\t=&quot;. Please use alias to rename it.;&#xA;        at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkConversionRequirement(ParquetSchemaConverter.scala:581)&#xA;        at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkFieldName(ParquetSchemaConverter.scala:567)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;when I use registerTempTable on the same table, things work fine:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df.registerTempTable('sometablename')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, I in spark-sql, I am able to create tables which have spaces in the column names. Is there any way I can get around this situation in pyspark ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am running this on a EMR 5.10.0 cluster which internally uses Spark 2.2.0.&lt;/p&gt;&#xA;" OwnerUserId="2809743" LastEditorUserId="2809743" LastEditDate="2018-03-08T13:02:15.787" LastActivityDate="2018-03-08T13:02:15.787" Title="saveAsTable for column with spaces failing" Tags="&lt;pyspark&gt;&lt;apache-spark-sql&gt;&lt;spark-dataframe&gt;&lt;emr&gt;&lt;amazon-emr&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="49171272" PostTypeId="1" AcceptedAnswerId="49172624" CreationDate="2018-03-08T10:49:11.833" Score="1" ViewCount="42" Body="&lt;p&gt;When we write&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;RDD.map(x =&amp;gt; x + 1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This corresponds to a task that the &lt;code&gt;master&lt;/code&gt; will send to all the workers to execute inside their partition.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But I'm interested in the detail of these magics. Let's say we submit a jar that contains all these functions using &lt;code&gt;spark-submit&lt;/code&gt;. Once this jar submitted to the &lt;code&gt;master&lt;/code&gt;, how the master do to understand and extract all these transformations and send it to all the workers ? Does it use the &lt;code&gt;reflecton&lt;/code&gt; mechanism of java ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;For the sake of example, can you make a simple &lt;code&gt;map&lt;/code&gt; and use for example &lt;code&gt;akka&lt;/code&gt; under the hood to do the same magics?&lt;/p&gt;&#xA;" OwnerUserId="631914" LastEditorUserId="2707792" LastEditDate="2018-03-08T11:44:47.550" LastActivityDate="2018-03-08T12:07:58.297" Title="How Spark distribute tasks to multiple workers" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49171385" PostTypeId="1" CreationDate="2018-03-08T10:54:37.040" Score="0" ViewCount="26" Body=" &#xA;&#xA;&lt;p&gt;I have Id, StartDateTime, EndDateTime in my DataFrame&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;Id      StartDateTime                   EndDateTime&#xA;165     2017-06-05 12:45:14             2017-06-07 21:00:01  # Goes on for 2 days&#xA;166     2017-06-05 14:02:00             2017-06-05 14:22:45  # starts at 14:02 and ends at 14:22 (20 mmns)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to split the total difference into 4 buckets (6 hrs each) and day. (Data in minutes)&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;Id        Day     00 - 06      06 - 12     12 - 18    18 - 24 &#xA;165       MON     0            0           314        360&#xA;165       TUE     360          360         360        360&#xA;165       WED     360          360         360        180&#xA;166       WED     0            0           20         0  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I tried with &lt;code&gt;&quot;datetime.timedelta&quot;&lt;/code&gt; to create time series but I couldnot achieve it for multiple days and time buckets. Any easy way in Spark (pySpark)?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My Code snippet &lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;myDf = (&#xA;    sc.parallelize([&#xA;        (34, 30, &quot;2016-07-02 12:01:40&quot;, &quot;2016-07-02 14:01:40&quot;), (184, 32, &quot;2016-07-03 12:21:23&quot;, &quot;2016-07-07 12:01:40&quot;)&#xA;    ]).toDF([&quot;itemid&quot;, &quot;eventid&quot;, &quot;startTime&quot;, &quot;endTime&quot;])&#xA;    .withColumn(&quot;startTime&quot;, col(&quot;startTime&quot;).cast(&quot;timestamp&quot;))&#xA;)&#xA;myDf = myDf.withColumn(&quot;endTime&quot;, col(&quot;endTime&quot;).cast(&quot;timestamp&quot;))&#xA;timeFormat = &quot;yyyy-MM-dd'T'HH:mm:ss.SSS&quot;&#xA;timeFormat = (F.unix_timestamp('endTime', format=timeFormat) - F.unix_timestamp('startTime', format=timeFormat))&#xA;myDf = myDf.withColumn(&quot;Trip Duration&quot;, timeFormat)&#xA;myDf = myDf.withColumn(&quot;Day&quot; , date_format('startTime', 'E'))&#xA;myDf.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="903521" LastEditorUserId="5858851" LastEditDate="2018-03-08T16:12:13.397" LastActivityDate="2018-03-08T16:12:13.397" Title="Generating time slots in PySpark" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="49171705" PostTypeId="1" CreationDate="2018-03-08T11:10:58.407" Score="0" ViewCount="6" Body="&lt;p&gt;My use is the following. Consider I have a pyspark dataframe which has the following format:&#xA;df.columns:&#xA;1. hh: Contains the hour of the day (type int)&#xA;2. userId :  some unique identifier.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What I want to do is I want to figure out list of userIds which have anomalous hits onto the page. So I first do a groupby as so:&#xA;df=df.groupby(&quot;hh&quot;,&quot;userId).count().alias(&quot;LoginCounts)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now the format of the dataframe would be:&#xA;1. hh&#xA;2. userId&#xA;3.LoginCounts: Number of times a specific user logs in at a particular hour.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to use the pyspark kde function as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark.mllib.stat import KernelDensity&#xA;kd=KernelDensity()&#xA;kd.setSample(df.select(&quot;LoginCounts&quot;).rdd)&#xA;kd.estimate([13.0,14.0]).&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I get the error:&#xA;Py4JJavaError: An error occurred while calling o647.estimateKernelDensity.&#xA;: org.apache.spark.SparkException: Job aborted due to stage failure&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now my end goal is to fit a kde on say a day's hour based data and then use the next day's data to get the probability estimates for each login count. &#xA;Eg: I would like to achieve something of this nature:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df.withColumn(&quot;kdeProbs&quot;,kde.estimate(col(&quot;LoginCounts)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So the column kdeProbs will contain P(LoginCount=x | estimated kde).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have tried searching for an example of the same but am always redirected to the standard kde example on the spark.apache.org page, which does not solve my case. &lt;/p&gt;&#xA;" OwnerUserId="9169198" LastActivityDate="2018-03-08T11:10:58.407" Title="How to fit a kernel density estimate on a pyspark dataframe column and use it for creating a new column with the estimates" Tags="&lt;pyspark&gt;&lt;kernel-density&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49171748" PostTypeId="1" AcceptedAnswerId="49172057" CreationDate="2018-03-08T11:13:40.063" Score="2" ViewCount="39" Body="&lt;p&gt;I have a DataFrame like this : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;finalSondDF.show()&#xA;    +---------------+------------+----------------+&#xA;    |webService_Name|responseTime|numberOfSameTime|&#xA;    +---------------+------------+----------------+&#xA;    |    webservice1|          80|               1|&#xA;    |    webservice1|          87|               2|&#xA;    |    webservice1|         283|               1|&#xA;    |    webservice2|          77|               2|&#xA;    |    webservice2|          80|               1|&#xA;    |    webservice2|          81|               1|&#xA;    |    webservice3|          63|               3|&#xA;    |    webservice3|         145|               1|&#xA;    |    webservice4|         167|               1|&#xA;    |    webservice4|         367|               2|&#xA;    |    webservice4|         500|               1|&#xA;    +---------------+------------+----------------+  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and I want to get a result like this :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+---------------+------------+----------------+------+&#xA;|webService_Name|responseTime|numberOfSameTime|Result|&#xA;+---------------+------------+----------------+------+&#xA;|    webservice1|          80|               1|     1|&#xA;|    webservice1|          87|               2|     3|  ==&amp;gt; 2+1&#xA;|    webservice1|         283|               1|     4|  ==&amp;gt; 1+2+1&#xA;|    webservice2|          77|               2|     2|  &#xA;|    webservice2|          80|               1|     3|  ==&amp;gt; 2+1&#xA;|    webservice2|          81|               1|     4|  ==&amp;gt; 2+1+1&#xA;|    webservice3|          63|               3|     3|&#xA;|    webservice3|         145|               1|     4|  ==&amp;gt; 3+1&#xA;|    webservice4|         167|               1|     1|&#xA;|    webservice4|         367|               2|     3|  ==&amp;gt; 1+2&#xA;|    webservice4|         500|               1|     4|  ==&amp;gt; 1+2+1&#xA;+---------------+------------+----------------+------+  &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;here the &lt;strong&gt;result&lt;/strong&gt; is the sum of &lt;code&gt;numberOfSameTime&lt;/code&gt; inferior of the current &lt;code&gt;responseTime&lt;/code&gt;&lt;br&gt;&#xA;I can't find a logic to do that. Can any one help me !!&lt;/p&gt;&#xA;" OwnerUserId="9412895" LastEditorUserId="6378311" LastEditDate="2018-03-08T11:58:40.750" LastActivityDate="2018-03-08T11:58:40.750" Title="Incremental addition with condition in dataframe" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;spark-dataframe&gt;" AnswerCount="2" CommentCount="1" ClosedDate="2018-03-08T12:54:31.537" />
  <row Id="49171810" PostTypeId="1" CreationDate="2018-03-08T11:17:33.870" Score="0" ViewCount="24" Body="&lt;p&gt;I have two kinds of task : A and B&lt;/p&gt;&#xA;&#xA;&lt;p&gt;(a task means a RDD's whole procedure, for example  RDD.map.reduce ... is one task. The RDD is defined by us, which is data separated to many partitions. each partition do its map job separately and will be combined together in reduce.)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;A is a short task which only takes less than 5s while B is a big task which will take more than 30 minutes to finish.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;We need to get the result of A as quick as possible, while B is a background task, we don't care even if B queued up for an hour or more.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Both A and B have many task partitions.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The case is, if B is schedule before A, A will wait a long time for B.&#xA;This is not allowed.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't think FAIR is a good way, because if B starts while A is executing, B will still starts its task partition which will affect the execution of A.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any way to give a priority to tasks? A have a higher priority over B.  Even A is scheduled after B, after the executing partition is finished, A will be executed immediately and the rest of B will be waiting.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Or is there any way to reserve task A some certain resource. Every time A is executed, it can always be scheduled immediately.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I found a way of using scheduler pool, but how to indicate A to a certain pool?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;I am using spark with java in standalone mode. I submit the job like javaRDD.map(..).reduce... The javaRDD is a sub-clesse extended form JavaRDD. Task A and B have different RDD class like ARDD and BRDD. They run in the same spark application.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The procedure is like: The app start up -&gt; spark application created, but no job runs -&gt; I click &quot;run A&quot; on the app ui, then ARDD will run. -&gt; I click &quot;run B&quot; on the app ui, then BRDD will run in the same spark application as A.&lt;/p&gt;&#xA;" OwnerUserId="7043615" LastEditorUserId="7043615" LastEditDate="2018-03-09T06:02:17.820" LastActivityDate="2018-03-09T06:02:17.820" Title="How to schedule a task with priority in spark?" Tags="&lt;java&gt;&lt;apache-spark&gt;" AnswerCount="0" CommentCount="5" />
  <row Id="49172300" PostTypeId="1" CreationDate="2018-03-08T11:45:17.793" Score="0" ViewCount="22" Body="&lt;p&gt;I have a problem in which i am counting number of errors occured in a spark streaming job and have to create metrics around it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But i want to write metric creation logic in sparkdriver so that it create metric for entire spark job.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The approach i thought of is to create an accumlator and increment it as soon as i get the error by one.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now i will write a sparkListener and use the accumlator value to generte metrics.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;SO the issue is that i am not able to read value of accumlator from listener as it is a different class. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can we reconstruct an accumlator in spark using sparkConstruct so that we can use the same accumlator across different componenets.&lt;/p&gt;&#xA;" OwnerUserId="4680712" LastEditorUserId="4680712" LastEditDate="2018-03-09T07:31:41.173" LastActivityDate="2018-03-09T07:31:41.173" Title="Reconstructing accumulator obj and reading its value in Spark Driver" Tags="&lt;java&gt;&lt;apache-spark&gt;&lt;spark-streaming&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49173055" PostTypeId="1" CreationDate="2018-03-08T12:24:26.343" Score="1" ViewCount="29" Body="&lt;p&gt;Im trying to run spark-submit to kubernetes cluster with spark 2.3 docker container image&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The challenge im facing is application have a mainapplication.jar and other dependency files &amp;amp; jars which are located in Remote location like AWS s3 ,but as per spark 2.3 documentation there is something called kubernetes init-container to download remote dependencies but in this case im not creating any Podspec to include init-containers in kubernetes, as per documentation Spark 2.3 spark/kubernetes internally creates Pods (driver,executor) So not sure how can i use init-container for spark-submit when there are remote dependencies.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://spark.apache.org/docs/latest/running-on-kubernetes.html#using-remote-dependencies&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://spark.apache.org/docs/latest/running-on-kubernetes.html#using-remote-dependencies&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please suggest&lt;/p&gt;&#xA;" OwnerUserId="1065358" LastEditorUserId="1065358" LastEditDate="2018-03-08T12:31:36.167" LastActivityDate="2018-03-08T12:31:36.167" Title="handling Remote dependencies for spark-submit in spark 2.3 with kubernetes" Tags="&lt;apache-spark&gt;&lt;amazon-s3&gt;&lt;kubernetes&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="49173079" PostTypeId="1" CreationDate="2018-03-08T12:26:19.770" Score="0" ViewCount="22" Body="&lt;p&gt;How can we handle below warning message in spark  - 1.6.2. Can someone help on this.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;java.lang.IllegalStateException: RpcEnv already stopped&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;WARN  2018-03-08 07:20:37,049 org.apache.spark.rpc.netty. Dispatcher:&#xA;  Message&#xA;  RemoteProcessDisconnected(prdshsvcs-cassandra-cluster-002:60090)&#xA;  dropped. java.lang.IllegalStateException: RpcEnv already stopped.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Thanks,&#xA;Chandra&lt;/p&gt;&#xA;" OwnerUserId="5433741" LastEditorUserId="4229270" LastEditDate="2018-03-08T15:32:36.153" LastActivityDate="2018-03-08T15:32:36.153" Title="java.lang.IllegalStateException: RpcEnv already stopped" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;datastax&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49173141" PostTypeId="1" CreationDate="2018-03-08T12:30:06.960" Score="0" ViewCount="20" Body="&lt;p&gt;I just installed VSCode and connected to HDInsight cluster, when I try to run &quot;PySpark Interactive&quot;,  I am getting below error &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&quot;Pip not found! Please make sure &lt;code&gt;pip&lt;/code&gt; in the system PATH, and then&#xA;  restart VSCode&quot;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;and in the OutPut console:&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;&quot;[Info] Error: spawn pip3 ENOENT&quot;.&lt;br&gt;&#xA;  &quot;[Error] undefined&quot;&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;I am following &lt;a href=&quot;https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-for-vscode&quot; rel=&quot;nofollow noreferrer&quot;&gt;this&lt;/a&gt;&#xA;I have installed Anaconda with Python 3.6, also I have added below path in my system PATH. Not sure if I am missing anything...&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;C:\ProgramData\Anaconda3;C:\ProgramData\Anaconda3\Library\mingw-w64\bin;C:\ProgramData\Anaconda3\Library\usr\bin;C:\ProgramData\Anaconda3\Library\bin;C:\ProgramData\Anaconda3\Scripts;C:\ProgramData\Anaconda3\Scripts\pip.exe;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="5733342" LastActivityDate="2018-03-08T12:30:06.960" Title="Pip not found! Please make sure `pip` in the system PATH, and then &gt; restart VSCode" Tags="&lt;pyspark&gt;&lt;pip&gt;&lt;visual-studio-code&gt;&lt;hdinsight&gt;&lt;vscode-settings&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49173182" PostTypeId="1" CreationDate="2018-03-08T12:31:40.850" Score="0" ViewCount="23" Body="&#xA;&#xA;&lt;p&gt;I have a text file with contents&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;&quot;Amount in millions (millions of US$) (November 2017 Version)&quot;,,some info,,,,,,,,&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;with which a &lt;code&gt;DF&lt;/code&gt; created&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;rdd = sc.textFile(&quot;file:///home/cloudera/tst.txt&quot;)&#xA;schema = StructType([StructField(&quot;line&quot;,StringType(),True)])&#xA;df = rdd.toDF(schema)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;which throws&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;raise ValueError(&quot;Unexpected tuple %r with StructType&quot; % obj)&#xA;ValueError: Unexpected tuple u'&quot;Amount in millions (millions of US$) (November 2017 Version)&quot;,,some info,,,,,,,,' with StructType&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;when executing &lt;code&gt;df.show()&lt;/code&gt;.  Any idea how this could be addressed?&lt;/p&gt;&#xA;" OwnerUserId="514438" LastEditorUserId="514438" LastEditDate="2018-03-08T16:12:35.660" LastActivityDate="2018-03-08T16:12:35.660" Title="How to deal with &quot;Unexpected tuple...&quot;" Tags="&lt;dataframe&gt;&lt;pyspark&gt;&lt;spark-dataframe&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49173600" PostTypeId="1" AcceptedAnswerId="49177210" CreationDate="2018-03-08T12:53:08.090" Score="1" ViewCount="31" Body="&lt;p&gt;So, what I'm doing below is I drop a column &lt;code&gt;A&lt;/code&gt; from a &lt;code&gt;DataFrame&lt;/code&gt; because I want to apply a transformation (here I just &lt;code&gt;json.loads&lt;/code&gt; a JSON string) and replace the old column with the transformed one. After the transformation I just join the two resulting data frames. &lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;df = df_data.drop('A').join(&#xA;    df_data[['ID', 'A']].rdd\&#xA;        .map(lambda x: (x.ID, json.loads(x.A)) &#xA;             if x.A is not None else (x.ID, None))\&#xA;        .toDF()\&#xA;        .withColumnRenamed('_1', 'ID')\&#xA;        .withColumnRenamed('_2', 'A'),&#xA;    ['ID']&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The thing I dislike about this is of course the overhead I'm faced because I had to do the &lt;code&gt;withColumnRenamed&lt;/code&gt; operations.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;With pandas All I'd do something like this:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;pdf = pd.DataFrame([json.dumps([0]*np.random.randint(5,10)) for i in range(10)], columns=['A'])&#xA;pdf.A = pdf.A.map(lambda x: json.loads(x))&#xA;pdf&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;but the following does not work in pyspark:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;df.A = df[['A']].rdd.map(lambda x: json.loads(x.A))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So is there an easier way than what I'm doing in my first code snipped?&lt;/p&gt;&#xA;" OwnerUserId="826983" LastActivityDate="2018-03-08T15:57:08.613" Title="Transforming a column and update the DataFrame" Tags="&lt;pyspark&gt;&lt;spark-dataframe&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="49173893" PostTypeId="1" CreationDate="2018-03-08T13:08:57.673" Score="0" ViewCount="25" Body="&lt;p&gt;I am getting the below error in Spark 1.5 :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; Diagnostics: Container [pid=19554,containerID=container_e94_1518800506024_42837_02_000017] is running beyond physical memory limits. Current usage: 3.5 GB of 3.5 GB physical memory used; 4.3 GB of 7.3 GB virtual memory used. Killing container. Dump of the process-tree for container_e94_1518800506024_42837_02_000017&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;MASTER_URL=yarn-cluster&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;NUM_EXECUTORS=10&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;EXECUTOR_MEMORY=4G&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;EXECUTOR_CORES=6&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;DRIVER_MEMORY=3G&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;The data the application reads is 7MB of avro file, but there are multiple writes in spark application.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there any problem with Job configuration ?&lt;/p&gt;&#xA;" OwnerUserId="3240790" LastEditorUserId="6115238" LastEditDate="2018-03-08T13:38:42.457" LastActivityDate="2018-03-08T13:38:42.457" Title="Spark Memory error" Tags="&lt;apache-spark&gt;" AnswerCount="0" CommentCount="6" />
  <row Id="49174202" PostTypeId="1" CreationDate="2018-03-08T13:25:35.837" Score="0" ViewCount="13" Body="&lt;p&gt;I am writing a def that lets users import rows into a case class from JDBC by specifying the case class type only (I will provide the encoders)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The column names are in French and I would like to know if it is possible to write an encoder that converts a dataframe with columns &lt;code&gt;name1 -&amp;gt; T1&lt;/code&gt;, &lt;code&gt;name2 -&amp;gt; T2&lt;/code&gt;... into a Dataset[T] where T is a case &lt;code&gt;class SomeCaseClass(name1': T1, name2': T2...)&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To summarise I may want to rename or omit some columns but types will not change.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I realise I can do something like the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark.sqlContext.read&#xA;  .format(&quot;jdbc&quot;)&#xA;  .option(&quot;driver&quot;, jdbcConfig.driver)&#xA;  .option(&quot;url&quot;, jdbcConfig.url)&#xA;  .option(&quot;dbtable&quot;, table)&#xA;  .option(&quot;user&quot;, jdbcConfig.user)&#xA;  .option(&quot;password&quot;, jdbcConfig.password)&#xA;  .load&#xA;  .toDF(Utils.caseClassConstructorParams[T].keys.toSeq: _*)&#xA;  .as[T]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;however this seems less efficient, I can't drop columns and I cant guarantee columns will be returned in the same order as my case class constructor parameters.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Cheers for any advice.&lt;/p&gt;&#xA;" OwnerUserId="5516172" LastEditorUserId="5516172" LastEditDate="2018-03-08T14:15:12.400" LastActivityDate="2018-03-08T14:15:12.400" Title="Spark custom Encoder - can I drop/rename columns" Tags="&lt;apache-spark&gt;&lt;apache-spark-sql&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49174530" PostTypeId="1" CreationDate="2018-03-08T13:40:46.653" Score="0" ViewCount="37" Body="&lt;p&gt;I have a dataframe, that contain,  2 columns of date &lt;code&gt;start_date&lt;/code&gt; and &lt;code&gt;finish_date&lt;/code&gt;; and I created a new column to add the moyen between the 2 dates.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+-----+--------+-------+---------+-----+--------------------+-------------------&#xA;start_date|                      finish_date|                  moyen_date|&#xA;+-----+--------+-------+---------+-----+--------------------+-------------------&#xA;     2010-11-03 15:56:...      |2010-11-03 17:43:...|                 0|&#xA;    2010-11-03 17:43:...      |2010-11-05 13:21:...|                  2|&#xA;    2010-11-05 13:21:...      |2010-11-05 14:08:...|                  0|&#xA;    2010-11-05 14:08:...      |2010-11-05 14:08:...|                  0|&#xA;+-----+--------+-------+---------+-----+--------------------+-------------------&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I calculated the difference between the 2 dates:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;var result = sqlDF.withColumn(&quot;moyen_date&quot;,datediff(col(&quot;finish_date&quot;), col(&quot;start_date&quot;)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But I want to convert start_date and finish_date to integer, knowing that each column contain &lt;code&gt;date + time&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Someone can help me please. ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you&lt;/p&gt;&#xA;" OwnerUserId="8170477" LastActivityDate="2018-03-09T08:41:09.690" Title="convert date to integer scala spark" Tags="&lt;scala&gt;&lt;date&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="8" />
  <row Id="49174567" PostTypeId="1" CreationDate="2018-03-08T13:42:34.657" Score="0" ViewCount="9" Body="&lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm working on an azure HDI R server cluster with rstudio and sparkR package.&#xA;I'm reading file, modifying it and then i want to write it with write.df, but the problem is that when i write the file, my column names disappear.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My code is the following:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;write.df(spdf,&quot;///Output/File&quot;,&quot;csv&quot;,&quot;overwrite&quot;,header=T)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;here is the file i want to write in csv format&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Num,Letter&#xA;5.5,a&#xA;9.,b&#xA;5.5,c&#xA;9,d&#xA;5.5,e&#xA;9,f&#xA;5.5,g&#xA;9,h&#xA;5.5,i&#xA;9,j&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;here is the file i get:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    0,1&#xA;    5.5,a&#xA;    9.,b&#xA;    5.5,c&#xA;    9,d&#xA;    5.5,e&#xA;    9,f&#xA;    5.5,g&#xA;    9,h&#xA;    5.5,i&#xA;    9,j&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="7855896" LastEditorUserId="7855896" LastEditDate="2018-03-08T14:17:23.750" LastActivityDate="2018-03-08T14:17:23.750" Title="Losing columns names when writing sparkdataframe with sparkR write.df" Tags="&lt;r&gt;&lt;azure&gt;&lt;hadoop&gt;&lt;spark-dataframe&gt;&lt;sparkr&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49175083" PostTypeId="1" CreationDate="2018-03-08T14:07:36.487" Score="0" ViewCount="25" Body="&lt;p&gt;Assume I have a Pyspark dataframe as shown below. Each user bought one item on some specific date.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+--+-------------+-----------+&#xA;|ID|  Item Bought| Date      |&#xA;+--+-------------+-----------+&#xA;|1 |  Laptop     | 01/01/2018|  &#xA;|1 |  Laptop     | 12/01/2017|  &#xA;|1 |  Car        | 01/12/2018|  &#xA;|2 |  Cake       | 02/01/2018|  &#xA;|3 |  TV         | 11/02/2017| &#xA;+--+-------------+-----------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now I would like to create a new data frame as shown below.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+---+--------+-----+------+----+&#xA;|ID | Laptop | Car | Cake | TV |&#xA;+---+--------+-----+------+----+&#xA;|1  | 2      | 1   | 0    | 0  | &#xA;|2  | 0      | 0   | 1    | 0  |&#xA;|3  | 0      | 0   | 0    | 1  |&#xA;+---+--------+-----+------+----+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;There are item columns, each column for one item. For each user, the number on each column is the number of that items user bought.&lt;/p&gt;&#xA;" OwnerUserId="9207907" LastEditorUserId="5858851" LastEditDate="2018-03-08T15:02:48.220" LastActivityDate="2018-03-08T19:07:18.830" Title="How to make a new column per group to indicate the number of items each user has?" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;&lt;pivot&gt;&lt;spark-dataframe&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="49175088" PostTypeId="1" CreationDate="2018-03-08T14:07:54.963" Score="0" ViewCount="21" Body="&lt;p&gt;I run a single spark job on a local cluster(1 master-2workers/executors). &lt;/p&gt;&#xA;&#xA;&lt;p&gt;From what i have understood until now, all stages of a job are splited into tasks. Each stage has its own task set. Each task of this TaskSet will be scheduled on an executor of the local cluster.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to make TaskSetManager of Spark to schedule all tasks of a TaskSet(of a single Stage) on the same (local) executor, but i have not figured up how to do so.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks,&#xA;Jim&lt;/p&gt;&#xA;" OwnerUserId="6905476" LastActivityDate="2018-03-08T14:07:54.963" Title="Is it possible to make Spark run whole Taskset on a single executor?" Tags="&lt;apache-spark&gt;&lt;taskmanager&gt;&lt;apache-spark-standalone&gt;" AnswerCount="0" CommentCount="4" />
  <row Id="49176236" PostTypeId="1" CreationDate="2018-03-08T15:04:01.553" Score="0" ViewCount="14" Body="&lt;p&gt;In a &lt;a href=&quot;https://stackoverflow.com/questions/49151938/how-to-turn-pip-pypi-installed-python-packages-into-zip-files-to-be-used-in-aw/49175969#49175969&quot;&gt;related question&lt;/a&gt; I am trying to get 3rd party packages working on Amazon AWS Glue PySpark (Python 2.7) where &lt;strong&gt;I cannot run &lt;code&gt;pip install&lt;/code&gt;&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The way to get 3rd party packages there is to &lt;code&gt;zip&lt;/code&gt; them up and put them up on Amazon S3 where a PySpark  node can pick them up and load them as a module dependency. I managed to make this work for many basic packages I tested, &lt;a href=&quot;https://stackoverflow.com/questions/49151938/how-to-turn-pip-pypi-installed-python-packages-into-zip-files-to-be-used-in-aw/49175969#49175969&quot;&gt;detailed in this thread&lt;/a&gt;, but for other packages importing fails where for others it works.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As the most important example, I'm trying to get the Google Cloud Python client libs to work, namely the BigQuery library and the authentication client.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here's an example of what happens:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import requests&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Result: (nothing, succeeds)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from google.cloud import bigquery&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Result:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ImportError: No module named google.cloud&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Let's try this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import google&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Result:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ImportError: No module named google&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;After googling around for a while it seems like the problem has to do with the fact that Google Cloud libs use the &lt;a href=&quot;https://www.python.org/dev/peps/pep-0420/&quot; rel=&quot;nofollow noreferrer&quot;&gt;Implicit Namespace Packages (PEP-420)&lt;/a&gt; approach so they don't have the traditional &lt;strong&gt;init&lt;/strong&gt;.py files. Adding them manually breaks dependencies.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are many suggestions for situations that are close to, but not identical to this, including using &lt;code&gt;from __future__ import absolute_import&lt;/code&gt; &lt;a href=&quot;https://stackoverflow.com/questions/33743880/what-does-from-future-import-absolute-import-actually-do&quot;&gt;like so&lt;/a&gt;, but they haven't worked for me. So now I am at a loss what to do. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since, as mentioned above, I don't have access to the command line, I cannot &lt;code&gt;pip install&lt;/code&gt; anything.&lt;/p&gt;&#xA;" OwnerUserId="1525876" LastActivityDate="2018-03-08T15:04:01.553" Title="How to import nested libraries with Implicit Namespaces (PEP-420) without __init__.py files in Python 2.7" Tags="&lt;python&gt;&lt;python-2.7&gt;&lt;amazon-web-services&gt;&lt;pyspark&gt;&lt;google-cloud-python&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49176771" PostTypeId="1" CreationDate="2018-03-08T15:29:59.480" Score="-2" ViewCount="15" Body="&lt;p&gt;Say we have stream of events like this one from meetup &lt;a href=&quot;http://meetup.github.io/stream/rsvpTicker/&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://meetup.github.io/stream/rsvpTicker/&lt;/a&gt;&#xA;or in json style &lt;a href=&quot;http://stream.meetup.com/2/rsvps&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://stream.meetup.com/2/rsvps&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can i connect to this public web sockets and process data?&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    JavaReceiverInputDStream&amp;lt;String&amp;gt; lines = ssc&#xA;            .socketTextStream(&quot;ws://stream.meetup.com/2/rsvps&quot;, 80)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;this works if I open socket server at localhost and write to it but not when try to connect to public websocket.&#xA;I also tried with my implementation of JavaCustomReceiver() giving host and port number but still miss something.&#xA;Do i have to include websocket handshake in http header like&lt;/p&gt;&#xA;&#xA;&lt;p&gt;GET / HTTP/1.1&#xA;Host: localhost:1337&#xA;User-Agent: Mozilla/5.0 [...]&#xA;Upgrade: websocket&#xA;...  &lt;/p&gt;&#xA;" OwnerUserId="6377316" LastActivityDate="2018-03-08T15:29:59.480" Title="Spark streaming connect to websocket" Tags="&lt;java&gt;&lt;apache-spark&gt;&lt;websocket&gt;&lt;streaming&gt;" AnswerCount="0" CommentCount="1" FavoriteCount="0" />
  <row Id="49176896" PostTypeId="1" CreationDate="2018-03-08T15:36:11.960" Score="0" ViewCount="15" Body="&lt;p&gt;a colleague has created some files using pig and have saved them on hdfs.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Folder name XYZ&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    .pig_header&#xA;    .pig_schema&#xA;    _SUCCESS&#xA;    part-r-00000&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I like to read them in pyspark.&#xA;How do i do this ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have already read in succesfully files from hive.&#xA;Furthermore I was able to read files from hdfs which I have created on my own using pyspark&lt;/p&gt;&#xA;&#xA;&lt;p&gt;using &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    df = spark.read.csv('XYZ', sep=&quot;;&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am able to read the pig generated data on hdfs but when I do this it doesnt contain the header because the header seems to be save in .pig_header file&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can someone tell how I can read these files with pyspark into a dataframe such that the columns of the dataframe uses the header specified in .pig_header ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am using Python 3.6&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Kind regards,&#xA;Jonathan&lt;/p&gt;&#xA;" OwnerUserId="9462998" LastActivityDate="2018-03-08T15:36:11.960" Title="How to read files with pyspark stored on hdfs which were created with pig?" Tags="&lt;python&gt;&lt;python-3.x&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;apache-pig&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49177033" PostTypeId="1" CreationDate="2018-03-08T15:42:28.690" Score="0" ViewCount="14" Body="&lt;p&gt;I have a RDD that gives me the Mean from some values group by Key.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My tuple is the following one.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;        # KEY                    MEAN X    MEAN y   MEAN Z&#xA;{(&quot;nexus4&quot;,&quot;nexus_4&quot;,&quot;stand&quot;),(4.21029901,3.210292,0.302193)}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The code for calculating it is the following one.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;rdd_ori = sc.textFile(&quot;asdasd.csv&quot;) \&#xA;    .map(lambda x: ((x.split(&quot;,&quot;)[6], x.split(&quot;,&quot;)[7], x.split(&quot;,&quot;)[9]),(float(x.split(&quot;,&quot;)[3]),float(x.split(&quot;,&quot;)[4]),float(x.split(&quot;,&quot;)[5]))))&#xA;&#xA;meanRDD = rdd_ori.mapValues(lambda x: (x, 1)) \&#xA;    .reduceByKey(lambda a, b: ((a[0][0] + b[0][0], a[0][1] + b[0][1], a[0][2] + b[0][2]), a[1] + b[1]))\&#xA;    .mapValues(lambda a : (a[0][0]/a[1], a[0][1]/a[1],a[0][2]/a[1]))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;So, my meanRDD has the mean from each coordinate.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is the following one, it's possible to calculate the standard deviation from each coordinate using my &quot;meanRDD&quot; to avoid extra calculation?&lt;/p&gt;&#xA;" OwnerUserId="9326486" LastEditorUserId="4685471" LastEditDate="2018-03-08T16:07:50.047" LastActivityDate="2018-03-08T16:07:50.047" Title="Calculate deviation - PySpark" Tags="&lt;pyspark&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49177288" PostTypeId="1" AcceptedAnswerId="49177894" CreationDate="2018-03-08T15:55:24.750" Score="0" ViewCount="10" Body="&lt;p&gt;I am deleting output folders before running my spark job .&#xA;Sometime it deletes it but sometime it delete all the files inside it but top level folder still remains .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have sub folders kind of structure .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is how i am deleting the folders .&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def DeleteDescrFolder(fs: org.apache.hadoop.fs.FileSystem, descrFileURL: String) = {&#xA;    val bDescr = fs.exists(new Path(descrFileURL))&#xA;    if (true.equals(bDescr)) {&#xA;      val outputFile = fs.globStatus(new Path(descrFileURL))&#xA;      for (DeleteFilePath &amp;lt;- outputFile) {&#xA;        fs.delete(DeleteFilePath.getPath)&#xA;      }&#xA;      println(&quot;Descr File is delete from  &quot; + descrFileURL)&#xA;    } else {&#xA;      println(descrFileURL + &quot;Decsr Does not Exist&quot;)&#xA;    }&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How can i remove folder name also ?&lt;/p&gt;&#xA;" OwnerUserId="9175539" LastEditorUserId="9175539" LastEditDate="2018-03-08T16:03:21.613" LastActivityDate="2018-03-08T16:35:12.087" Title="Not able to remove folder name inspite of delting all files inside it in spark" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;hdfs&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49177319" PostTypeId="1" CreationDate="2018-03-08T15:56:46.677" Score="1" ViewCount="26" Body="&lt;p&gt;I've just setup a new hadoop 3.0 cluster with Hive 2.3.2 and Spark 2.3. When I want to run some queries on Hive tables, getting following error.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know there were some bugs in Hive, but seems like it was fixed for 2.1.1, but not sure what's the situation with 2.3.2 version. Do you have any idea if that could be handled somehow?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Using Scala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_151)&#xA;Type in expressions to have them evaluated.&#xA;Type :help for more information.&#xA;&#xA;scala&amp;gt; import spark.sql&#xA;import spark.sql&#xA;&#xA;scala&amp;gt; sql(&quot;show databases&quot;)&#xA;java.lang.NoSuchFieldError: HIVE_STATS_JDBC_TIMEOUT&#xA;  at org.apache.spark.sql.hive.HiveUtils$.formatTimeVarsForHiveClient(HiveUtils.scala:205)&#xA;  at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:286)&#xA;  at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)&#xA;  at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)&#xA;  at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)&#xA;  at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)&#xA;  at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)&#xA;  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)&#xA;  at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)&#xA;  at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)&#xA;  at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)&#xA;  at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)&#xA;  at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)&#xA;  at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)&#xA;  at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.&amp;lt;init&amp;gt;(HiveSessionStateBuilder.scala:69)&#xA;  at org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)&#xA;  at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)&#xA;  at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)&#xA;  at org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)&#xA;  at org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)&#xA;  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)&#xA;  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)&#xA;  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)&#xA;  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)&#xA;  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:638)&#xA;  ... 49 elided&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="8048750" LastActivityDate="2018-03-08T15:56:46.677" Title="HIVE_STATS_JDBC_TIMEOUT for Hive queries in Spark" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;hive&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49177352" PostTypeId="1" AcceptedAnswerId="49177752" CreationDate="2018-03-08T15:58:12.870" Score="0" ViewCount="12" Body="&lt;p&gt;I'm trying to load in a DataFrame a set of vectors that i saved on a file with a textual format, for example:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;2.846110820770263672e+00 -1.368924856185913086e+00 6.769183874130249023e-01&#xA;2.846110820770263672e+00 -1.368924856185913086e+00 6.769183874130249023e-01&#xA;...&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'm using  Spark 2.4.0 (built from sources) and my code is something like:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;sc = SparkSession.builder.appName(appName).master(&quot;local[4]&quot;).getOrCreate()&#xA;data = sc.read.text(PATH_TO_VECTORS)&#xA;parsedData = data.rdd.map(lambda line: np.asarray([float(x) for x in line[0].split()]))&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sometime I obtain a float parse error on some randomic value that is not in the file itself.&#xA;For example, the first time the error could be &quot;ValueError: could not convert string to float: '95e-01\x00\x001\x00\x00\x00\x00\x00\x00\x00...&quot; and the second time it could be &quot;could not convert string to float: '-1.436311483383178711e+00\x005e-01'&quot; etc...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does someone have an idea of what is going on? The weird thing is that each time I run my code I have a different value for which the float conversion is not working OR it works.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank in advance!&lt;/p&gt;&#xA;" OwnerUserId="8065897" LastActivityDate="2018-03-08T16:18:41.977" Title="Pyspark 2.4.0: Rdd map with split on lines from file random errors" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;spark-dataframe&gt;&lt;rdd&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49177432" PostTypeId="1" CreationDate="2018-03-08T16:02:24.720" Score="2" ViewCount="24" Body="&lt;p&gt;Our team is having a lot of issues with the Spark API particularly with large schema tables. We currently have a program written in Scala that utilizes the Apache spark API to create two Hive tables from raw files. We have one particularly very large raw data file that is giving us issues that contains around ~4700 columns and ~200,000 rows. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Every week we get a new file that shows the updates, inserts and deletes that happened in the last week. Our program will create two tables – a master table and a history table. The master table will be the most up to date version of this table while the history table shows all changes inserts and updates that happened to this table and showing what changed. For example, if we have the following schema where A and B are the primary keys: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;      Week 1                      Week 2&#xA;|-----|-----|-----|         |-----|-----|-----|&#xA;|  A  |  B  |  C  |         |  A  |  B  |  C  |&#xA;|-----|-----|-----|         |-----|-----|-----|&#xA;|  1  |  2  |  3  |         |  1  |  2  |  4  |&#xA;|-----|-----|-----|         |-----|-----|-----|&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then the master table will now be&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;|-----|-----|-----|      &#xA;|  A  |  B  |  C  |       &#xA;|-----|-----|-----|       &#xA;|  1  |  2  |  4  |       &#xA;|-----|-----|-----| &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And The history table will be&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;|-----|-----|-------------------|----------------|-------------|-------------|      &#xA;|  A  |  B  |  changed_column   |  change_type   |  old_value  |  new_value  |     &#xA;|-----|-----|-------------------|----------------|-------------|-------------|      &#xA;|  1  |  2  |        C          |    Update      |     3       |     4       |&#xA;|-----|-----|-------------------|----------------|-------------|-------------|&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This process is working flawlessly for shorter schema tables. We have a table that has 300 columns but over 100,000,000 rows and this code still runs as expected. The process above for the larger schema table runs for around 15 hours, and then crashes with the following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Exception in thread &quot;main&quot; java.lang.StackOverflowError&#xA;    at scala.collection.generic.Growable$class.loop$1(Growable.scala:52)&#xA;    at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:57)&#xA;    at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:183)&#xA;    at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:45)&#xA;    at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)&#xA;    at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)&#xA;    at scala.collection.immutable.List.foreach(List.scala:381)&#xA;    at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)&#xA;    at scala.collection.immutable.List.flatMap(List.scala:344)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is a code example that takes around 4 hours to run for this larger table, but runs in 20 seconds for other tables:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;var dataframe_result = dataframe1.join(broadcast(dataframe2), Seq(listOfUniqueIds:_*)).repartition(100).cache()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;We have tried all of the following with no success:&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Using hash broad-cast joins (dataframe2 is smaller, dataframe1 is huge)&lt;/li&gt;&#xA;&lt;li&gt;Repartioining on different numbers, as well as not repartitioning at all&lt;/li&gt;&#xA;&lt;li&gt;Caching the result of the dataframe (we originally did not do this).&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;What is causing this error and how can we fix it? The only difference between this problem table is that it has so many columns. Is there an upper limit to how many columns Spark can handle? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Note: We are running this code on a very large MAPR cluster and we tried giving the code 500GB of RAM and its still failing.&lt;/p&gt;&#xA;" OwnerUserId="9119091" LastActivityDate="2018-03-08T16:02:24.720" Title="What is the column limit for Spark Data Frames?" Tags="&lt;scala&gt;&lt;spark-dataframe&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49177531" PostTypeId="1" CreationDate="2018-03-08T16:07:23.847" Score="1" ViewCount="19" Body="&lt;p&gt;I am working on a project at which we have a billion of images with their metadata on MongoDB.  I want to store this image on HDFS for later image processing. The size of image is between 500K to 4MB, thus, I have the problem of small files with Hadoop. I found 3 main possible solutions for this problem which are HBase, har or sequence files.  What is best suitable solution knowing that I need to use Spark in processing these images rather than Map-Reduce?&lt;/p&gt;&#xA;" OwnerUserId="3041392" LastEditorUserId="3041392" LastEditDate="2018-03-09T13:54:25.953" LastActivityDate="2018-03-09T13:54:25.953" Title="Storing small size big quantities image on hdfs for later processing" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;tensorflow&gt;&lt;hbase&gt;&lt;hdfs&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="49177811" PostTypeId="1" AcceptedAnswerId="49193591" CreationDate="2018-03-08T16:21:43.440" Score="2" ViewCount="30" Body="&lt;p&gt;I am following some code that connects to twitter and then writes out that data to a local text file. Here is my code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;System.setProperty(&quot;twitter4j.oauth.consumerKey&quot;, &quot;Mycode - Not going to put real one in for obvious reasons&quot;)&#xA;System.setProperty(&quot;twitter4j.oauth.consumerSecret&quot;, &quot;Mycode&quot;)&#xA;System.setProperty(&quot;twitter4j.oauth.accessToken&quot;, &quot;Mycode&quot;)&#xA;System.setProperty(&quot;twitter4j.oauth.accessTokenSecret&quot;, &quot;Mycode&quot;)&#xA;&#xA;  val ssc = new StreamingContext(spark.sparkContext, Seconds(5))&#xA;&#xA;  val twitterStream = TwitterUtils.createStream(ssc, None)&#xA;&#xA;  twitterStream.saveAsTextFiles(&quot;streamouts/tweets&quot;, &quot;txt&quot;)&#xA;  ssc.start()&#xA;  Thread.sleep(30000)&#xA;  ssc.stop(false)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now, the code is not complaining about any missing references or anything. I believe I have the correct SBT dependencies.&#xA;The following code seems to run. It creates the folder structure and text files within. However, ALL of the text files are completely blank. 0kb in size.&#xA;What am i doing wrong? Anyone any ideas, as to why it look likes it is creating the output text files, but not actually writing into the files?&#xA;By the way:&#xA;I have triple checked the consumer keys, access tokens etc from the twitter app. I'm certain I have copied them over correctly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Conor&lt;/p&gt;&#xA;" OwnerUserId="2952012" LastActivityDate="2018-03-09T12:13:06.363" Title="Scala Spark and Twitter feed" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;twitter&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="49177976" PostTypeId="1" CreationDate="2018-03-08T16:29:47.863" Score="0" ViewCount="34" Body="&lt;p&gt;I have a problem while using pyspark to compute a new column in my dataframe with a mathematical formula. Basically, I am doing the following computation from two columns composed of dates :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ED_difference=F.udf(lambda x,y: (x-y).days)&#xA;&#xA;df_assoc_copy=df_assoc_copy.withColumn('ED_diff',F.abs(ED_difference('TRAC_ED_new','BW_ED_new').cast(DoubleType())))&#xA;&#xA;df_assoc_copy=df_assoc_copy.withColumn('B_ED',1-1/(1+F.exp(-(2*df_assoc_copy.ED_diff-coeff1)/coeff2)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This code is working well but it returns wrong values for specific rows. Here I used excel to do the same computation and for most of the rows it returns the same value but there are completely wrong matches between them (and even impossible values such as the 2.35 since the function should return numbers between 0 and 1)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Excel   --------- Spark&lt;br&gt;&#xA;0 ---------------          0&lt;br&gt;&#xA;0.999759688 0.999759688&lt;br&gt;&#xA;0.999759688 0.999759688&lt;br&gt;&#xA;0.150161706 0.150161706&lt;br&gt;&#xA;0 ---------------   0&lt;br&gt;&#xA;&lt;strong&gt;0.963267409   2.355137640&lt;/strong&gt;&lt;br&gt;&#xA;&lt;strong&gt;0.791391473   0.999725422&lt;/strong&gt;&lt;br&gt;&#xA;&lt;strong&gt;0.791391473   0.999725422&lt;/strong&gt;&lt;br&gt;&#xA;0.516660497 0.516660497&lt;br&gt;&#xA;8.84192E-05 8.84E-05&lt;br&gt;&#xA;0 ---------------   0&lt;br&gt;&#xA;0.999759688 0.999759688&lt;br&gt;&#xA;0.999759688 0.999759688&lt;br&gt;&#xA;0 ---------------   0&lt;br&gt;&#xA;0.989369423 0.989369423&lt;br&gt;&#xA;0.999532043 0.999532043  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I don't know at all what to do to correct that issue, I tried changing the function but the same rows are making problems. Can you help me ?&lt;/p&gt;&#xA;" OwnerUserId="9335058" LastActivityDate="2018-03-08T16:29:47.863" Title="Pyspark : wrong values on specific rows when using a mathematical function" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;dataframe&gt;&lt;pyspark&gt;" AnswerCount="0" CommentCount="1" FavoriteCount="1" />
  <row Id="49177985" PostTypeId="1" CreationDate="2018-03-08T16:30:24.933" Score="0" ViewCount="12" Body="&lt;p&gt;I have a CustomWholeFileInputFormat that I'm trying to use in &lt;code&gt;sc.newAPIHadoopFile&lt;/code&gt; but I get the following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;inferred type arguments [Nothing,Nothing,packageName.CustomWholeFileInputFormat]&#xA;do not conform to method newAPIHadoopFile's type parameter bounds&#xA;[K,V,F &amp;lt;: org.apache.hadoop.mapreduce.InputFormat[K,V]]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The CustomWholeFileInputFormat looks like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;public class CustomWholeFileInputFormat extends FileInputFormat&amp;lt;NullWritable, BytesWritable&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and I'm using it this way:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val textFile = sc.newAPIHadoopFile(&#xA;  &quot;InputFile&quot;,&#xA;  classOf[packageName.CustomWholeFileInputFormat],&#xA;  classOf[NullWritable],&#xA;  classOf[BytesWritable]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;To me CustomWholeFileInputFormat is a subtype of FileInputFormat which is subtype of InputFormat.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Not sure why I get this error.&lt;/p&gt;&#xA;" OwnerUserId="4755028" LastEditorUserId="9297144" LastEditDate="2018-03-08T18:56:31.150" LastActivityDate="2018-03-08T18:56:31.150" Title="spark scala: Using custom Input format error inferred type arguments" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="0" CommentCount="0" FavoriteCount="1" />
  <row Id="49178255" PostTypeId="1" CreationDate="2018-03-08T16:45:53.097" Score="0" ViewCount="18" Body="&lt;p&gt;I'm working on a spark streaming job which runs on standalone mode. The executors by default append the logs in &lt;code&gt;$SPARK_HOME/work/app_idxxxx/stderr&lt;/code&gt; and &lt;code&gt;stdout&lt;/code&gt; files. Now the problem comes when app runs for a long time say a month or more and it generates a lot of logs inside &lt;code&gt;stderr&lt;/code&gt; file. I would like to rollup the stderr daily for  a week  and archive(delete) that after that. I changed the &lt;code&gt;log4j.properties&lt;/code&gt; with &lt;code&gt;org.apache.log4j.RollingFileAppender&lt;/code&gt; and directed the logs to a file instead of &lt;code&gt;stderr&lt;/code&gt; but the file doesn't respect the rolling and keeps growing. &#xA;Creating a cron job to do that is also not working since spark has a pointer to that specific file and changing the name probably not working. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I could't find any documentations for these specific logs. I really appreciate for any help. &lt;/p&gt;&#xA;" OwnerUserId="1029112" LastEditorUserId="1029112" LastEditDate="2018-03-09T18:33:40.877" LastActivityDate="2018-03-10T04:50:36.150" Title="How can I prune executors' logs in spark streaming" Tags="&lt;apache-spark&gt;&lt;logging&gt;&lt;log4j&gt;&lt;spark-streaming&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49178696" PostTypeId="1" CreationDate="2018-03-08T17:08:24.800" Score="0" ViewCount="4" Body="&lt;p&gt;I am trying to create a sub matrix from a large matrix. I need to convert the RDD[vector] to DenseMatrix form using scala over spark.&lt;/p&gt;&#xA;" OwnerUserId="5527971" LastActivityDate="2018-03-08T17:08:24.800" Title="How to convert RDD[vector] to DenseMatrix in spark (using scala)" Tags="&lt;apache-spark-mllib&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49179005" PostTypeId="1" CreationDate="2018-03-08T17:24:00.740" Score="-1" ViewCount="10" Body="&lt;p&gt;Data structure:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{&quot;Emp&quot;:{&quot;Name&quot;:&quot;John&quot;, &quot;Sal&quot;:&quot;2000&quot;, &quot;Address&quot;:[{&quot;loc&quot;:&quot;Sanjose&quot;,&quot;Zip&quot;:&quot;222&quot;},{&quot;loc&quot;:&quot;dayton&quot;,&quot;Zip&quot;:&quot;333&quot;}]}}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now I want to load the data into a data frame and want to append zip to loc. The loc column name should be same (loc). The transformed data should be like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{&quot;Emp&quot;:{&quot;Name&quot;:&quot;John&quot;, &quot;Sal&quot;:&quot;2000&quot;, &quot;Address&quot;:[{&quot;loc&quot;:&quot;Sanjose222&quot;,&quot;Zip&quot;:&quot;222&quot;},{&quot;loc&quot;:&quot;dayton333&quot;,&quot;Zip&quot;:&quot;333&quot;}]}}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;No RDDs. I need a data frame operation to achieve this, preferably with the &lt;code&gt;withColumn&lt;/code&gt; function. How can I do this?&lt;/p&gt;&#xA;" OwnerUserId="9045617" LastEditorUserId="5880706" LastEditDate="2018-03-09T05:18:47.730" LastActivityDate="2018-03-09T05:18:47.730" Title="Spark SQL data frame" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;apache-spark-sql&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49179383" PostTypeId="1" CreationDate="2018-03-08T17:47:10.230" Score="0" ViewCount="17" Body="&lt;p&gt;I have a list of topics as an argument.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;stream&lt;/strong&gt; is a &lt;em&gt;Kafka DStream object&lt;/em&gt; and is subscribed to &lt;strong&gt;topicsArr&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to filter each DStream based on key and do some transformation on top of individual DStream without breaking the distributed nature of processing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;When I try to do this I encounter &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Object Not Serializable error&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;Below is the snippet :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;topicsArr.map(y =&amp;gt; {&#xA;     stream&#xA;      .filter(x =&amp;gt; x.key() == y)&#xA;      .map(x =&amp;gt; x.value())&#xA;      .foreachRDD(rdd =&amp;gt;&#xA;      if (!rdd.isEmpty()) {&#xA;          println &quot;rdd.count()&quot; //More code goes here&#xA;     })&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="8127563" LastActivityDate="2018-03-08T17:47:10.230" Title="How to filter on a Kafka Dstream in scala" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;apache-kafka&gt;&lt;spark-streaming&gt;" AnswerCount="0" CommentCount="1" FavoriteCount="1" />
  <row Id="49179421" PostTypeId="1" CreationDate="2018-03-08T17:48:53.523" Score="0" ViewCount="14" Body="&lt;p&gt;&lt;br/&gt;&#xA;Technology Versions used: &lt;br/&gt;&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;Hadoop 2.6.0-cdh5.13.1 &lt;br/&gt;&#xA;  SPARK2-2.2.0.cloudera2-1.cdh5.12.0.p0.232957 &lt;br/&gt;&#xA;  Accumulo 1.8&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Hadoop cluster is Kerberized. &lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;I run a Spark application that tries to read data from an Accummulo database within Spark Executor (not Spark Driver).&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;I pass all the required configuration files like jaas, core-site.xml etc., required for keytab based authentication. The login goes successful and I see that in logs. However, it errors out because UserGroupInformation.getCurrentUser().hasKerberosCredentials() is returned to be false.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Code Snippets:&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Below is how login and try to return the Kerberos Token from within Spark Executor (not driver):&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;@Override&#xA;public AuthenticationToken getToken() throws AuthenticationException {&#xA;    UserGroupInformation.loginUserFromKeytab(&quot;principal&quot;, &quot;keyTab&quot;);&#xA;    context.login();&#xA;    context.commit();&#xA;    return  new KerberosToken(); &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Code snippet from &lt;strong&gt;org.apache.accumulo.core.client.security.tokens.KerbererosToken&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;   public KerberosToken() throws IOException {&#xA;     this(UserGroupInformation.getCurrentUser().getUserName());&#xA;   }&#xA;&#xA;   public KerberosToken(String principal) throws IOException {&#xA;     requireNonNull(principal);&#xA;     final UserGroupInformation ugi = UserGroupInformation.getCurrentUser();&#xA;     checkArgument(ugi.hasKerberosCredentials(), &quot;Subject is not logged in via Kerberos&quot;);&#xA;     checkArgument(principal.equals(ugi.getUserName()), &quot;Provided principal does not match currently logged-in user&quot;);&#xA;     this.principal = ugi.getUserName();&#xA; }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;ugi.hasKerberosCredentials() call in the above method returns false. So, it fails by given &quot;Subject is not logged in via Kerberos&quot; exception message.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Below is code snippet from &lt;strong&gt;org.apache.hadoop.security.UserGroupInformation&lt;/strong&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  private UserGroupInformation(Subject subject, boolean isLoginExternal)&#xA;  {&#xA;this.subject = subject;&#xA;this.user = ((User)subject.getPrincipals(User.class).iterator().next());&#xA;this.isKeytab = (!subject.getPrivateCredentials(KeyTab.class).isEmpty());&#xA;this.isKrbTkt = (!subject.getPrivateCredentials(KerberosTicket.class).isEmpty());&#xA;this.isLoginExternal = isLoginExternal;&#xA; }&#xA;&#xA;public boolean hasKerberosCredentials()&#xA;{&#xA;   return (this.isKeytab) || (this.isKrbTkt);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;jaas.conf&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Client {&#xA;  com.sun.security.auth.module.Krb5LoginModule required&#xA;  useKeyTab=true&#xA;  storeKey=true&#xA;  doNotPrompt=true&#xA;  useTicketCache=false&#xA;  keyTab=&quot;xyz.keytab&quot;&#xA;  principal=&quot;xyz@xyz.io&quot;&#xA;  debug = true;&#xA;};&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How I submit the spark application?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Launch python3.4 shell, and configure PYTHONSTARTUP environment variable to point to below script (pyspark-init.py):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;conf = SparkConf().setAppName(&quot;appname&quot;).setMaster(&quot;yarn&quot;).set(&quot;spark.submit.deployMode&quot;, &quot;client&quot;).set(&quot;spark.driver.extraClassPath&quot;, &quot;core-site.xml,hdfs-site.xml,app.properties&quot;).set(&quot;spark.driver.extraJavaOptions&quot;, &quot;-Djava.security.krb5.conf=krb5.conf -Djava.security.auth.login.config=jaas.conf&quot;).set(&quot;spark.jars&quot;, &quot;/opt/cloudera/parcels/SPARK2/lib/spark2/jars/app-spark-0.1.jar&quot;).set(&quot;spark.yarn.dist.files&quot;, &quot;app.properties,core-site.xml,hdfs-site.xml,krb5.conf,jaas.conf,client.conf,xyz.keytab,log4j.properties&quot;).set(&quot;spark.yarn.keytab&quot;, &quot;xyzExec.keytab&quot;).set(&quot;spark.yarn.principal&quot;, &quot;xyz@xyz.io&quot;).set(&quot;spark.executor.extraJavaOptions&quot;, &quot;-Djava.security.krb5.conf=krb5.conf -Djava.security.auth.login.config=jaas.conf&quot;).set(&quot;spark.executor.extraClassPath&quot;, &quot;app.properties,core-site.xml,hdfs-site.xml,krb5.conf,jaas.conf,client.conf,xyz.keytab&quot;)&#xA;&#xA;&#xA;sqlCtx = SQLContext(sc)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;After that, i will issue the below commands:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df = sqlCtx.read.format(&quot;...&quot;).load()&#xA;&#xA;df.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It fails at &lt;strong&gt;&lt;em&gt;df.show()&lt;/em&gt;&lt;/strong&gt; command throwing the below exception:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Caused by: org.apache.accumulo.core.client.AccumuloException: java.lang.IllegalArgumentException: Subject is not logged in via Kerberos&#xA;    at com.google.common.base.Preconditions.checkArgument(Preconditions.java:88)&#xA;    at org.apache.accumulo.core.client.security.tokens.KerberosToken.&amp;lt;init&amp;gt;(KerberosToken.java:56)&#xA;    at org.apache.accumulo.core.client.security.tokens.KerberosToken.&amp;lt;init&amp;gt;(KerberosToken.java:110)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Can you please guide where am I going wrong?&lt;/p&gt;&#xA;" OwnerUserId="8246716" LastEditorUserId="8246716" LastEditDate="2018-03-09T08:56:35.337" LastActivityDate="2018-03-09T08:56:35.337" Title="spark executor fails to communicate with accumulo via keytab-based authentication in kerberized hadoop cluster" Tags="&lt;apache-spark&gt;&lt;kerberos&gt;&lt;executor&gt;&lt;accumulo&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49179535" PostTypeId="1" CreationDate="2018-03-08T17:56:24.297" Score="0" ViewCount="14" Body="&lt;p&gt;I'm trying to reuse the dataframe by persisting it within a function.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def run() {&#xA;  val df1 = getdf1(spark: SparkSession)&#xA;  val df2 = getdf2(spark:SparkSession, df1)&#xA;}&#xA;&#xA;def getdf1(spark: SparkSession): DataFrame {&#xA;  val sqltxt = &quot;select * from emp&quot;&#xA;  val df1 = spark.sql(sqltxt)&#xA;  df1.persist&#xA;  spark.sql(&quot;SET spark.sql.hive.convertMetastoreParquet=false&quot;)&#xA;  df1.write.mode(SaveMode.Overwrite).parquet(&quot;/user/user1/emp&quot;)&#xA;  df1&#xA;}&#xA;&#xA;def getdf2(spark: SparkSession, df1: DataFrame): DataFrame {&#xA;   // perform some operations&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But, when getdf2 is executing it is performing all operations again. Not sure, if I'm doing anything wrong here. Please help me understand above scenario. Thanks.&lt;/p&gt;&#xA;" OwnerUserId="5944508" LastActivityDate="2018-03-08T18:27:21.003" Title="Spark: Persisting a dataframe within a function" Tags="&lt;apache-spark&gt;&lt;dataframe&gt;&lt;persist&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49179848" PostTypeId="1" CreationDate="2018-03-08T18:15:03.287" Score="-1" ViewCount="15" Body="&lt;p&gt;I have a question about what data structure in Spark is better for my use case?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have CSV file, that has millions lines of records (Around 2GB) and 15 columns. I need to calculate different metrics in this file, such as median, mean, average and some other statistic.&#xA;I’m new to Spark, and not sure what Spark Data Structure I need to hold all this records?&#xA;Should I create class that will represent one line of the data - MyClass?&#xA;And then read the data into JavaRDD?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;OR it’s better to use DataFrame? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How to create DataFrame from .csv file and how to create JavaRDD from .csv file?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;UPDATE: if it's matter, .csv file does'nt have schema. Schema lives into separate csv file&lt;/p&gt;&#xA;" OwnerUserId="8541082" LastEditorUserId="8541082" LastEditDate="2018-03-08T18:29:18.723" LastActivityDate="2018-03-08T19:14:53.717" Title="Java Spark data structure to read records from .csv and perform data analysis" Tags="&lt;java&gt;&lt;apache-spark&gt;&lt;rdd&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49179884" PostTypeId="1" CreationDate="2018-03-08T18:17:15.073" Score="0" ViewCount="24" Body="&#xA;&#xA;&lt;p&gt;In AWS Glue, I need to convert a float value (celsius to fahrenheit) and am using an UDF.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Following is my UDF:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;toFahrenheit = udf(lambda x: '-1' if x in not_found else x * 9 / 5 + 32, StringType())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am using the UDF as follows, in the spark dataframe:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;weather_df.withColumn(&quot;new_tmax&quot;, toFahrenheit(weather_df[&quot;tmax&quot;])).drop(&quot;tmax&quot;).withColumnRenamed(&quot;new_tmax&quot;,&quot;tmax&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I run the code, am getting the error message as :&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;IllegalArgumentException: u&quot;requirement failed: The number of columns doesn't match.\nOld column names (11): station, name, latitude, longitude, elevation, date, awnd, prcp, snow, tmin, tmax\nNew column names (0): &quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Not sure how to invoke the UDF, as am new to python / pyspark, and my new column schema is not created, and empty.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The code snipped used for above sample is :&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;%pyspark&#xA;import sys&#xA;from pyspark.context import SparkContext&#xA;from awsglue.context import GlueContext&#xA;from awsglue.context import DynamicFrame&#xA;from awsglue.transforms import *&#xA;from awsglue.utils import getResolvedOptions&#xA;from awsglue.job import Job&#xA;from pyspark.sql import SparkSession&#xA;from pyspark.sql.functions import udf&#xA;from pyspark.sql.types import StringType&#xA;&#xA;glueContext = GlueContext(SparkContext.getOrCreate())&#xA;&#xA;weather_raw = glueContext.create_dynamic_frame.from_catalog(database = &quot;ohare-airport-2006&quot;, table_name = &quot;ohare_intl_airport_2006_08_climate_csv&quot;)&#xA;print &quot;cpnt : &quot;, weather_raw.count()&#xA;weather_raw.printSchema()&#xA;weather_raw.toDF().show(10)&#xA;&#xA;#UDF to convert the air temperature from celsius to fahrenheit (For sample transformation)&#xA;#toFahrenheit = udf((lambda c: c[1:], c * 9 / 5 + 32)&#xA;toFahrenheit = udf(lambda x: '-1' if x in not_found_cat else x * 9 / 5 + 32, StringType())&#xA;&#xA;#Apply the UDF to maximum and minimum air temperature&#xA;wthdf = weather_df.withColumn(&quot;new_tmin&quot;, toFahrenheit(weather_df[&quot;tmin&quot;])).withColumn(&quot;new_tmax&quot;, toFahrenheit(weather_df[&quot;tmax&quot;])).drop(&quot;tmax&quot;).drop(&quot;tmin&quot;).withColumnRenamed(&quot;new_tmax&quot;,&quot;tmax&quot;).withColumnRenamed(&quot;new_tmin&quot;,&quot;tmin&quot;)&#xA;&#xA;wthdf.toDF().show(5)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The schema for&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt; weather_df:&#xA;root&#xA;|-- station: string&#xA;|-- name: string&#xA;|-- latitude: double&#xA;|-- longitude: double&#xA;|-- elevation: double&#xA;|-- date: string&#xA;|-- awnd: double&#xA;|-- fmtm: string&#xA;|-- pgtm: string&#xA;|-- prcp: double&#xA;|-- snow: double&#xA;|-- snwd: long&#xA;|-- tavg: string&#xA;|-- tmax: long&#xA;|-- tmin: long&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Error trace:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;/tmp/zeppelin_pyspark-3684249459612979499.py&quot;, line 349, in &amp;lt;module&amp;gt;&#xA;    raise Exception(traceback.format_exc())&#xA;Exception: Traceback (most recent call last):&#xA;  File &quot;/tmp/zeppelin_pyspark-3684249459612979499.py&quot;, line 342, in &amp;lt;module&amp;gt;&#xA;    exec(code)&#xA;  File &quot;&amp;lt;stdin&amp;gt;&quot;, line 3, in &amp;lt;module&amp;gt;&#xA;  File &quot;/usr/lib/spark/python/pyspark/sql/dataframe.py&quot;, line 1558, in toDF&#xA;    jdf = self._jdf.toDF(self._jseq(cols))&#xA;  File &quot;/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py&quot;, line 1133, in __call__&#xA;    answer, self.gateway_client, self.target_id, self.name)&#xA;  File &quot;/usr/lib/spark/python/pyspark/sql/utils.py&quot;, line 79, in deco&#xA;    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)&#xA;IllegalArgumentException: u&quot;requirement failed: The number of columns doesn't match.\nOld column names (11): station, name, latitude, longitude, elevation, date, awnd, prcp, snow, tmin, tmax\nNew column names (0): &quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;" OwnerUserId="4470126" LastEditorUserId="4470126" LastEditDate="2018-03-09T03:23:05.470" LastActivityDate="2018-03-09T03:23:05.470" Title="AWS Glue pyspark UDF" Tags="&lt;pyspark&gt;&lt;aws-glue&gt;" AnswerCount="0" CommentCount="5" />
  <row Id="49180047" PostTypeId="1" CreationDate="2018-03-08T18:28:11.707" Score="0" ViewCount="24" Body="&#xA;&#xA;&lt;p&gt;I have to create around 800 dummy columns in a dataframe which have Null values in it.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't want to use &lt;code&gt;df.withColumn('x', lit(None))&lt;/code&gt; for individual columns as there are many columns.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried &lt;code&gt;map(lambda x: df.withColumn(x, lit(None)), column_list)&lt;/code&gt; but it's not working. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Writing the snippet below also looks like a bad approach.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;for column in columns:&#xA;    df = df.withColumn(column, lit(None))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Can someone suggest what is the best optimum way.&lt;/p&gt;&#xA;" OwnerUserId="1140759" LastEditorUserId="5858851" LastEditDate="2018-03-08T19:04:28.827" LastActivityDate="2018-03-08T19:28:40.040" Title="How can we create many new columns in a dataframe in pyspark using withcolumn" Tags="&lt;python&gt;&lt;pyspark&gt;&lt;spark-dataframe&gt;&lt;pyspark-sql&gt;" AnswerCount="1" CommentCount="6" />
  <row Id="49180129" PostTypeId="1" AcceptedAnswerId="49180706" CreationDate="2018-03-08T18:33:46.197" Score="0" ViewCount="15" Body="&lt;p&gt;I have a directory structured like so:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;temp/Tweets/userId123/Tweets.csv&#xA;temp/Tweets/userId456/Tweets.csv&#xA;temp/Tweets/userId789/Tweets.csv&#xA;&#xA;temp/Mentions/userId123/Mentions.csv&#xA;temp/Mentions/userId456/Mentions.csv&#xA;temp/Mentions/userId789/Mentions.csv&#xA;&#xA;.&#xA;.&#xA;.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The data is structured by the type of data entity, I want to restructure it by the user, like so:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;final/userId123/Tweets.csv&#xA;final/userId123/Mentions.csv&#xA;.&#xA;.&#xA;&#xA;final/userId456/Tweets.csv&#xA;final/userId456/Mentions.csv&#xA;.&#xA;.&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I've been looking around on google/StackOverflow/Spark docs, but haven't seen a way to do this, but I think there should be a way to modify the directory structure. How can I do this?&lt;/p&gt;&#xA;" OwnerUserId="1375688" LastActivityDate="2018-03-08T19:49:36.983" Title="Spark - how to restructure directories in HDFS" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;hdfs&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49180145" PostTypeId="1" CreationDate="2018-03-08T18:34:45.427" Score="0" ViewCount="24" Body="&lt;p&gt;I am getting the exception below while trying to connect &lt;strong&gt;HBase&lt;/strong&gt; from &lt;strong&gt;spark2&lt;/strong&gt; and insert some data using the &lt;strong&gt;HBaseConfiguration&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;18/03/05 15:18:47 ERROR yarn.ApplicationMaster: User class threw exception: java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/HBaseConfiguration&#xA;java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/HBaseConfiguration&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I read on &lt;strong&gt;Cloudera&lt;/strong&gt; that &lt;strong&gt;Spark2&lt;/strong&gt; On &lt;strong&gt;HBase&lt;/strong&gt; is not Supported as Spark On HBase is a CDH component that has a dependency on Spark 1.6. Is there any workaround to insert data into &lt;strong&gt;HBase&lt;/strong&gt; from &lt;strong&gt;Spark2&lt;/strong&gt;?&lt;/p&gt;&#xA;" OwnerUserId="2111009" LastActivityDate="2018-03-08T18:34:45.427" Title="Spark2 On HBase Cloudera" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;hbase&gt;&lt;cloudera&gt;&lt;apache-spark-2.0&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="49180236" PostTypeId="1" CreationDate="2018-03-08T18:40:10.540" Score="0" ViewCount="11" Body="&lt;p&gt;I would like to write out Scala RDD to MapR table. Online search gave this website: &lt;a href=&quot;http://www.openkb.info/2015/01/how-to-use-scala-on-spark-to-load-data.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;http://www.openkb.info/2015/01/how-to-use-scala-on-spark-to-load-data.html&lt;/a&gt;. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How do I look for Maven repos that will bring in the following dependencies? First time using Maven/IntelliJ, so any pointers will be greatly appreciated.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import org.apache.hadoop.hbase.{HBaseConfiguration, HTableDescriptor}&#xA;import org.apache.hadoop.hbase.client.HBaseAdmin&#xA;import org.apache.hadoop.hbase.mapreduce.TableInputFormat&#xA;import org.apache.hadoop.hbase.HColumnDescriptor&#xA;import org.apache.hadoop.hbase.util.Bytes&#xA;import org.apache.hadoop.hbase.client.Put;&#xA;import org.apache.hadoop.hbase.client.HTable; &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="1124702" LastActivityDate="2018-03-08T18:40:10.540" Title="Writing Spark RDD to MapR table in Spark 2.0.1" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;hbase&gt;&lt;mapr&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49180536" PostTypeId="1" CreationDate="2018-03-08T18:58:12.890" Score="0" ViewCount="12" Body="&lt;p&gt;I have two dataframes (deleting the fields that are not relevant to the question):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df1: org.apache.spark.sql.DataFrame = [rawValue: bigint]&#xA;df2: org.apache.spark.sql.DataFrame = [startLong: bigint, endLong: bigint]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I now want to join the two dataframes where rawValue(df1) &gt;= startLong(df2) AND &amp;lt;= endLong(df2)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Can anyone recommend an efficient way of doing this? The one option I was thinking of was to flatmap df2 and then do a straight join, but I don't want to do that if there is an efficient way to do the above join&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Appreciate any help!!&lt;/p&gt;&#xA;" OwnerUserId="5420381" LastEditorUserId="5420381" LastEditDate="2018-03-08T19:10:01.877" LastActivityDate="2018-03-09T04:02:55.430" Title="Spark SCALA - Joining two dataframes where join value in one dataframe is between two fields in the second dataframe" Tags="&lt;apache-spark&gt;&lt;spark-dataframe&gt;&lt;spark-streaming&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49180669" PostTypeId="1" CreationDate="2018-03-08T19:06:56.340" Score="0" ViewCount="15" Body="&lt;p&gt;What's the typical approach for getting JSON from REST API using databricks?&#xA;It returns nested structure, which can change over time and doesn't have any schema:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{ &quot;page&quot;: &quot;1&quot;,&#xA;  &quot;total&quot;: &quot;10&quot;,&#xA;  &quot;payload&quot;: [&#xA;     { &quot;param1&quot;: &quot;value1&quot;,&#xA;       &quot;param2&quot;: &quot;value2&quot;&#xA;     },&#xA;     { &quot;param2&quot;: &quot;value2&quot;,&#xA;       &quot;param3&quot;: &quot;value3&quot;&#xA;     }&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm trying to put it into dataframe.&lt;/p&gt;&#xA;" OwnerUserId="2961270" LastActivityDate="2018-03-08T19:06:56.340" Title="Databricks get JSON without schema" Tags="&lt;apache-spark&gt;&lt;databricks&gt;" AnswerCount="0" CommentCount="5" />
  <row Id="49180879" PostTypeId="1" CreationDate="2018-03-08T19:19:53.237" Score="-1" ViewCount="10" Body="&lt;p&gt;I want to write an udf to process more than hundred columns from a rdd or dataframe . I don't have the column names.&#xA;How to write it in Scala.&lt;/p&gt;&#xA;" OwnerUserId="1326784" LastActivityDate="2018-03-08T19:19:53.237" Title="How to write a udf passing hundreds of columns from a file using spark scala 1.6 version" Tags="&lt;scala&gt;&lt;hadoop&gt;&lt;apache-spark&gt;&lt;rdd&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="49180931" PostTypeId="1" CreationDate="2018-03-08T19:22:54.817" Score="0" ViewCount="11" Body="&lt;p&gt;I'm trying to open a Kafka (tried versions 0.11.0.2 and 1.0.1) stream using &lt;code&gt;createDirectStream&lt;/code&gt; method and getting this AbstractMethodError error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Exception in thread &quot;main&quot; java.lang.AbstractMethodError&#xA;    at org.apache.spark.internal.Logging$class.initializeLogIfNecessary(Logging.scala:99)&#xA;    at org.apache.spark.streaming.kafka010.KafkaUtils$.initializeLogIfNecessary(KafkaUtils.scala:39)&#xA;    at org.apache.spark.internal.Logging$class.log(Logging.scala:46)&#xA;    at org.apache.spark.streaming.kafka010.KafkaUtils$.log(KafkaUtils.scala:39)&#xA;    at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66)&#xA;    at org.apache.spark.streaming.kafka010.KafkaUtils$.logWarning(KafkaUtils.scala:39)&#xA;    at org.apache.spark.streaming.kafka010.KafkaUtils$.fixKafkaParams(KafkaUtils.scala:201)&#xA;    at org.apache.spark.streaming.kafka010.DirectKafkaInputDStream.&amp;lt;init&amp;gt;(DirectKafkaInputDStream.scala:63)&#xA;    at org.apache.spark.streaming.kafka010.KafkaUtils$.createDirectStream(KafkaUtils.scala:147)&#xA;    at org.apache.spark.streaming.kafka010.KafkaUtils$.createDirectStream(KafkaUtils.scala:124)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is how I'm calling it:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val preferredHosts = LocationStrategies.PreferConsistent&#xA;    val kafkaParams = Map(&#xA;      &quot;bootstrap.servers&quot; -&amp;gt; &quot;localhost:9092&quot;,&#xA;      &quot;key.deserializer&quot; -&amp;gt; classOf[IntegerDeserializer],&#xA;      &quot;value.deserializer&quot; -&amp;gt; classOf[StringDeserializer],&#xA;      &quot;group.id&quot; -&amp;gt; groupId,&#xA;      &quot;auto.offset.reset&quot; -&amp;gt; &quot;earliest&quot;&#xA;    )&#xA;&#xA;    val aCreatedStream = createDirectStream[String, String](ssc, preferredHosts,&#xA;      ConsumerStrategies.Subscribe[String, String](topics, kafkaParams))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have Kafka running on 9092 and I'm able to create producers and consumers and pass messages between them so not sure why it's not working from Scala code. Any ideas appreciated.&lt;/p&gt;&#xA;" OwnerUserId="3004041" LastEditorUserId="4953079" LastEditDate="2018-03-08T22:52:44.257" LastActivityDate="2018-03-08T22:52:44.257" Title="AbstractMethodError creating Kafka stream" Tags="&lt;scala&gt;&lt;apache-kafka&gt;&lt;spark-streaming&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49180993" PostTypeId="1" CreationDate="2018-03-08T19:26:53.827" Score="0" ViewCount="8" Body="&lt;p&gt;I am running a Spark Job using spark-submit.&#xA;In this i am Iterating 3-4 queries which will run sequentially.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While the Entire jobs gets Completed Successfully WHY it still shows Active Jobs and Pending Tasks. As we can see in Job ID : 0 , None of the stages are complete. Also under Tasks. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;What can be the reason for this ?? Why this happens&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/kfHX2.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/kfHX2.jpg&quot; alt=&quot;Incomplete Job Progress&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="4265823" LastActivityDate="2018-03-08T19:26:53.827" Title="Spark Job Completes but shows Incomplete Tasks : Spark" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;spark-dataframe&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49181139" PostTypeId="1" CreationDate="2018-03-08T19:36:25.660" Score="-2" ViewCount="19" Body="&lt;p&gt;I am using spark scala 1.6 version.&#xA;I have 2 files, one is a schema file which has hundreds of columns separated by commas and another file is .gz file which contains data.&#xA;I am trying to read the data using the schema file and apply different transformation logic on a set of few columns .&#xA;I tried running a sample code but I have hardcoded the columns numbers in the attached pic.&#xA;Also I want to write a udf which could read any set of columns and apply the transformation like replacing a special character and give the output.&#xA;Appreciate any suggestion&lt;/p&gt;&#xA;&#xA;&lt;p&gt;import org.apache.spark.SparkContext&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  val rdd1 = sc.textFile(&quot;../inp2.txt&quot;)&#xA;&#xA;  val rdd2 = rdd1.map(line =&amp;gt; line.split(&quot;\t&quot;))&#xA;  val rdd2 = rdd1.map(line =&amp;gt; line.split(&quot;\t&quot;)(1)).toDF&#xA;&#xA;  val replaceUDF = udf{s: String =&amp;gt; s.replace(&quot;.&quot;, &quot;&quot;)}&#xA;&#xA;  rdd2.withColumn(&quot;replace&quot;, replaceUDF('_1)).show&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="1326784" LastEditorUserId="6551426" LastEditDate="2018-03-09T13:27:03.077" LastActivityDate="2018-03-09T13:30:30.490" Title="Spark scala- How to apply transformation logic on a generic set of columns defined in a file" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;rdd&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49181175" PostTypeId="1" CreationDate="2018-03-08T19:38:33.893" Score="0" ViewCount="25" Body="&lt;p&gt;I have set of Scala where i'm trying to load data into hive table using spark while connect hive table I'm experiencing below error   &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Code&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val spark = SparkSession.builder().master(&quot;local[2]&quot;).appName(&quot;interfacing spark sql to hive metastore without configuration file&quot;)&#xA;      .config(&quot;hive.metastore.warehouse.dir&quot;, &quot;C:\\xxxx\\xxxx\\xxxx\\&quot;)&#xA;      .enableHiveSupport() // don't forget to enable hive support&#xA;      .getOrCreate()&#xA;&#xA;    val sc = spark.sparkContext&#xA;    val sqlContext = spark.sqlContext&#xA;    val driverName = &quot;org.apache.hive.jdbc.HiveDriver&quot;&#xA;&#xA;    System.setProperty(&quot;javax.net.ssl.trustStore&quot;, &quot;C:\\xxxx\\xxxx\\xxxx\\xxxx\\xxxx\\security\\jssecacerts&quot;)&#xA;    System.setProperty(&quot;java.security.krb5.debug&quot;,&quot;true&quot;)&#xA;    System.setProperty(&quot;java.security.krb5.conf&quot;,new File(&quot;C:\\xxxx\\xxxx\\krb5.conf&quot;).getAbsolutePath)&#xA;    System.setProperty(&quot;javax.security.auth.useSubjectCredsOnly&quot;, &quot;false&quot;)&#xA;    System.setProperty(&quot;java.security.auth.login.config&quot;, new File(&quot;C:\\xxxx\\xxxx\\jaas.conf&quot;).getAbsolutePath)&#xA;    val hiveurl=&quot;jdbc:hive2://xxxxx.octorp.com:10000/devl_dkp;user=pcpdosr;password=Kopdevp1;ssl=true;AuthMech=3&quot;&#xA;    //;mapred.job.queue.name=dkl&quot;&#xA;    val connectionProperties = new java.util.Properties()&#xA;&#xA;    sc.setLocalProperty(&quot;spark.scheduler.pool&quot;, &quot;dkl&quot;)&#xA;    val hiveQuery = &quot;select * from devl_dkp.employee&quot;&#xA;&#xA;    val hiveResult = spark.read.option(&quot;driver&quot;,driverName).jdbc(hiveurl, hiveQuery, connectionProperties).collect()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;exception caught: org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: ParseException line 1:14 cannot recognize input near 'select' '*' 'from' in join source&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any help will be appreciated thanks &lt;/p&gt;&#xA;" OwnerUserId="9318576" LastActivityDate="2018-03-08T19:38:33.893" Title="Error while compiling statement: FAILED: ParseException line 1:14 cannot recognize input near 'select' '*' 'from' in join source" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;jdbc&gt;&lt;hive&gt;&lt;apache-spark-sql&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="49181179" PostTypeId="1" CreationDate="2018-03-08T19:38:47.470" Score="0" ViewCount="18" Body="&lt;p&gt;I'm using the &lt;a href=&quot;https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.classification.GBTClassifier&quot; rel=&quot;nofollow noreferrer&quot;&gt;Spark ML GBTClassifier&lt;/a&gt; in &lt;code&gt;pyspark&lt;/code&gt; to train a binary classification model on a dataframe with ~400k rows and ~9k columns on an AWS EMR cluster. I'm comparing this against my current solution, which is running XGBoost on a huge EC2 that can fit the whole dataframe in memory.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My hope was that I could train (and score new observations) much faster in Spark because it would be distributed/parallel. However, when watch my cluster (through ganglia) I see that only 3-4 nodes have active CPU while the rest of the nodes are just sitting there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I can't seem to find anything in the documentation about a node limit or partitions or anything that seems relevant to why this is happening. Maybe I'm just misunderstanding the implementation of the algorithm, but I assumed that it was implemented such a way that training could be parallelized to take advantage of the EMR/cluster aspect of Spark. If not, is there any advantage to doing it this way vs. just doing it in memory on a single EC2? I guess you don't have to load the data into memory, but that's not really much of an advantage.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is some boilerplate of my code. Thanks for any ideas!&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import pyspark&#xA;from pyspark.sql import SparkSession&#xA;from pyspark.sql.functions import udf&#xA;from pyspark.sql.types import DoubleType&#xA;from pyspark.ml.classification import GBTClassifier&#xA;from pyspark.ml.evaluation import BinaryClassificationEvaluator&#xA;&#xA;# Start Spark context:&#xA;sc = pyspark.SparkContext()&#xA;sqlContext = SparkSession.builder.enableHiveSupport().getOrCreate()&#xA;&#xA;# load data&#xA;df = sqlContext.sql('SELECT label, features FROM full_table WHERE train = 1')&#xA;&#xA;test_df = sqlContext.sql('SELECT label, features FROM full_table WHERE train = 0')&#xA;&#xA;#Create evaluator&#xA;evaluator = BinaryClassificationEvaluator()&#xA;evaluator.setRawPredictionCol('prob')&#xA;evaluator.setLabelCol('label')&#xA;&#xA;# train model&#xA;gbt = GBTClassifier(maxIter=500, &#xA;                    maxDepth=3, &#xA;                    stepSize=0.1,&#xA;                    labelCol=&quot;label&quot;, &#xA;                    seed=42)&#xA;&#xA;model = gbt.fit(df)&#xA;&#xA;&#xA;# get predictions&#xA;gbt_preds = model.transform(test_df)&#xA;gbt_preds.show(10)&#xA;&#xA;&#xA;# evaluate predictions&#xA;getprob=udf(lambda v:float(v[1]),DoubleType())&#xA;preds = gbt_preds.withColumn('prob', getprob('probability'))\&#xA;        .drop('features', 'rawPrediction', 'probability', 'prediction')&#xA;preds.show(10)&#xA;&#xA;auc = evaluator.evaluate(preds)&#xA;auc&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Sidenote: the tables I am using are already vectorized. The model runs with this code, it just runs slow (~10-15 mins to train) and only uses 3-4 cores.&lt;/p&gt;&#xA;" OwnerUserId="3743213" LastActivityDate="2018-03-08T19:38:47.470" Title="Spark ML gradient boosted trees not using all nodes" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;apache-spark-ml&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="49181403" PostTypeId="1" CreationDate="2018-03-08T19:53:24.150" Score="0" ViewCount="11" Body="&lt;p&gt;Given an unbound stream of words coming in let's say you would like a word count only over the last day and alert if it goes over a certain threshold. I have modified an example to accept a kakfa stream and save word count over the entire job run but I'm not sure how I would reset it daily.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;from __future__ import print_function&#xA;&#xA;import sys&#xA;&#xA;from pyspark import SparkContext&#xA;from pyspark.streaming import StreamingContext&#xA;from pyspark.streaming.kafka import KafkaUtils&#xA;&#xA;if __name__ == &quot;__main__&quot;:&#xA;    if len(sys.argv) != 3:&#xA;        print(&quot;Usage: kafka_wordcount.py &amp;lt;zk&amp;gt; &amp;lt;topic&amp;gt;&quot;, file=sys.stderr)&#xA;        exit(-1)&#xA;&#xA;    sc = SparkContext(appName=&quot;PythonStreamingKafkaWordCount&quot;)&#xA;    sc.setLogLevel(&quot;ERROR&quot;)&#xA;    ssc = StreamingContext(sc, 5)&#xA;    ssc.checkpoint(&quot;checkpoint&quot;)&#xA;&#xA;    # RDD with initial state (key, value) pairs&#xA;    initialStateRDD = sc.parallelize([(u'hello', 1), (u'world', 1)])&#xA;&#xA;    def updateFunc(new_values, last_sum):&#xA;        return sum(new_values) + (last_sum or 0)&#xA;&#xA;    zkQuorum, topic = sys.argv[1:]&#xA;    kvs = KafkaUtils.createStream(ssc, zkQuorum, &quot;spark-streaming-consumer&quot;, {topic: 1})&#xA;    lines = kvs.map(lambda x: x[1])&#xA;&#xA;    running_counts = lines.flatMap(lambda line: line.split(&quot; &quot;)) \&#xA;                          .map(lambda word: (word, 1)) \&#xA;                          .updateStateByKey(updateFunc, initialRDD=initialStateRDD)&#xA;&#xA;    # print the results to stdout&#xA;    running_counts.pprint()&#xA;    ssc.start()&#xA;    ssc.awaitTermination()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2588946" LastActivityDate="2018-03-08T19:53:24.150" Title="Resetting RDD state over a certain window in a spark streaming job" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;&lt;spark-streaming&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49181614" PostTypeId="1" CreationDate="2018-03-08T20:07:40.600" Score="0" ViewCount="16" Body="&lt;p&gt;I'm writing a UDAF window function to calculate median on rows with timestamps occurring before the current row's timestamp. One of the problems I'm facing is what to do when a row is a first of its kind and has no rows that occurred before it. Currently the way my code is written leads to the following errors in such cases: &lt;code&gt;java.util.NoSuchElementException: next on empty iterator&lt;/code&gt; and &lt;code&gt;java.util.NoSuchElementException: None.get&lt;/code&gt;. The full stack trace his huge, but those seem to be the useful parts. Anyways, in SQL I would expect to get a NULL result in this case, and I don't want to get rid of these rows because knowing that there are no preceding values is important information. How can I update my code so that these errors go away? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Given a query like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sql(raw&quot;&quot;&quot;&#xA; SELECT DISTINCT,&#xA;        table_1.date,&#xA;        table_2.a,&#xA;        table_2.b,&#xA;        COUNT (*) OVER lifetime AS count,&#xA;        MEDIAN(table_2.c) OVER lifetime AS median&#xA;   FROM table_1&#xA;   LEFT JOIN table_2&#xA;        ON table_1.date &amp;gt;= table_2.date&#xA; WINDOW lifetime AS (PARTITION BY table_1.date, a, b&#xA;        ORDER BY CAST(CAST(table_2.date AS timestamp) AS long) ASC&#xA;        RANGE BETWEEN UNBOUNDED PRECEDING AND 1 * $minutes PRECEDING)&#xA;&quot;&quot;&quot;).show(4)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'd like results like the below:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+----------+---+-----+-----+------+                                             &#xA;|      date|  a|    b|count|median|&#xA;+----------+---+-----+-----+------+&#xA;|2013-01-13| 61| 9843|   66|  null|&#xA;|2013-01-13| 61|15452|   31|  null|&#xA;|2013-01-13| 80|23141|    0|  null|&#xA;|2013-01-13| 80|46370|    0|  null|&#xA;+----------+---+-----+-----+------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Below is my UDAF code: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;:paste&#xA;import org.apache.spark.sql.Row&#xA;import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}&#xA;import org.apache.spark.sql.types.{ArrayType, DataType, StructType, DoubleType, StructField}&#xA;import scala.collection._&#xA;&#xA;&#xA;class Median() extends UserDefinedAggregateFunction {&#xA;&#xA;  // Input Data Type Schema&#xA;  // Not clear why the StructField here has to be named at all&#xA;  def inputSchema: StructType = StructType(Array(StructField(&quot;x&quot;, DoubleType)))&#xA;&#xA;  // Intermediate Schema&#xA;  def bufferSchema = StructType(Array(&#xA;    StructField(&quot;y&quot;, ArrayType(DoubleType))&#xA;  ))&#xA;&#xA;  // Returned Data Type&#xA;  def dataType: DataType = StructType(Array(StructField(&quot;z&quot;, DoubleType, nullable=true)))&#xA;&#xA;  // Means that the function should always return the same result given same inputs&#xA;  def deterministic = true&#xA;&#xA;  // This function is called whenever key changes&#xA;  def initialize(buffer: MutableAggregationBuffer) = {&#xA;    // Should I use WrappedArray instead of ArrayBuffer? Not sure the difference but this seems to work&#xA;    buffer(0) = mutable.ArrayBuffer.empty[Double]&#xA;  }&#xA;&#xA;  // Iterate over each entry of a group&#xA;  def update(buffer: MutableAggregationBuffer, input: Row) = {&#xA;    buffer(0) = buffer.getAs[mutable.WrappedArray[(Double)]](0) :+ input.getDouble(0)&#xA;  }&#xA;&#xA;  // Merge two partial aggregates&#xA;  def merge(buffer1: MutableAggregationBuffer, buffer2: Row) = {&#xA;    // ++ is Scala's unintuitive way of saying &quot;merge two arrays&quot;&#xA;    buffer1(0) = buffer1.getAs[mutable.WrappedArray[(Double)]](0) ++ buffer2.getAs[mutable.WrappedArray[(Double)]](0)&#xA;  }&#xA;&#xA;  // Called after all the entries are exhausted.&#xA;  def evaluate(buffer: Row) = {&#xA;&#xA;    // Calculate median. This is ugly code copy-pasted from Google, but it works&#xA;    val seq = buffer.getAs[mutable.WrappedArray[(Double)]](0)&#xA;    val sortedSeq = seq.sorted&#xA;    if (seq.size % 2 == 1) sortedSeq(sortedSeq.size / 2)&#xA;    else {&#xA;      val (up, down) = sortedSeq.splitAt(seq.size / 2)&#xA;      (up.last + down.head) / 2&#xA;    }&#xA;  }&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;For business purposes I cannot use a median approximation and I am aware of the scalability issues of calculating a median on a large distributed dataset. I am new to Scala, however, so please let me know if I have any obvious Scala mistakes. &lt;/p&gt;&#xA;" OwnerUserId="554481" LastActivityDate="2018-03-08T20:07:40.600" Title="UDAF to gracefully handle cases where there are no inputs" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;apache-spark-sql&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49181636" PostTypeId="1" CreationDate="2018-03-08T20:09:11.940" Score="0" ViewCount="14" Body="&#xA;&#xA;&lt;p&gt;I have a dataset &lt;code&gt;dataset&lt;/code&gt; which is partitioned on values 00-99 and want to create an RDD &lt;code&gt;first_rdd&lt;/code&gt; to read in the data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I then want to count how many times the word &quot;foo&quot; occurs in the second element of each partition and store the records of each partition in a list. My output would be &lt;code&gt;final_rdd&lt;/code&gt; where each record is of the form &lt;code&gt;(partition_key, (count, record_list))&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;def to_list(a):&#xA;    return [a]&#xA;&#xA;def append(a, b):&#xA;    a.append(b)&#xA;    return a&#xA;&#xA;def extend(a, b):&#xA;    a.extend(b)&#xA;    return a&#xA;&#xA;first_rdd = sqlContext.sql(&quot;select * from dataset&quot;).rdd&#xA;kv_rdd = first_rdd.map(lambda x: (x[4], x)) # x[4] is the partition value&#xA;# Group each partition to (partition_key, [list_of_records])&#xA;grouped_rdd = kv_rdd.combineByKey(to_list, append, extend)&#xA;&#xA;def count_foo(x):&#xA;    count = 0&#xA;    for record in x:&#xA;        if record[1] == &quot;foo&quot;:&#xA;            count = count + 1&#xA;    return (count, x)&#xA;&#xA;final_rdd = grouped_rdd.mapValues(count_foo)&#xA;print(&quot;Counted 'foo' for %s partitions&quot; % (final_rdd.count))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Since each partition of the dataset is totally independent from one another computationally, Spark shouldn't need to shuffle, yet when I look at the SparkUI, I notice that the combineByKey is resulting in a very large shuffle.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have the correct number of initial partitions, and have also tried reading from the partitioned data in HDFS. Each way I try it, I still get a shuffle. What am I doing wrong?&lt;/p&gt;&#xA;" OwnerUserId="4302324" LastEditorUserId="5858851" LastEditDate="2018-03-08T20:47:16.783" LastActivityDate="2018-03-08T20:47:16.783" Title="Avoiding a shuffle in Spark by pre-partitioning files (PySpark)" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;&lt;rdd&gt;&lt;pyspark-sql&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49181701" PostTypeId="1" CreationDate="2018-03-08T20:13:18.057" Score="0" ViewCount="12" Body="&lt;p&gt;I have one &lt;strong&gt;JavaRdd records&lt;/strong&gt;&#xA;I would like to create &lt;strong&gt;3 JavaRdd&lt;/strong&gt; from records depending on condition:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;JavaRdd&amp;lt;MyClass&amp;gt; records1 =records1.filter(record -&amp;gt; “A”.equals(record.getName()));&#xA;JavaRdd&amp;lt;MyClass&amp;gt; records2 =records1.filter(record -&amp;gt; “B”.equals(record.getName()));&#xA;JavaRdd&amp;lt;MyClass&amp;gt; records13=records1.filter(record -&amp;gt; “C”.equals(record.getName()));&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The problem is, that I can do like I show above, but my records may have millions record and I don’t want to scan all records 3 times.&#xA;So I want to do it in one iteration over the records.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I need something like this:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;div class=&quot;snippet&quot; data-lang=&quot;js&quot; data-hide=&quot;false&quot; data-console=&quot;true&quot; data-babel=&quot;false&quot;&gt;&#xD;&#xA;&lt;div class=&quot;snippet-code&quot;&gt;&#xD;&#xA;&lt;pre class=&quot;snippet-code-html lang-html prettyprint-override&quot;&gt;&lt;code&gt;records&#xD;&#xA;	.forEach(record -&amp;gt; {&#xD;&#xA;		if (“A”.equals(records.getName()))&#xD;&#xA;		{&#xD;&#xA;			records1(record);	&#xD;&#xA;		}&#xD;&#xA;		else if (“B”.equals(records.getName()))&#xD;&#xA;		{&#xD;&#xA;			records2(record);	&#xD;&#xA;		}&#xD;&#xA;		else if (“C”.equals(records.getName()))&#xD;&#xA;		{&#xD;&#xA;			records3(record);	&#xD;&#xA;		}&#xD;&#xA;	});&lt;/code&gt;&lt;/pre&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How can I achieve this in Spark usin JavaRDD?&lt;/p&gt;&#xA;" OwnerUserId="8541082" LastActivityDate="2018-03-08T22:56:22.417" Title="Filter JavaRDD into multiple JavaRDD based on Condtion" Tags="&lt;apache-spark&gt;&lt;java-8&gt;&lt;rdd&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49181725" PostTypeId="1" CreationDate="2018-03-08T20:15:38.040" Score="0" ViewCount="12" Body="&lt;p&gt;I'm doing a basic program of implementing Drools and the program runs on an application configuration but when I try to run the JAR, I face an error.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The error I get on the terminal:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    `Suhita-MacBookPro:Drool-CreditScore-Sample sgoswami$ spark-submit --class main.scala.suhita.Sample --master local[*] target/DroolsMaven-1.0-SNAPSHOT.jar &#xA;java.lang.ClassNotFoundException: main.scala.suhita.Sample&#xA;    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)&#xA;    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&#xA;    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&#xA;    at java.lang.Class.forName0(Native Method)&#xA;    at java.lang.Class.forName(Class.java:348)&#xA;    at org.apache.spark.util.Utils$.classForName(Utils.scala:230)&#xA;    at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:712)&#xA;    at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)&#xA;    at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)&#xA;    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)&#xA;    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)`&#xA;&#xA;&#xA;Drools-Project&#xA;  &amp;gt;src&#xA;   &amp;gt;main&#xA;    &amp;gt;scala&#xA;     &amp;gt;suhita&#xA;      - Sample&#xA;      - Applicant&#xA;   &amp;gt;META-INF&#xA;    -kmodule.xml&#xA;    -manifest.MF&#xA;   &amp;gt;resources.rules&#xA;    -rules &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="9448119" LastEditorUserId="1286528" LastEditDate="2018-03-08T22:45:17.150" LastActivityDate="2018-03-09T04:20:34.540" Title="Spark submit in IntelliJ of project jar yields Class not found error" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;jar&gt;&lt;classnotfoundexception&gt;&lt;intellij-14&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="49182070" PostTypeId="1" CreationDate="2018-03-08T20:40:55.830" Score="0" ViewCount="42" Body="&lt;p&gt;I have a dataset (&quot;guid&quot;, &quot;timestamp&quot;, &quot;agt&quot;) like below&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val df = List(Test(&quot;a&quot;, &quot;1&quot;, null),&#xA;   Test(&quot;b&quot;, &quot;2&quot;, &quot;4&quot;),&#xA;   Test(&quot;a&quot;, &quot;1&quot;, &quot;3&quot;),&#xA;   Test(&quot;b&quot;, &quot;2&quot;, &quot;4&quot;),&#xA;   Test(&quot;c&quot;, &quot;1&quot;, &quot;3&quot;),&#xA;   Test(&quot;a&quot;, &quot;6&quot;, &quot;8&quot;),&#xA;   Test(&quot;b&quot;, &quot;2&quot;, &quot;4&quot;),&#xA;   Test(&quot;a&quot;, &quot;1&quot;, &quot;4&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I need to compute &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the minimum timestamp for each row when grouped by guid.   &lt;/li&gt;&#xA;&lt;li&gt;The count for each key when grouped by (guid, timestamp)  &lt;/li&gt;&#xA;&lt;li&gt;The agtM of row when grouped by guid and ordered by timestamp(desc) and then take first non empty agt else &quot;&quot; &lt;/li&gt;&#xA;&lt;li&gt;Drop duplicates&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;So the output will be like below.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+----+---------+---+-------+-----+----+&#xA;|guid|timestamp|agt|minimum|count|agtM|&#xA;+----+---------+---+-------+-----+----+&#xA;|   c|        1|  3|      1|    1|   3|&#xA;|   b|        2|  4|      2|    3|   4|&#xA;|   a|        1|   |      1|    3|   8|&#xA;|   a|        6|  8|      1|    1|   8|&#xA;+----+---------+---+-------+-----+----+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have tried &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val w = Window.partitionBy($&quot;guid&quot;)&#xA;&#xA;    val w1 = Window.partitionBy($&quot;guid&quot;, $&quot;timestamp&quot;)&#xA;    val w2 = Window.partitionBy($&quot;guid&quot;).orderBy($&quot;timestamp&quot;.desc).rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)&#xA;&#xA;    val gg = df.toDS()&#xA;      .withColumn(&quot;minimum&quot;, min(&quot;timestamp&quot;).over(w))&#xA;      .withColumn(&quot;count&quot;, count(&quot;*&quot;).over(w1))&#xA;      .withColumn(&quot;agtM&quot;, coalesce(first($&quot;agt&quot;, true).over(w2), lit(&quot;&quot;)))&#xA;      .dropDuplicates(&quot;guid&quot;, &quot;timestamp&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The agtM calculation I am not so confident though. My target is to achieve minimum shuffling as in this scenario we first group by guid and then group by (guid, timestamp) and logically the second grouping should happen in the first created partition. the output is then grouped by guid and joined with another table. Both data are pretty huge (in TBs) so wanted to achieve this with minimum shuffling and didn't want to move the computation inside mapGroups later (I could have done the agtM calculation simply by filtering the group with non-empty agenttime and then maxBy timestamp). Can you please suggest the the best way to achieve the above?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The agtM calculation has been fixed. Just to give more context for the operations ahead, union of output and another dataset(one extra field, we kept it dummy in the output) will then need to be grouped by key to produce final results. I was also thinking of calculating these values(except window w) inside each partition (mapPartitions), then take the list inside each partition as another list and do further calculation. &lt;/p&gt;&#xA;" OwnerUserId="2114993" LastEditorUserId="2114993" LastEditDate="2018-03-10T04:27:37.247" LastActivityDate="2018-03-10T08:34:21.607" Title="Efficient spark dataset operations when partitioned by overlapping columns" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;spark-dataframe&gt;&lt;apache-spark-dataset&gt;&lt;hadoop-partitioning&gt;" AnswerCount="2" CommentCount="3" />
  <row Id="49182427" PostTypeId="1" CreationDate="2018-03-08T21:05:02.907" Score="0" ViewCount="25" Body="&lt;p&gt;started on Spark couple of days back. I am not able to find enough info to solve this issue. Reading some records from Cassandra and trying to group it, before doing additional logic.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am getting Task Not Serializable in this code block. All of the Custom defined entities are Serializable &lt;/p&gt;&#xA;&#xA;&lt;p&gt;rdd.cache();&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;        JavaPairRDD&amp;lt;EventStatusGroupKey, Iterable&amp;lt;TestEvent&amp;gt;&amp;gt; groupedRdd = rdd.groupBy(new Function&amp;lt;TestEvent, EventStatusGroupKey&amp;gt;() {&#xA;            @Override&#xA;            public EventStatusGroupKey call(TestEvent testEvent) throws Exception {&#xA;                int minute = LocalDateTime.ofInstant(Instant.ofEpochMilli(UUID.fromString(testEvent.getEid()).timestamp()), ZoneId.systemDefault()).getMinute();&#xA;                return new EventStatusGroupKey(testEvent.getStatus(), minute);&#xA;            }&#xA;        });&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What could be causing this issue ?&lt;/p&gt;&#xA;" OwnerUserId="1533080" LastEditorUserId="806736" LastEditDate="2018-03-08T22:26:59.330" LastActivityDate="2018-03-08T22:40:27.107" Title="Spark Java Serialization Exception - Task not Serializable" Tags="&lt;apache-spark&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49182458" PostTypeId="1" CreationDate="2018-03-08T21:07:13.640" Score="1" ViewCount="17" Body="&lt;p&gt;Trying to run Spark program on Pycharm but constantly getting following error, I have configured pycharm with Spark environment but still getting the same result   &lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-none prettyprint-override&quot;&gt;&lt;code&gt;Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties&#xA;Setting default log level to &quot;WARN&quot;.&#xA;To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).&#xA;WARNING: An illegal reflective access operation has occurred&#xA;WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/usr/local/spark/jars/hadoop-auth-2.7.3.jar) to method sun.security.krb5.Config.getInstance()&#xA;WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil&#xA;WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations&#xA;WARNING: All illegal access operations will be denied in a future release&#xA;18/03/08 15:02:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&#xA;Traceback (most recent call last):&#xA;  File &quot;/Users/JKLM/PycharmProjects/test/test.py&quot;, line 14, in &amp;lt;module&amp;gt;&#xA;    output = counts.collect()&#xA;  File &quot;/usr/local/Cellar/apache-spark/2.2.1/libexec/python/pyspark/rdd.py&quot;, line 809, in collect&#xA;    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())&#xA;  File &quot;/Library/Python/2.7/site-packages/py4j/java_gateway.py&quot;, line 1133, in __call__&#xA;    answer, self.gateway_client, self.target_id, self.name)&#xA;  File &quot;/Library/Python/2.7/site-packages/py4j/protocol.py&quot;, line 319, in get_return_value&#xA;    format(target_id, &quot;.&quot;, name), value)&#xA;py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.&#xA;: java.lang.IllegalArgumentException&#xA;    at org.apache.xbean.asm5.ClassReader.&amp;lt;init&amp;gt;(Unknown Source)&#xA;    at org.apache.xbean.asm5.ClassReader.&amp;lt;init&amp;gt;(Unknown Source)&#xA;    at org.apache.xbean.asm5.ClassReader.&amp;lt;init&amp;gt;(Unknown Source)&#xA;    at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46)&#xA;    at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:443)&#xA;    at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:426)&#xA;    at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)&#xA;    at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)&#xA;    at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)&#xA;    at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)&#xA;    at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)&#xA;    at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)&#xA;    at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)&#xA;    at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:426)&#xA;    at org.apache.xbean.asm5.ClassReader.a(Unknown Source)&#xA;    at org.apache.xbean.asm5.ClassReader.b(Unknown Source)&#xA;    at org.apache.xbean.asm5.ClassReader.accept(Unknown Source)&#xA;    at org.apache.xbean.asm5.ClassReader.accept(Unknown Source)&#xA;    at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:257)&#xA;    at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:256)&#xA;    at scala.collection.immutable.List.foreach(List.scala:381)&#xA;    at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:256)&#xA;    at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:156)&#xA;    at org.apache.spark.SparkContext.clean(SparkContext.scala:2294)&#xA;    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2068)&#xA;    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)&#xA;    at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)&#xA;    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&#xA;    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)&#xA;    at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)&#xA;    at org.apache.spark.rdd.RDD.collect(RDD.scala:935)&#xA;    at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)&#xA;    at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)&#xA;    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#xA;    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#xA;    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#xA;    at java.base/java.lang.reflect.Method.invoke(Method.java:564)&#xA;    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#xA;    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#xA;    at py4j.Gateway.invoke(Gateway.java:280)&#xA;    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#xA;    at py4j.commands.CallCommand.execute(CallCommand.java:79)&#xA;    at py4j.GatewayConnection.run(GatewayConnection.java:214)&#xA;    at java.base/java.lang.Thread.run(Thread.java:844)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="4682302" LastEditorUserId="5858851" LastEditDate="2018-03-08T21:25:37.323" LastActivityDate="2018-03-08T21:25:37.323" Title="Not able to run spark file in Pycharm" Tags="&lt;macos&gt;&lt;python-2.7&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;pycharm&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49182536" PostTypeId="1" CreationDate="2018-03-08T21:12:03.623" Score="0" ViewCount="8" Body="&lt;p&gt;I'm trying to import a custom library from a subdirectory.  My files are organized like so:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;proj&#xA;    main.py&#xA;    subdir&#xA;        library.py&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I attach &lt;code&gt;library.py&lt;/code&gt; to the job through &lt;code&gt;sc.addPyFile('./subdir/library.py')&lt;/code&gt; within main.py and then I've tried to import again within &lt;code&gt;main.py&lt;/code&gt; through both &lt;code&gt;import subdir.library&lt;/code&gt; and &lt;code&gt;from subdir import library&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;But once I submit my job, I get &lt;code&gt;ImportError: No module named subdir.library&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since I'm able to import this library just fine in Python either of the above commands, I think it must be somehow connected to how I'm zipping the pyFile to the job but I can't figure it out. &lt;/p&gt;&#xA;" OwnerUserId="8941248" LastActivityDate="2018-03-08T21:12:03.623" Title="Pyspark cannot find local submodule (ImportError)" Tags="&lt;pyspark&gt;&lt;importerror&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49182779" PostTypeId="1" CreationDate="2018-03-08T21:29:15.627" Score="1" ViewCount="16" Body="&lt;p&gt;I'm trying to run a pySpark job with custom inputs, for testing purposes.&#xA;The job has three sets of input, each read from a table in a different metastore database.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The data is read in spark with: &lt;code&gt;hiveContext.table('myDb.myTable')&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The test inputs are three files. In an attempt to not change any of the original code, I read all three inputs into &lt;code&gt;DataFrame&lt;/code&gt;s, and attempt to register a temp table with &lt;code&gt;myDF.registerTempTable('myDb.myTable')&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The problem is that spark fails with &lt;code&gt;org.apache.spark.sql.catalyst.analysis.NoSuchTableException&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've also tried:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;hiveContext.sql('create database if not exists myDb')&#xA;hiveContext.sql('use myDb')&#xA;myDF.registerTempTable('myTable')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But that fails as well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any idea why the table cannot be found?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Using Spark 1.6&lt;/p&gt;&#xA;" OwnerUserId="416300" LastActivityDate="2018-03-08T21:29:15.627" Title="Spark temp tables not found" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;&lt;apache-spark-1.6&gt;&lt;hive-metastore&gt;" AnswerCount="0" CommentCount="6" />
  <row Id="49182855" PostTypeId="1" CreationDate="2018-03-08T21:34:43.540" Score="0" ViewCount="7" Body="&lt;p&gt;I have a custom merger function. I need to pass my partitions 2 x 2 to get 1 instead. What do i need to do this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;MapPartitions does not work since it is for one partition at a time.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;An option that I was thinking was to create N single-partition RDDs and then use collected rows for each pair&lt;/p&gt;&#xA;" OwnerUserId="5669031" LastActivityDate="2018-03-08T22:19:47.343" Title="Merging Partitions 2 x 2 in Spark" Tags="&lt;apache-spark&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49182910" PostTypeId="1" CreationDate="2018-03-08T21:38:02.343" Score="0" ViewCount="37" Body="&lt;p&gt;I am connecting spark with Cassandra and I am storing csv file in Cassandra, when I enter this command I got error.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; dfprev.write.format(&quot;org.apache.spark.sql.cassandra&quot;) .options(Map(&quot;keyspace&quot;-&amp;gt;&quot;sensorkeyspace&quot;,&quot;table&quot;-&amp;gt;&quot;sensortable&quot;)).save()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then I got this error.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;java.io.IOException: Failed to open native connection to Cassandra at {127.0.0.1}:9042&#xA;    at com.datastax.spark.connector.cql.CassandraConnector$.com$datastax$spark$connector$cql$CassandraConnector$$createSession(CassandraConnector.scala:168)&#xA;    at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$8.apply(CassandraConnector.scala:154)&#xA;    at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$8.apply(CassandraConnector.scala:154)&#xA;    at com.datastax.spark.connector.cql.RefCountedCache.createNewValueAndKeys(RefCountedCache.scala:32)&#xA;    at com.datastax.spark.connector.cql.RefCountedCache.syncAcquire(RefCountedCache.scala:69)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="6450118" LastActivityDate="2018-03-09T06:20:04.467" Title="Cassandra connection to Spark" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;cassandra&gt;&lt;spark-dataframe&gt;&lt;cassandra-3.0&gt;" AnswerCount="2" CommentCount="2" />
  <row Id="49183006" PostTypeId="1" AcceptedAnswerId="49183145" CreationDate="2018-03-08T21:44:53.133" Score="1" ViewCount="26" Body="&lt;p&gt;CAn anyone tell me please what is wrong with my code:&#xA;Below is my spark code in scala:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import java.text.SimpleDateFormat&#xA;import org.apache.spark.sql.SparkSession&#xA;import scala.xml.XML&#xA;&#xA;object TopTenTags09 {&#xA;  def main(args:Array[String]){&#xA;    val format = new SimpleDateFormat(&quot;yyyy-MM-dd'T'HH:mm:ss.SSS&quot;)&#xA;    val format2 = new SimpleDateFormat(&quot;yyyy-MM&quot;)&#xA;&#xA;    val spark = SparkSession.builder().appName(&quot;Number of posts which are questions and contains specified words&quot;).master(&quot;local&quot;).getOrCreate()&#xA;&#xA;    val data = spark.read.textFile(&quot;/home/harsh/Hunny/HadoopPractice/Spark/DF/StackOverFlow/Posts.xml&quot;).rdd&#xA;&#xA;    val result = data.filter{line=&amp;gt;{line.trim().startsWith(&quot;&amp;lt;row&quot;)}}&#xA;    .filter{line=&amp;gt;{line.contains(&quot;PostTypeId=\&quot;1\&quot;&quot;)}}&#xA;    .map { line=&amp;gt;{&#xA;      val xml = XML.loadString(line)&#xA;      if(xml.attribute(&quot;Tags&quot;).mkString.toLowerCase().contains(&quot;hadoop&quot;) ||&#xA;          xml.attribute(&quot;Tags&quot;).mkString.toLowerCase().contains(&quot;spark&quot;)){&#xA;        (Integer.parseInt(xml.attribute(&quot;Score&quot;).toString()),Integer.parseInt(xml.attribute(&quot;Score&quot;).toString()))&#xA;      }   &#xA;    }}/*.filter(line=&amp;gt;line._1&amp;gt;2)&#xA;    .sortByKey(false)*/&#xA;&#xA;    result.foreach(println) //throwing error while printing&#xA;&#xA;    spark.stop&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And below is the error I am getting while running it:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;java.lang.NumberFormatException: For input string: &quot;Some(12)&quot;&#xA;at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)&#xA;at java.lang.Integer.parseInt(Integer.java:580)&#xA;at java.lang.Integer.parseInt(Integer.java:615)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am new to spark and the the error is making me crazy because as mentioned in error ther is no &quot;Some&quot; in code or in data.Can anyone help me please.&#xA;Sample data&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  &amp;lt;row Id=&quot;5&quot; PostTypeId=&quot;1&quot; CreationDate=&quot;2014-05-13T23:58:30.457&quot; Score=&quot;7&quot; ViewCount=&quot;286&quot; Body=&quot;&amp;amp;lt;p&amp;amp;gt;I've always been interested in machine learning, but I can't figure out one thing about starting out with a simple &amp;amp;quot;Hello World&amp;amp;quot; example - how can I avoid hard-coding behavior?&amp;amp;lt;/p&amp;amp;gt;&amp;amp;#xA;&amp;amp;#xA;&amp;amp;lt;p&amp;amp;gt;For example, if I wanted to &amp;amp;quot;teach&amp;amp;quot; a bot how to avoid randomly placed obstacles, I couldn't just use relative motion, because the obstacles move around, but I don't want to hard code, say, distance, because that ruins the whole point of machine learning.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;#xA;&amp;amp;#xA;&amp;amp;lt;p&amp;amp;gt;Obviously, randomly generating code would be impractical, so how could I do this?&amp;amp;lt;/p&amp;amp;gt;&amp;amp;#xA;&quot; OwnerUserId=&quot;5&quot; LastActivityDate=&quot;2014-05-14T00:36:31.077&quot; Title=&quot;How can I do simple machine learning without hard-coding behavior?&quot; Tags=&quot;&amp;amp;lt;machine-learning&amp;amp;gt;&quot; AnswerCount=&quot;1&quot; CommentCount=&quot;1&quot; FavoriteCount=&quot;1&quot; ClosedDate=&quot;2014-05-14T14:40:25.950&quot; /&amp;gt;&#xA;  &amp;lt;row Id=&quot;7&quot; PostTypeId=&quot;1&quot; AcceptedAnswerId=&quot;10&quot; CreationDate=&quot;2014-05-14T00:11:06.457&quot; Score=&quot;2&quot; ViewCount=&quot;266&quot; Body=&quot;&amp;amp;lt;p&amp;amp;gt;As a researcher and instructor, I'm looking for open-source books (or similar materials) that provide a relatively thorough overview of data science from an applied perspective. To be clear, I'm especially interested in a thorough overview that provides material suitable for a college-level course, not particular pieces or papers.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;#xA;&quot; OwnerUserId=&quot;36&quot; LastEditorUserId=&quot;97&quot; LastEditDate=&quot;2014-05-16T13:45:00.237&quot; LastActivityDate=&quot;2014-05-16T13:45:00.237&quot; Title=&quot;What open-source books (or other materials) provide a relatively thorough overview of data science?&quot; Tags=&quot;&amp;amp;lt;education&amp;amp;gt;&amp;amp;lt;open-source&amp;amp;gt;&quot; AnswerCount=&quot;3&quot; CommentCount=&quot;4&quot; FavoriteCount=&quot;1&quot; ClosedDate=&quot;2014-05-14T08:40:54.950&quot; /&amp;gt;&#xA;  &amp;lt;row Id=&quot;9&quot; PostTypeId=&quot;2&quot; ParentId=&quot;5&quot; CreationDate=&quot;2014-05-14T00:36:31.077&quot; Score=&quot;4&quot; Body=&quot;&amp;amp;lt;p&amp;amp;gt;Not sure if this fits the scope of this SE, but here's a stab at an answer anyway.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;#xA;&amp;amp;#xA;&amp;amp;lt;p&amp;amp;gt;With all AI approaches you have to decide what it is you're modelling and what kind of uncertainty there is. Once you pick a framework that allows modelling of your situation, you then see which elements are &amp;amp;quot;fixed&amp;amp;quot; and which are flexible. For example, the model may allow you to define your own network structure (or even learn it) with certain constraints. You have to decide whether this flexibility is sufficient for your purposes. Then within a particular network structure, you can learn parameters given a specific training dataset.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;#xA;&amp;amp;#xA;&amp;amp;lt;p&amp;amp;gt;You rarely hard-code behavior in AI/ML solutions. It's all about modelling the underlying situation and accommodating different situations by tweaking elements of the model.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;#xA;&amp;amp;#xA;&amp;amp;lt;p&amp;amp;gt;In your example, perhaps you might have the robot learn how to detect obstacles (by analyzing elements in the environment), or you might have it keep track of where the obstacles were and which way they were moving.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;#xA;&quot; OwnerUserId=&quot;51&quot; LastActivityDate=&quot;2014-05-14T00:36:31.077&quot; CommentCount=&quot;0&quot; /&amp;gt;&#xA;  &amp;lt;row Id=&quot;10&quot; PostTypeId=&quot;2&quot; ParentId=&quot;7&quot; CreationDate=&quot;2014-05-14T00:53:43.273&quot; Score=&quot;9&quot; Body=&quot;&amp;amp;lt;p&amp;amp;gt;One book that's freely available is &amp;amp;quot;The Elements of Statistical Learning&amp;amp;quot; by Hastie, Tibshirani, and Friedman (published by Springer): &amp;amp;lt;a href=&amp;amp;quot;http://statweb.stanford.edu/~tibs/ElemStatLearn/&amp;amp;quot;&amp;amp;gt;see Tibshirani's website&amp;amp;lt;/a&amp;amp;gt;.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;#xA;&amp;amp;#xA;&amp;amp;lt;p&amp;amp;gt;Another fantastic source, although it isn't a book, is Andrew Ng's Machine Learning course on Coursera. This has a much more applied-focus than the above book, and Prof. Ng does a great job of explaining the thinking behind several different machine learning algorithms/situations.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;#xA;&quot; OwnerUserId=&quot;22&quot; LastActivityDate=&quot;2014-05-14T00:53:43.273&quot; CommentCount=&quot;1&quot; /&amp;gt;&#xA;  &amp;lt;row Id=&quot;14&quot; PostTypeId=&quot;1&quot; CreationDate=&quot;2014-05-14T01:25:59.677&quot; Score=&quot;14&quot; ViewCount=&quot;686&quot; Body=&quot;&amp;amp;lt;p&amp;amp;gt;I am sure data science as will be discussed in this forum has several synonyms or at least related fields where large data is analyzed.&amp;amp;lt;/p&amp;amp;gt;&amp;amp;#xA;&amp;amp;#xA;&amp;amp;lt;p&amp;amp;gt;My particular question is in regards to Data Mining.  I took a graduate class in Data Mining a few years back.  What are the differences between Data Science and Data Mining and in particular what more would I need to look at to become proficient in Data Mining?&amp;amp;lt;/p&amp;amp;gt;&amp;amp;#xA;&quot; OwnerUserId=&quot;66&quot; LastEditorUserId=&quot;322&quot; LastEditDate=&quot;2014-06-17T16:17:20.473&quot; LastActivityDate=&quot;2014-06-20T17:36:05.023&quot; Title=&quot;Is Data Science the Same as Data Mining?&quot; Tags=&quot;&amp;amp;lt;data-mining&amp;amp;gt;&amp;amp;lt;definitions&amp;amp;gt;&quot; AnswerCount=&quot;4&quot; CommentCount=&quot;1&quot; FavoriteCount=&quot;2&quot; /&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="9323888" LastEditorUserId="9323888" LastEditDate="2018-03-08T21:58:21.503" LastActivityDate="2018-03-08T21:58:21.503" Title="java.lang.NumberFormatException: For input string: &quot;Some(12)&quot;" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="49183699" PostTypeId="1" CreationDate="2018-03-08T22:41:01.923" Score="-1" ViewCount="26" Body="&lt;p&gt;I have the following tuple,&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;         #KEY                       X          Y         Z&#xA;    [(('a', 'nexus4', 'stand'), ((-5.958191, 0.6880646, 8.135345), &#xA;   # X MEAN                 Y MEAN            Z MEAN&#xA;(-5.944848294736841, 0.6861853331578948, 8.128266868421052))),(('a', 'nexus4', 'stand'), ((-5.95224, 0.6702118, 8.136536), (-5.944848294736841, 0.6861853331578948, 8.128266868421052))), (('a', 'nexus4', 'stand'), ((-5.9950867, 0.6535491999999999, 8.204376), (-5.944848294736841, 0.6861853331578948, 8.128266868421052)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As you can see, all the KEY's have different X,Y,Z coordinates but the same MEAN for each one, because as they refer to the same mobile model, and after grouping and calculating the mean, if they are the same Model (same key) they will have the same Mean.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, what I want to do , is to calculate the standar deviation for each KEY.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My idea is, using the mean and reduceByKey to each coordinate for example X subtract X mean and raise it by 2, then with a MapValue function apply sqrt function to each one.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't know if it's a good idea for doing that, but it's what I thought.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Another idea is using the RDD I have created for calculating my MEAN, that  it has the following structure. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;             #KEY                     MEAN X              MEAN Y&#xA;    [(('a', 'nexus4', 'stand'), (-5.944848294736841, 0.6861853331578948, &#xA;       MEAN Z&#xA;8.128266868421052))]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is a code (not working) where to my first idea I try to get all X,Y,Z values ... to then subtract to them each corresponding MEAN and then ^2.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;std = meanRDD1.reduceByKey(lambda a , b: ((a[0][0] - b[1][0])**2 , (a[0][1] - b[1])**2 , (a[0][2] - b[2])**2))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm confused about accesing to my tuple float numbers, so I'm getting this error : &lt;strong&gt;TypeError: unsupported operand type(s) for -: 'float' and 'tuple'&lt;/strong&gt;&lt;/p&gt;&#xA;" OwnerUserId="6297869" LastActivityDate="2018-03-08T22:41:01.923" Title="Obtain deviation using Mean RDD - Pyspark" Tags="&lt;pyspark&gt;&lt;std&gt;&lt;mean&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49184050" PostTypeId="1" CreationDate="2018-03-08T23:11:20.220" Score="0" ViewCount="14" Body="&lt;p&gt;I have a general question regarding the appropriateness of using Spark for a type of problem I frequently encounter in Python: performing the same task on the same set of data using different parameter settings using the &lt;code&gt;multiprocessing&lt;/code&gt; package.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Consider the following toy example (note this is just an example of processing in Python; you might have used another approach):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import multiprocessing as mp&#xA;import pandas as pd&#xA;import numpy as np&#xA;&#xA;mydf = pd.DataFrame({'a':np.random.random(100)})&#xA;&#xA;output = mp.Queue()&#xA;&#xA;def count_number_of_rows_above_k(indf,k,output):&#xA;  answer = sum(indf.a &amp;gt; k)&#xA;  output.put(answer)&#xA;&#xA;processes = [mp.Process(target=count_number_of_rows_above_k,args=(mydf,k,output)) for k in np.random.random(10)]&#xA;&#xA;for p in processes:&#xA;  p.start()&#xA;for p in processes:&#xA;  p.join()&#xA;&#xA;results = [output.get() for item in processes]&#xA;print results&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The point is that I have a blob of data, in this case a Pandas dataframe, and I performing a standard function to it using different parameter values. I perform this in parallel and then collect the results at the end. &lt;em&gt;This is what I would like to do in Spark&lt;/em&gt;, under the belief that I could scale more easily and benefit from the builtin fault tolerance. In real life, the function would of course be significantly more complex and the data would be much larger.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In my reading on Spark, all the examples I have seen feature builtin routines using Spark dataframes. For example, counting the number of columns, summing a column, filtering, etc. I want to apply a &lt;strong&gt;custom&lt;/strong&gt; function to my data.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is Spark appropriate for my problem? If so, how do I implement this? Do I need to push the dataframe to all the worker nodes beforehand?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am just asking for a few pointers. There must be documentation on this out there that I haven't found yet. Thanks.&lt;/p&gt;&#xA;" OwnerUserId="1472433" LastActivityDate="2018-03-10T13:07:38.517" Title="Using Apache Spark to parallelize processing of a Pandas dataframe" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;multiprocessing&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49184596" PostTypeId="1" CreationDate="2018-03-09T00:07:26.550" Score="0" ViewCount="11" Body="&lt;p&gt;I've got a large datastore of JSON objects that I'm trying to load into Spark in a dataframe, but I'm receiving an AnalysisException when trying to do any processing on the data.  There are several differently formatted JSON objects in the input data.  Some of them have the same fields in different orders and/or levels of the JSON.  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've loaded the JSON in with the following code.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    val messagesDF = spark.read.json(&quot;data/test&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But once I try to perform any operations on the data I receive the following stack trace:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Caused by: java.lang.reflect.InvocationTargetException: org.apache.spark.sql.AnalysisException: Reference 'SGLN' is ambiguous, could be: SGLN#1099, SGLN#1204.;&#xA;  at sun.reflect.GeneratedMethodAccessor86.invoke(Unknown Source)&#xA;  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#xA;  at java.lang.reflect.Method.invoke(Method.java:483)&#xA;  at org.apache.zeppelin.spark.ZeppelinContext.showDF(ZeppelinContext.java:235)&#xA;  ... 48 more&#xA;Caused by: org.apache.spark.sql.AnalysisException: Reference 'SGLN' is ambiguous, could be: SGLN#1099, SGLN#1204.;&#xA;  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:264)&#xA;  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:158)&#xA;  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolve$1.apply(LogicalPlan.scala:130)&#xA;  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolve$1.apply(LogicalPlan.scala:129)&#xA;  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#xA;  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&#xA;  at scala.collection.Iterator$class.foreach(Iterator.scala:893)&#xA;  at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)&#xA;  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)&#xA;  at org.apache.spark.sql.types.StructType.foreach(StructType.scala:96)&#xA;  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&#xA;  at org.apache.spark.sql.types.StructType.map(StructType.scala:96)&#xA;  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:129)&#xA;  at org.apache.spark.sql.execution.datasources.FileSourceStrategy$.apply(FileSourceStrategy.scala:83)&#xA;  at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:62)&#xA;  at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:62)&#xA;  at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)&#xA;  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)&#xA;  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)&#xA;  at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)&#xA;  at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:77)&#xA;  at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:74)&#xA;  at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)&#xA;  at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)&#xA;  at scala.collection.Iterator$class.foreach(Iterator.scala:893)&#xA;  at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)&#xA;  at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)&#xA;  at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)&#xA;  at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:74)&#xA;  at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:66)&#xA;  at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)&#xA;  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)&#xA;  at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)&#xA;  at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:84)&#xA;  at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:80)&#xA;  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:89)&#xA;  at &#xA;&#xA;org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:89)&#xA;      at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2814)&#xA;      at org.apache.spark.sql.Dataset.head(Dataset.scala:2127)&#xA;      at org.apache.spark.sql.Dataset.take(Dataset.scala:2342)&#xA;      ... 52 more&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Everything I've been able to find on this error is from joining different dataframes which is certainly similar to what I'm trying to do, but I can't go in and refactor the dataframe to provide unique ids.  &lt;/p&gt;&#xA;" OwnerUserId="5495519" LastActivityDate="2018-03-09T00:07:26.550" Title="org.apache.spark.sql.AnalysisException when loading JSON file" Tags="&lt;json&gt;&lt;apache-spark&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49184825" PostTypeId="1" CreationDate="2018-03-09T00:35:44.403" Score="0" ViewCount="18" Body="&lt;p&gt;If I have:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SparkSession spk = SparkSession.builder().master(&quot;somewhere&quot;).getOrCreate();&#xA;Dataset&amp;lt;Row&amp;gt; x = spk.createDataFrame(someList);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How do I instruct spark to take x and distribute it immediately among worker nodes such that memory on the driver is freed, such that more can be loaded? I'd like to use a small host to load up data, which is processed on the workers. Is &lt;code&gt;x.repartition(numWorkers).cache()&lt;/code&gt; sufficient?&lt;/p&gt;&#xA;" OwnerUserId="1763955" LastEditorUserId="184201" LastEditDate="2018-03-09T09:41:14.810" LastActivityDate="2018-03-09T09:41:14.810" Title="How do I move a cache to spark storage from the driver?" Tags="&lt;apache-spark&gt;&lt;spark-dataframe&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49184830" PostTypeId="1" CreationDate="2018-03-09T00:36:45.310" Score="3" ViewCount="37" Body="&lt;p&gt;As a bit of background, I'm trying to implement the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator&quot; rel=&quot;nofollow noreferrer&quot;&gt;Kaplan-Meier&lt;/a&gt; in Spark.  In particular, I assume I have a data frame/set with a &lt;code&gt;Double&lt;/code&gt; column denoted as &lt;code&gt;Data&lt;/code&gt; and an &lt;code&gt;Int&lt;/code&gt; column named &lt;code&gt;censorFlag&lt;/code&gt; (&lt;code&gt;0&lt;/code&gt; value if censored, &lt;code&gt;1&lt;/code&gt; if not, prefer this over &lt;code&gt;Boolean&lt;/code&gt; type).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Example:  &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val df = Seq((1.0, 1), (2.3, 0), (4.5, 1), (0.8, 1), (0.7, 0), (4.0, 1), (0.8, 1)).toDF(&quot;data&quot;, &quot;censorFlag&quot;).as[(Double, Int)] &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now I need to compute a column &lt;code&gt;wins&lt;/code&gt; that counts instances of each &lt;code&gt;data&lt;/code&gt; value. I achieve that with the following code:  &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val distDF = df.withColumn(&quot;wins&quot;, sum(col(&quot;censorFlag&quot;)).over(Window.partitionBy(&quot;data&quot;).orderBy(&quot;data&quot;)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The problem comes when I need to compute a quantity called &lt;code&gt;atRisk&lt;/code&gt; which counts, for each value of &lt;code&gt;data&lt;/code&gt;, the number of &lt;code&gt;data&lt;/code&gt; points that are greater than or equal to it (a cumulative filtered count, if you will).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The following code works:  &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// We perform the counts per value of &quot;bins&quot;. This is an array of doubles&#xA;val bins = df.select(col(&quot;data&quot;).as(&quot;dataBins&quot;)).distinct().sort(&quot;dataBins&quot;).as[Double].collect &#xA;val atRiskCounts = bins.map(x =&amp;gt; (x, df.filter(col(&quot;data&quot;).geq(x)).count)).toSeq.toDF(&quot;data&quot;, &quot;atRisk&quot;)&#xA;// this works:&#xA;atRiskCounts.show&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, the use case involves deriving &lt;code&gt;bins&lt;/code&gt; from the column &lt;code&gt;data&lt;/code&gt; &lt;em&gt;itself&lt;/em&gt;, which I'd rather leave as a single column data set (or RDD at worst), but certainly not local array. But this doesn't work:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// Here, 'bins' rightfully come from the data itself.&#xA;val bins = df.select(col(&quot;data&quot;).as(&quot;dataBins&quot;)).distinct().as[Double]&#xA;val atRiskCounts = bins.map(x =&amp;gt; (x, df.filter(col(&quot;data&quot;).geq(x)).count)).toSeq.toDF(&quot;data&quot;, &quot;atRisk&quot;)&#xA;// This doesn't work -- NullPointerException&#xA;atRiskCounts.show&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Nor does this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// Manually creating the bins and then parallelizing them.&#xA;val bins = Seq(0.7, 0.8, 1.0, 3.0).toDS&#xA;val atRiskCounts = bins.map(x =&amp;gt; (x, df.filter(col(&quot;data&quot;).geq(x)).count)).toDF(&quot;data&quot;, &quot;atRisk&quot;)&#xA;// Also fails with a NullPointerException&#xA;atRiskCounts.show&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Another approach that &lt;em&gt;does&lt;/em&gt; work, but is also not satisfactory from a parallelization perspective is using &lt;code&gt;Window&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// Do the counts in one fell swoop using a giant window per value.&#xA;val atRiskCounts = df.withColumn(&quot;atRisk&quot;, count(&quot;censorFlag&quot;).over(Window.orderBy(&quot;data&quot;).rowsBetween(0, Window.unboundedFollowing))).groupBy(&quot;data&quot;).agg(first(&quot;atRisk&quot;).as(&quot;atRisk&quot;))&#xA;// Works, BUT, we get a &quot;WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.&quot; &#xA;atRiskCounts.show&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This last solution isn't useful as it ends up shuffling my data to a single partition (and in that case, I might as well go with Option 1 tha works).  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;The successful approaches are fine except that the bins are not parallel, which is something I'd really like to keep if possible.  I've looked at &lt;code&gt;groupBy&lt;/code&gt; aggregations, &lt;code&gt;pivot&lt;/code&gt; type of aggregations, but none seem to make sense.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My question is: is there any way to compute &lt;code&gt;atRisk&lt;/code&gt; column in a distributed way?  Also, why do I get a &lt;code&gt;NullPointerException&lt;/code&gt; in the failed solutions?&lt;/p&gt;&#xA;" OwnerUserId="6802640" LastActivityDate="2018-03-09T03:25:36.300" Title="Spark RDD Or SQL operations to compute conditional counts" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;spark-dataframe&gt;&lt;rdd&gt;" AnswerCount="2" CommentCount="1" FavoriteCount="0" />
  <row Id="49185006" PostTypeId="1" CreationDate="2018-03-09T00:58:08.237" Score="0" ViewCount="13" Body="&lt;p&gt;I am trying to allow users to see their token.   Laravek\Spark\Token looks partially like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;?php&#xA;&#xA;namespace Laravel\Spark;&#xA;&#xA;use Carbon\Carbon;&#xA;use Illuminate\Support\Arr;&#xA;use Illuminate\Database\Eloquent\Model;&#xA;&#xA;class Token extends Model&#xA;{&#xA;&#xA;/**&#xA; * The guarded attributes on the model.&#xA; *&#xA; * @var array&#xA; */&#xA;protected $guarded = [];&#xA;&#xA;/**&#xA; * The attributes excluded from the model's JSON form.&#xA; *&#xA; * @var array&#xA; */&#xA;protected $hidden = [&#xA;    'token',&#xA;];&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am trying to remove token from being hidden.   If I simply remove it from this core file, then when I update spark it is overridden.   How do I change this value in code?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I extend the token class, then I have to change other core files to use the extended class.  &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;?php&#xA;&#xA;namespace App\Models;&#xA;&#xA;use Laravel\Spark\Token;&#xA;&#xA;class VisibleToken extends Token {&#xA;&#xA;    protected $hidden = [];&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any insights are appreciated!  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;UPDATE:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I thought for sure this would work, but it doesn't.  I still don't receive the token attribute.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;      $userId = Auth::user()-&amp;gt;id;&#xA;      $tokenModel = new Token();&#xA;      $tokenModel-&amp;gt;setVisible(['token']);&#xA;      $tokenModel-&amp;gt;setHidden([]);&#xA;      $tokens = $tokenModel-&amp;gt;where('user_id', '=', $userId)-&amp;gt;get();&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="55124" LastEditorUserId="55124" LastEditDate="2018-03-09T02:28:32.327" LastActivityDate="2018-03-09T02:28:32.327" Title="Laravel Spark Token Visibility" Tags="&lt;php&gt;&lt;laravel&gt;&lt;apache-spark&gt;&lt;laravel-spark&gt;&lt;extending-classes&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="49185249" PostTypeId="1" CreationDate="2018-03-09T01:29:24.420" Score="0" ViewCount="17" Body="&lt;p&gt;I install zeppelin by binary package with all interpreters, the version is 0.7.3. After I run some spark job and when all the job is finish, I found that zeppelin is still running and take all the resources of CPU, but there is not spark job run. Is there anyone know what happen or how to debug this?&lt;/p&gt;&#xA;" OwnerUserId="9465161" LastEditorUserId="2308683" LastEditDate="2018-03-09T01:30:43.890" LastActivityDate="2018-03-09T06:04:03.727" Title="zeppelin take all the resources of CPU when there are not job in running" Tags="&lt;apache-spark&gt;&lt;apache-zeppelin&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="49185461" PostTypeId="1" CreationDate="2018-03-09T01:55:35.827" Score="1" ViewCount="27" Body="&lt;p&gt;I am very new to Python. Using Python 2.7 &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am trying to run this simple code.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am creating this DF from a CSV file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This Dataframe has just 2 columns. I have tried below code snippets but it is getting Failed with every try&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;newDf = fullDf.rdd.map(lambda x: str(x[1])).collect()   # FAILS&#xA;newDf = fullDf.rdd.map(lambda x: x.split(&quot;,&quot;)[1]).collect()   # FAILS&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What is the Issue here. Same thing works in Scala-Spark.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My Spark version is 2.1.0, Python version 2.7&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I don't know what this Error is :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;18/03/08 17:47:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&#xA;18/03/08 17:47:17 ERROR Executor: Exception in task 1.0 in stage 3.0 (TID 5)&#xA;java.io.IOException: Cannot run program &quot;python&quot;: CreateProcess error=2, The system cannot find the file specified&#xA;    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)&#xA;    at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:120)&#xA;    at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:67)&#xA;    at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:116)&#xA;    at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)&#xA;    at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)&#xA;    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)&#xA;    at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)&#xA;    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)&#xA;    at org.apache.spark.scheduler.Task.run(Task.scala:99)&#xA;    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;    at java.lang.Thread.run(Thread.java:748)&#xA;Caused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified&#xA;    at java.lang.ProcessImpl.create(Native Method)&#xA;    at java.lang.ProcessImpl.&amp;lt;init&amp;gt;(ProcessImpl.java:386)&#xA;    at java.lang.ProcessImpl.start(ProcessImpl.java:137)&#xA;    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)&#xA;    ... 13 more&#xA;18/03/08 17:47:17 ERROR Executor: Exception in task 2.0 in stage 3.0 (TID 6)&#xA;java.io.IOException: Cannot run program &quot;python&quot;: CreateProcess error=2, The system cannot find the file specified&#xA;    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)&#xA;    at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:120)&#xA;    at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:67)&#xA;    at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:116)&#xA;    at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)&#xA;    at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)&#xA;    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)&#xA;    at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)&#xA;    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)&#xA;    at org.apache.spark.scheduler.Task.run(Task.scala:99)&#xA;    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;    at java.lang.Thread.run(Thread.java:748)&#xA;Caused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified&#xA;    at java.lang.ProcessImpl.create(Native Method)&#xA;    at java.lang.ProcessImpl.&amp;lt;init&amp;gt;(ProcessImpl.java:386)&#xA;    at java.lang.ProcessImpl.start(ProcessImpl.java:137)&#xA;    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)&#xA;    ... 13 more&#xA;18/03/08 17:47:17 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 4)&#xA;java.io.IOException: Cannot run program &quot;python&quot;: CreateProcess error=2, The system cannot find the file specified&#xA;    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)&#xA;    at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:120)&#xA;    at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:67)&#xA;    at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:116)&#xA;    at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)&#xA;    at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)&#xA;    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)&#xA;    at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)&#xA;    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)&#xA;    at org.apache.spark.scheduler.Task.run(Task.scala:99)&#xA;    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;    at java.lang.Thread.run(Thread.java:748)&#xA;Caused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified&#xA;    at java.lang.ProcessImpl.create(Native Method)&#xA;    at java.lang.ProcessImpl.&amp;lt;init&amp;gt;(ProcessImpl.java:386)&#xA;    at java.lang.ProcessImpl.start(ProcessImpl.java:137)&#xA;    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)&#xA;    ... 13 more&#xA;18/03/08 17:47:17 WARN TaskSetManager: Lost task 1.0 in stage 3.0 (TID 5, localhost, executor driver): java.io.IOException: Cannot run program &quot;python&quot;: CreateProcess error=2, The system cannot find the file specified&#xA;    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)&#xA;    at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:120)&#xA;    at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:67)&#xA;    at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:116)&#xA;    at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)&#xA;    at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)&#xA;    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)&#xA;    at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)&#xA;    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)&#xA;    at org.apache.spark.scheduler.Task.run(Task.scala:99)&#xA;    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;    at java.lang.Thread.run(Thread.java:748)&#xA;Caused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified&#xA;    at java.lang.ProcessImpl.create(Native Method)&#xA;    at java.lang.ProcessImpl.&amp;lt;init&amp;gt;(ProcessImpl.java:386)&#xA;    at java.lang.ProcessImpl.start(ProcessImpl.java:137)&#xA;    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)&#xA;    ... 13 more&#xA;&#xA;18/03/08 17:47:17 ERROR TaskSetManager: Task 1 in stage 3.0 failed 1 times; aborting job&#xA;Traceback (most recent call last):&#xA;  File &quot;C:/Test.py&quot;, line 25, in &amp;lt;module&amp;gt;&#xA;    newDf = fullDf.rdd.map(lambda x: x.split(&quot;,&quot;)[1]).collect()&#xA;  File &quot;C:\spark-2.1.0\python\lib\pyspark.zip\pyspark\rdd.py&quot;, line 809, in collect&#xA;  File &quot;C:\spark-2.1.0\python\lib\py4j-0.10.4-src.zip\py4j\java_gateway.py&quot;, line 1133, in __call__&#xA;  File &quot;C:\spark-2.1.0\python\lib\pyspark.zip\pyspark\sql\utils.py&quot;, line 63, in deco&#xA;  File &quot;C:\spark-2.1.0\python\lib\py4j-0.10.4-src.zip\py4j\protocol.py&quot;, line 319, in get_return_value&#xA;py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.&#xA;: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 1 times, most recent failure: Lost task 1.0 in stage 3.0 (TID 5, localhost, executor driver): java.io.IOException: Cannot run program &quot;python&quot;: CreateProcess error=2, The system cannot find the file specified&#xA;    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)&#xA;    at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:120)&#xA;    at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:67)&#xA;    at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:116)&#xA;    at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)&#xA;    at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)&#xA;    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)&#xA;    at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)&#xA;    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)&#xA;    at org.apache.spark.scheduler.Task.run(Task.scala:99)&#xA;    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;    at java.lang.Thread.run(Thread.java:748)&#xA;Caused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified&#xA;    at java.lang.ProcessImpl.create(Native Method)&#xA;    at java.lang.ProcessImpl.&amp;lt;init&amp;gt;(ProcessImpl.java:386)&#xA;    at java.lang.ProcessImpl.start(ProcessImpl.java:137)&#xA;    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)&#xA;    ... 13 more&#xA;&#xA;Driver stacktrace:&#xA;    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)&#xA;    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)&#xA;    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)&#xA;    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&#xA;    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)&#xA;    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)&#xA;    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)&#xA;    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)&#xA;    at scala.Option.foreach(Option.scala:257)&#xA;    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)&#xA;    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)&#xA;    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)&#xA;    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)&#xA;    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)&#xA;    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)&#xA;    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)&#xA;    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)&#xA;    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)&#xA;    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)&#xA;    at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)&#xA;    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&#xA;    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)&#xA;    at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)&#xA;    at org.apache.spark.rdd.RDD.collect(RDD.scala:934)&#xA;    at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)&#xA;    at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)&#xA;    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#xA;    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#xA;    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#xA;    at java.lang.reflect.Method.invoke(Method.java:498)&#xA;    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)&#xA;    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&#xA;    at py4j.Gateway.invoke(Gateway.java:280)&#xA;    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#xA;    at py4j.commands.CallCommand.execute(CallCommand.java:79)&#xA;    at py4j.GatewayConnection.run(GatewayConnection.java:214)&#xA;    at java.lang.Thread.run(Thread.java:748)&#xA;Caused by: java.io.IOException: Cannot run program &quot;python&quot;: CreateProcess error=2, The system cannot find the file specified&#xA;    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)&#xA;    at org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:120)&#xA;    at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:67)&#xA;    at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:116)&#xA;    at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:128)&#xA;    at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)&#xA;    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)&#xA;    at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)&#xA;    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)&#xA;    at org.apache.spark.scheduler.Task.run(Task.scala:99)&#xA;    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;    ... 1 more&#xA;Caused by: java.io.IOException: CreateProcess error=2, The system cannot &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="4265823" LastActivityDate="2018-03-09T01:55:35.827" Title="Dataframe.rdd.map().collect Does not work in PySpark" Tags="&lt;python-2.7&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;apache-spark-sql&gt;&lt;spark-dataframe&gt;" AnswerCount="0" CommentCount="7" />
  <row Id="49185464" PostTypeId="1" CreationDate="2018-03-09T01:56:18.353" Score="1" ViewCount="16" Body="&lt;p&gt;I am working on pair RDDs. My aim is to calculate jaccard similarity &#xA;between the set of rdd values and cluster them according to the jaccard similarity threshold value.Structure of my RDD is :  &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val a= [Key,Set(String)]   //Pair RDD&#xA;&#xA;For example:-    &#xA;India,[Country,Place,....]  &#xA;USA,[Country,State,..]  &#xA;Berlin,[City,Popluatedplace,..]   &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;After finding jaccard similarity, I will cluster the similar entities into one cluster. In the above example, India and USA will be cluster into one cluster based on some threshold value whereas Berlin will be in the other cluster.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So I took the Cartesian  product of rdd a  &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val filterOnjoin = a.cartesian(a).filter(f =&amp;gt; &#xA;(!f._1._1.toString().contentEquals(f._2._1.toString()))) &#xA;//Cartesianproduct of rdd a and filtering rows with same key at both &#xA;//the position.&#xA;//e.g. ((India,Set[Country,Place,....]),(USA,Set[Country,State,..])) &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and compare the set of values with the help of jaccard similarity.  &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val Jsim = filterOnjoin.map(f =&amp;gt; (f._1._1, (f._2._1, &#xA;Similarity.sim(f._1._2, f._2._2)))) //calculating jaccard similarity.&#xA;//(India,USA,0.8)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The  code is running fine on smaller dataset. As the size of dataset is increased, Cartesian product is taking too much time. For 100 MB data(size of rdd &quot;a&quot;), its doing data shuffle read around 25 GB. For 3.5 GB data, its in TB.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have gone through various links. Like spark tuning methods and some on stack overflow. But most of the post it is written that broadcast the smaller RDD. But here the size of both the rdd is the same and its big.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Links which I followed :-&lt;br&gt;&#xA;&lt;a href=&quot;https://stackoverflow.com/questions/26557873/spark-produce-rddx-x&quot;&gt;Spark: produce RDD[(X, X)] of all possible combinations from RDD[X]&lt;/a&gt; of-all-possible-combinations-from-rddx&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/30042589/spark-repartition-is-slow-and-shuffles-too-much-data&quot;&gt;Spark repartition is slow and shuffles too much data&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/33200023/map-key-value-pair-based-on-similarity-of-their-value-in-spark&quot;&gt;Map key, value pair based on similarity of their value in Spark&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am new to Spark and Scala. I am unable to think beyond Cartesian product which is bottleneck here. Is it possible to solve this problem without Cartesian product. &lt;/p&gt;&#xA;" OwnerUserId="4030626" LastEditorUserId="4030626" LastEditDate="2018-03-09T02:02:57.027" LastActivityDate="2018-03-09T02:02:57.027" Title="Jaccard Similarity of an RDD with the help of Spark and Scala without Cartesian?" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;cartesian-product&gt;" AnswerCount="0" CommentCount="0" FavoriteCount="1" />
  <row Id="49185503" PostTypeId="1" CreationDate="2018-03-09T02:01:30.303" Score="0" ViewCount="10" Body="&lt;p&gt;We have a Spark code in production.  We also have written a new code which does  exactly the same job.  I've run these 2 codes against a large data input (12TB input).  When we load these in Hive, I can see that if do Sum(colName), and count(colName) against important metrics in the resulting tables, the sum and count are exactly the same; but when I try count(distinct(colName)) for the same metrics, the result is different.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The only explanation that I can come up with is that Spark aggregates the records slightly differently in each run.  So, although the overall count and sum is the same, but values in rows are slightly different.  Like for example in one run you have (10, 5, 5), but in the other run you would have (10, 4, 6).&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So, my question is has anyone compared results of the same Spark job run twice against the same data?  Should in such a situation the result be exactly the same?&lt;/p&gt;&#xA;" OwnerUserId="1888243" LastActivityDate="2018-03-09T02:01:30.303" Title="Can 2 Runs of Spark Against the Same Data Result in Sligtly Different Result" Tags="&lt;apache&gt;&lt;apache-spark&gt;&lt;aggregation&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49185646" PostTypeId="1" CreationDate="2018-03-09T02:21:33.840" Score="0" ViewCount="25" Body="&lt;p&gt;I'm a new learner of spark. There's one line of code estimating pi but I don't quite understand how it works.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;scala&amp;gt;val pi_approx = f&quot;pi = ${355f/113}%.5f&quot;&#xA;pi_approx: String = pi = 3.14159&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I don't understand the 'f' '$' and '%' in the expression above. Could anyone explain the usage of them? Thanks!&lt;/p&gt;&#xA;" OwnerUserId="8393141" LastEditorUserId="5880706" LastEditDate="2018-03-09T02:22:15.480" LastActivityDate="2018-03-09T06:53:25.467" Title="Could anyone explain this spark expression for me?" Tags="&lt;apache-spark&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="49185829" PostTypeId="1" CreationDate="2018-03-09T02:46:48.730" Score="2" ViewCount="18" Body="&lt;p&gt;I'm trying to dynamically add sparklines without success.  I need to add 'sparklike101', 'sparklike201', etc&lt;/p&gt;&#xA;&#xA;&lt;p&gt;ifaceID would be a number.  It just doesn't work.  Not sure if I'm doing it right.  I havce snipet of the code below.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;var addChart=function(ifaceID, ifaceRow) {&#xA;var inn = [1,2,3,4,5,6]&#xA;$('.sparkline'+ifaceID).sparkline(inn,{type: 'line'});                        &#xA;ifaceRow.append('&amp;lt;td&amp;gt;&amp;lt;span class=&quot;sparkline'+ifaceID+'&quot;&amp;gt;Loading...&amp;lt;/span&amp;gt;&amp;lt;/td&amp;gt;');&#xA;    }&#xA;    })}&#xA;for (var i in  msg.d.Rows) { var ifaceID = msg.d.Rows[i][0]; var ifaceRow = $('&amp;lt;tr&amp;gt;&amp;lt;/tr&amp;gt;'); table.append(ifaceRow); addChart(ifaceID, ifaceRow);  }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2092518" LastEditorUserId="2092518" LastEditDate="2018-03-09T02:52:21.717" LastActivityDate="2018-03-09T02:52:21.717" Title="Trying to get dynamic sparkline" Tags="&lt;jquery&gt;&lt;sparklines&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49185913" PostTypeId="1" CreationDate="2018-03-09T02:58:38.477" Score="0" ViewCount="25" Body="&lt;p&gt;first,my code as flow:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark import SparkContext&#xA;from pyspark import SparkConf&#xA;from pyspark.sql import SQLContext&#xA;conf = SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local&quot;)&#xA;sc = SparkContext(conf=conf)&#xA;sqlContext = SQLContext(sc)&#xA;df = sqlContext.read.csv(&quot;E://wine.csv&quot;,header=True)&#xA;df.show(10)&#xA;df.na.replace(10,1.7).show()&#xA;df1 = sqlContext.createDataFrame([&#xA;    (10.0,1.7,1),&#xA;    ],['Al','Ma','class'])&#xA;df1.show()&#xA;df1.na.replace(10,1.7).show()&#xA;sc.stop()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;in df1 it can be replaced,but in df it is invalid.&#xA;pic as flow:&#xA;df &#xA;&lt;a href=&quot;https://i.stack.imgur.com/coqVL.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;enter image description here&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;df1&#xA;&lt;a href=&quot;https://i.stack.imgur.com/MTDPl.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;enter image description here&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="9465323" LastActivityDate="2018-03-09T02:58:38.477" Title="why &quot;replace&quot;（a ways in pyspark）is invalid in DataFrame?" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;dataframe&gt;&lt;pyspark&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49186322" PostTypeId="1" AcceptedAnswerId="49186768" CreationDate="2018-03-09T03:49:01.930" Score="0" ViewCount="14" Body="&lt;p&gt;Let's say I have this line, I want to know if Spark automatically creates a folder path and writes to folder like it does in local. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Yes, s3 is not folder system rather a key val system.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val path=&quot;s3a://dev-us-east-1/&quot;&#xA;&#xA;val op = df_formatted.coalesce(1).write.mode(&quot;overwrite&quot;).format(&quot;csv&quot;).save(path + &quot;report/output&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Will this be written to &quot;s3a://dev-us-east-1/report/output&quot;&lt;/p&gt;&#xA;" OwnerUserId="5808291" LastEditorUserId="720977" LastEditDate="2018-03-09T06:32:14.227" LastActivityDate="2018-03-09T06:32:14.227" Title="Will Spark create a s3 folder path if it doesn't exist?" Tags="&lt;apache-spark&gt;&lt;amazon-s3&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49186659" PostTypeId="1" AcceptedAnswerId="49187274" CreationDate="2018-03-09T04:36:37.013" Score="0" ViewCount="16" Body="&lt;p&gt;I am trying to rename the S3 files which basically &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;copy to  target + delete source&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;But in my case I am able to copy target but not able to delete source properly .&#xA;All directory structure remains same without any file ..&#xA;also it creates temp files in the main directory .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Do I have to explicitly delete it after renaming ?&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my code which renames the files &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have subfolders insode the folder.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val file = fs.globStatus(new Path(outputFileURL + &quot;/*/*&quot;))&#xA;for (urlStatus &amp;lt;- file) {&#xA;&#xA;val DataPartitionName = urlStatus.getPath.toString.split(&quot;=&quot;)(1).split(&quot;\\/&quot;)(0).toString            &#xA;val finalFileName = finalPrefix + DataPartitionName + &quot;.&quot;  + intFileCounter + &quot;.&quot; + fileVersion + currentTime + fileExtention&#xA;val dest = new Path(mainFileURL + &quot;/&quot; + finalFileName)&#xA;&#xA;fs.rename(urlStatus.getPath, dest)&#xA;&#xA;intFileCounter += 1&#xA;&#xA; }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="9175971" LastActivityDate="2018-03-09T05:46:30.013" Title="Renaming S3 files creates temp folders" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;amazon-s3&gt;&lt;hdfs&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49186960" PostTypeId="1" CreationDate="2018-03-09T05:10:30.450" Score="0" ViewCount="13" Body="&lt;p&gt;We are working on an scenario for realtime analytics. What we are doing is loading transaction and log data into HDFS using Spark Streaming.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Configured Solr with HDFS.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now I want to show the transaction and log data within single dashboard of Lucidworks Banana.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;While creating the dashboard in Banana, we are able to give only a single collection name.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So how to create a collection for the transaction and log data? And how to visualize those in a single dashboard of Banana.&lt;/p&gt;&#xA;" OwnerUserId="9390875" LastEditorUserId="3867574" LastEditDate="2018-03-09T14:51:16.327" LastActivityDate="2018-03-09T14:51:16.327" Title="HDFS to Solr, then visualizing data in Banana" Tags="&lt;solr&gt;&lt;hdfs&gt;&lt;spark-streaming&gt;&lt;banana&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49187101" PostTypeId="1" CreationDate="2018-03-09T05:23:46.220" Score="0" ViewCount="26" Body="&lt;p&gt;When I run this code via sqoop command, it works&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sqoop import --connect &quot;jdbc:sqlserver://myhost:port;databaseName=DBNAME&quot; \&#xA; --username MYUSER -P \&#xA; --compress --compression-codec snappy \&#xA; --as-parquetfile \&#xA; --table MYTABLE \&#xA; --warehouse-dir /user/myuser/test1/ \&#xA; --m 1&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then I create spark scala code as below. But when I execute the project using spark-submit, it not working &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val sqoop_options: SqoopOptions = new SqoopOptions()&#xA;sqoop_options.setConnectString(&quot;jdbc:jdbc:sqlserver://myhost:port;databaseName=DBNAME&quot;)&#xA;sqoop_options.setTableName(&quot;MYTABLE&quot;);&#xA;sqoop_options.setUsername(&quot;MYUSER&quot;);&#xA;sqoop_options.setPassword(&quot;password&quot;);&#xA;sqoop_options.setNumMappers(1);&#xA;sqoop_options.setTargetDir(&quot;/user/myuser/test1/&quot;);&#xA;sqoop_options.setFileLayout(FileLayout.ParquetFile);&#xA;sqoop_options.setCompressionCodec(&quot;org.apache.hadoop.io.compress.SnappyCodec&quot;)&#xA;val importTool = new ImportTool&#xA;val sqoop = new Sqoop(importTool, conf, sqoop_options);&#xA;val retCode = ToolRunner.run(sqoop, null);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It return driver not found error, even though I run it on the same cluster.&#xA;I already put appropriate library on /var/lib/sqoop directory, that's why sqoop command run well. But, is it will refer to another library path when I run it via spark-submit?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Detail error log:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; /opt/cloudera/parcels/CDH-5.8.2-1.cdh5.8.2.p0.3/lib/spark/conf/spark-env.sh: line 75: spark.driver.extraClassPath=.:/etc/hbase/conf:/opt/cloudera/parcels/CDH/lib/hbase/hbase-common.jar:/opt/cloudera/parcels/CDH/lib/hbase/hbase-client.jar://opt/cloudera/parcels/CDH/lib/hbase/hbase-server.jar:/opt/cloudera/parcels/CDH/lib/hbase/hbase-protocol.jar:/opt/cloudera/parcels/CDH/lib/hbase/lib/guava-12.0.1.jar:/opt/cloudera/parcels/CDH/lib/hbase/lib/htrace-core.jar:/opt/cloudera/parcels/CDH/lib/hbase/lib/zookeeper.jar:/opt/cloudera/parcels/CDH/lib/hbase/lib/protobuf-java-2.5.0.jar:/opt/cloudera/parcels/CDH/lib/hbase/hbase-hadoop2-compat.jar:/opt/cloudera/parcels/CDH/lib/hbase/hbase-hadoop-compat.jar:/opt/cloudera/parcels/CDH/lib/hbase/lib/metrics-core-2.2.0.jar:/opt/cloudera/parcels/CDH/lib/hbase/hbase-spark.jar:/opt/cloudera/parcels/CDH/lib/hive/lib/hive-hbase-handler.jar: No such file or directory&#xA;/opt/cloudera/parcels/CDH-5.8.2-1.cdh5.8.2.p0.3/lib/spark/conf/spark-env.sh: line 77: spark.executor.extraClassPath=.:/opt/cloudera/parcels/CDH/lib/hbase/hbase-common.jar:/opt/cloudera/parcels/CDH/lib/hbase/hbase-client.jar://opt/cloudera/parcels/CDH/lib/hbase/hbase-server.jar:/opt/cloudera/parcels/CDH/lib/hbase/hbase-protocol.jar:/opt/cloudera/parcels/CDH/lib/hbase/lib/guava-12.0.1.jar:/opt/cloudera/parcels/CDH/lib/hbase/lib/htrace-core.jar:/opt/cloudera/parcels/CDH/lib/hbase/lib/zookeeper.jar:/opt/cloudera/parcels/CDH/lib/hbase/lib/protobuf-java-2.5.0.jar:/opt/cloudera/parcels/CDH/lib/hbase/hbase-hadoop2-compat.jar:/opt/cloudera/parcels/CDH/lib/hbase/hbase-hadoop-compat.jar:/opt/cloudera/parcels/CDH/lib/hbase/lib/metrics-core-2.2.0.jar:/opt/cloudera/parcels/CDH/lib/hbase/hbase-spark.jar:/opt/cloudera/parcels/CDH/lib/hive/lib/hive-hbase-handler.jar: No such file or directory&#xA;2018-03-09 13:59:37,332 INFO  [main] security.UserGroupInformation: Login successful for user myuser using keytab file myuser.keytab&#xA;2018-03-09 13:59:37,371 INFO  [main] sqoop.Sqoop: Running Sqoop version: 1.4.6&#xA;2018-03-09 13:59:37,426 WARN  [main] sqoop.ConnFactory: $SQOOP_CONF_DIR has not been set in the environment. Cannot check for additional configuration.&#xA;2018-03-09 13:59:37,478 INFO  [main] manager.SqlManager: Using default fetchSize of 1000&#xA;2018-03-09 13:59:37,479 INFO  [main] tool.CodeGenTool: Beginning code generation&#xA;2018-03-09 13:59:37,479 INFO  [main] tool.CodeGenTool: Will generate java class as codegen_MYTABLE&#xA;Exception in thread &quot;main&quot; java.lang.RuntimeException: Could not load db driver class: com.microsoft.sqlserver.jdbc.SQLServerDriver&#xA;        at org.apache.sqoop.manager.SqlManager.makeConnection(SqlManager.java:856)&#xA;        at org.apache.sqoop.manager.GenericJdbcManager.getConnection(GenericJdbcManager.java:52)&#xA;        at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:744)&#xA;        at org.apache.sqoop.manager.SqlManager.execute(SqlManager.java:767)&#xA;        at org.apache.sqoop.manager.SqlManager.getColumnInfoForRawQuery(SqlManager.java:270)&#xA;        at org.apache.sqoop.manager.SqlManager.getColumnTypesForRawQuery(SqlManager.java:241)&#xA;        at org.apache.sqoop.manager.SqlManager.getColumnTypes(SqlManager.java:227)&#xA;        at org.apache.sqoop.manager.ConnManager.getColumnTypes(ConnManager.java:295)&#xA;        at org.apache.sqoop.orm.ClassWriter.getColumnTypes(ClassWriter.java:1833)&#xA;        at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1645)&#xA;        at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:107)&#xA;        at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:478)&#xA;        at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:605)&#xA;        at org.apache.sqoop.Sqoop.run(Sqoop.java:143)&#xA;        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)&#xA;        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)&#xA;        at com.test.spark.sqoop.SqoopExample$.importSQLToHDFS(SqoopExample.scala:56)&#xA;        at com.test.spark.sqoop.SqoopExample$.main(SqoopExample.scala:18)&#xA;        at com.test.spark.sqoop.SqoopExample.main(SqoopExample.scala)&#xA;        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#xA;        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#xA;        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#xA;        at java.lang.reflect.Method.invoke(Method.java:498)&#xA;        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)&#xA;        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)&#xA;        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)&#xA;        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)&#xA;        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="968211" LastEditorUserId="968211" LastEditDate="2018-03-09T07:15:03.733" LastActivityDate="2018-03-09T10:26:43.850" Title="Sqoop Error while run via Spark" Tags="&lt;apache-spark&gt;&lt;cloudera&gt;&lt;sqoop&gt;&lt;apache-sqoop&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="49187416" PostTypeId="1" CreationDate="2018-03-09T05:52:28.327" Score="1" ViewCount="24" Body="&lt;p&gt;I have two kinds of tasks in spark : A and B&#xA;In spark.scheduler.pool, I have two pools: APool and BPool.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want task A to be executed aways in APool while B is in BPool.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The resources in APool is preserved to A. &#xA;Because task B may take too much resources to execute. Every time when B is executing, A needs to wait. I want no matter when the task is submitted, there will always be some resource for A to execute.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;I am using spark with java in standalone mode. I submit the job like javaRDD.map(..).reduce... The javaRDD is a sub-clesse extended form JavaRDD. Task A and B have different RDD class like ARDD and BRDD. They run in the same spark application.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The procedure is like: The app start up -&gt; spark application created, but no job runs -&gt; I click &quot;run A&quot; on the app ui, then ARDD will run. -&gt; I click &quot;run B&quot; on the app ui, then BRDD will run in the same spark application as A.&lt;/p&gt;&#xA;" OwnerUserId="7043615" LastEditorUserId="7043615" LastEditDate="2018-03-09T06:05:17.083" LastActivityDate="2018-03-09T06:05:17.083" Title="How to reserve resource for a certain task in spark?" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;yarn&gt;&lt;mesos&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49187647" PostTypeId="1" CreationDate="2018-03-09T06:13:31.990" Score="-1" ViewCount="20" Body="&lt;p&gt;How can I achieve data transformations like these in spark.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Input:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;startTime, Entity, Index &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;T1, E1, I1&lt;/li&gt;&#xA;&lt;li&gt;T1, E1, I2&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;T1, E1, I3&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;T2, E1, I2&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;T2, E1, I3&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;T2, E1, I4&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;T3, E1, I3&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;T3, E1, I4&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;T3, E1, I5&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;T4, E2, I6&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;output:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;startTime, Entity, Index, endTime&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;p&gt;T4, E2, I6, null&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;T3, E1, I5, null&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;T3, E1, I4, null&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;T3, E1, I3, null&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;T2, E1, I4, null&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;T2, E1, I3, null&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;T2, E1, I2, T3&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;T1, E1, I3, null&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;T1, E1, I2, T3&lt;/li&gt;&#xA;&lt;li&gt;T1, E1, I1, T2&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Logic:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please look at entity E2 - It has only one value I6 which does not have any overlap. -&gt; Hence for entity E2, I6 is active and hence no endTime.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;==== T3 ====&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now, Going back to entity E1 -&gt; Index I3, I4 and I5 are active since they correspond to max(timestamp) for that specific entity (E1)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hence all of them have endTime as null.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;T3, E1, I5, null&lt;/li&gt;&#xA;&lt;li&gt;T3, E1, I4, null&lt;/li&gt;&#xA;&lt;li&gt;T3, E1, I3, null&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;==== T2 ====&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Since, I3 and I4 are already present in T3 and the endTime is computed we need to keep same end time which is null.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;T2, E1, I4, null&lt;/li&gt;&#xA;&lt;li&gt;T2, E1, I3, null&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Index I2 is not present in T3, So index I2 was not active and it got expired at T3&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;T2, E1, I2, T3&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;==== T1 ====&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Index I2 and I3 are already present in T2 and hence use the same endTime.&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;T1, E1, I3, null&lt;/li&gt;&#xA;&lt;li&gt;T1, E1, I2, T3&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;Index I1 is not present in T2, so index I1 was not active and it got expired at T2&lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;T1, E1, I1, T2&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;I'm new to spark. Can any of you please help?&lt;/p&gt;&#xA;" OwnerUserId="3802293" LastEditorUserId="3802293" LastEditDate="2018-03-09T08:05:14.953" LastActivityDate="2018-03-09T08:05:14.953" Title="Spark Scala - Data Processing - Computing on previous values" Tags="&lt;scala&gt;&lt;spark-dataframe&gt;" AnswerCount="0" CommentCount="4" FavoriteCount="1" />
  <row Id="49187933" PostTypeId="1" CreationDate="2018-03-09T06:38:45.203" Score="0" ViewCount="14" Body="&lt;p&gt;I have a spark project using Janusgraph that I am trying to making a fat jar of. The projects runs well in IntelliJ but when I try to make a fat jar and build it and run it with spark-submit, it throws this error:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;NoSuchMethodError:com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am creating a fat jar using &lt;code&gt;sbt clean compile&lt;/code&gt; and &lt;code&gt;sbt assembly&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also unzipped the jar created and looked for the Stopwatch.class and the method mentioned. They seem to be present there. So, I don't understand why this error would come up.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My &lt;code&gt;build.sbt&lt;/code&gt; file is as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;name := &quot;proj1&quot;&#xA;&#xA;version := &quot;0.1&quot;&#xA;&#xA;scalaVersion := &quot;2.11.12&quot;&#xA;&#xA;&#xA;// https://mvnrepository.com/artifact/com.michaelpollmeier/gremlin-scala&#xA;libraryDependencies += &quot;com.michaelpollmeier&quot; %% &quot;gremlin-scala&quot; % &quot;3.3.1.1&quot;&#xA;// https://mvnrepository.com/artifact/org.janusgraph/janusgraph-core&#xA;libraryDependencies += &quot;org.janusgraph&quot; % &quot;janusgraph-core&quot; % &quot;0.2.0&quot;&#xA;&#xA;// https://mvnrepository.com/artifact/org.apache.spark/spark-core&#xA;libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % &quot;2.3.0&quot;&#xA;// https://mvnrepository.com/artifact/org.apache.spark/spark-sql&#xA;libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-sql&quot; % &quot;2.3.0&quot;&#xA;// https://mvnrepository.com/artifact/org.apache.spark/spark-mllib&#xA;libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-mllib&quot; % &quot;2.3.0&quot;&#xA;// https://mvnrepository.com/artifact/org.apache.spark/spark-hive&#xA;libraryDependencies += &quot;org.apache.spark&quot; %% &quot;spark-hive&quot; % &quot;2.3.0&quot;&#xA;&#xA;//libraryDependencies += &quot;com.google.guava&quot; %% &quot;guava&quot; % &quot;15.0&quot;&#xA;//libraryDependencies += &quot;org.apache.hadoop&quot; %% &quot;hadoop-client&quot; % &quot;2.7.2&quot;&#xA;// https://mvnrepository.com/artifact/org.janusgraph/janusgraph-es&#xA;libraryDependencies += &quot;org.janusgraph&quot; % &quot;janusgraph-es&quot; % &quot;0.2.0&quot;&#xA;&#xA;// https://mvnrepository.com/artifact/org.janusgraph/janusgraph-cassandra&#xA;libraryDependencies += &quot;org.janusgraph&quot; % &quot;janusgraph-cassandra&quot; % &quot;0.2.0&quot;&#xA;&#xA;// https://mvnrepository.com/artifact/com.google.guava/guava&#xA;libraryDependencies += &quot;com.google.guava&quot; % &quot;guava&quot; % &quot;24.0-jre&quot;&#xA;&#xA;// https://mvnrepository.com/artifact/commons-io/commons-io&#xA;libraryDependencies += &quot;commons-io&quot; % &quot;commons-io&quot; % &quot;2.6&quot;&#xA;&#xA;&#xA;&#xA;assemblyMergeStrategy in assembly := {&#xA;  case PathList(&quot;META-INF&quot;, xs @ _*) =&amp;gt; MergeStrategy.discard&#xA;  case x =&amp;gt; MergeStrategy.first&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Any help is appreciated. Thanks in advance!&lt;/p&gt;&#xA;" OwnerUserId="6421767" LastActivityDate="2018-03-09T06:38:45.203" Title="Error while building sbt project" Tags="&lt;apache-spark&gt;&lt;sbt&gt;&lt;executable-jar&gt;&lt;sbt-assembly&gt;&lt;janusgraph&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49188040" PostTypeId="1" AcceptedAnswerId="49188122" CreationDate="2018-03-09T06:46:27.617" Score="0" ViewCount="29" Body="&lt;p&gt;I have a use case where I want to use a value from another dataset. For example:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Table 1: Items&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Name | Price&#xA;------------&#xA;Apple |10&#xA;&#xA;Mango| 20&#xA;&#xA;Grape |30&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Table 2 : Item_Quantity&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Name | Quantity&#xA;Apple |5&#xA;Mango| 2&#xA;Grape |2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to calculate total cost and prepare a final dataset.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Cost&#xA;Name | Cost&#xA;Apple |50  (10*5)&#xA;Mango| 40  (20*2)&#xA;Grape |60   (30*2)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How can I achieve this in spark? Appreciate your help.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;===================&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Another use case:&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;Need help with this one too..&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Table 1: Items&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Name | Code | Quantity&#xA;-------------------&#xA;Apple-1 |APP | 10&#xA;Mango-1| MAN | 20&#xA;Grape-1|GRA | 30&#xA;Apple-2 |APP | 20&#xA;Mango-2| MAN | 30&#xA;Grape -2|GRA | 50&#xA;&#xA;&#xA;Table 2 : Item_CODE_Price&#xA;&#xA;Code | Price&#xA;----------------&#xA;APP |5&#xA;MAN| 2&#xA;GRA |2&#xA;&#xA;I want to calculate total cost using code to get the price and prepare a final dataset.&#xA;&#xA;Cost&#xA;Name | Cost&#xA;--------------&#xA;Apple-1 |50  (10*5)&#xA;Mango-1| 40  (20*2)&#xA;Grape-1 |60   (30*2)&#xA;Apple-2 |100  (20*5)&#xA;Mango-2| 60  (30*2)&#xA;Grape-2 |100   (50*2)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="2034519" LastEditorUserId="2034519" LastEditDate="2018-03-09T07:08:28.123" LastActivityDate="2018-03-09T07:22:38.590" Title="Accessing mutiple datasets in spark" Tags="&lt;java&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49188415" PostTypeId="1" CreationDate="2018-03-09T07:14:41.783" Score="0" ViewCount="26" Body="&lt;p&gt;I have a use case to minus two dataframes . So i have used the dataframe except() method.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is working fine locally on smaller set of data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;But when I ran over AWS S3 bucket ,the except() method is not making minus as expected . Is there anything needs to be taken care on distributed environment ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Does anyone faced this similar issue ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my sample code&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val values = List(List(&quot;One&quot;, &quot;2017-07-01T23:59:59.000&quot;, &quot;2017-11-04T23:59:58.000&quot;, &quot;A&quot;, &quot;Yes&quot;) &#xA;  , List(&quot;Two&quot;, &quot;2017-07-01T23:59:59.000&quot;, &quot;2017-11-04T23:59:58.000&quot;, &quot;X&quot;, &quot;No&quot;) &#xA;  , List(&quot;Three&quot;, &quot;2017-07-09T23:59:59.000&quot;, &quot;2017-12-05T23:59:58.000&quot;, &quot;M&quot;, &quot;Yes&quot;) &#xA;  , List(&quot;Four&quot;, &quot;2017-11-01T23:59:59.000&quot;, &quot;2017-12-09T23:59:58.000&quot;, &quot;A&quot;, &quot;No&quot;) &#xA;  , List(&quot;Five&quot;, &quot;2017-07-09T23:59:59.000&quot;, &quot;2017-12-05T23:59:58.000&quot;, &quot;&quot;, &quot;No&quot;) &#xA;  ,List(&quot;One&quot;, &quot;2017-07-01T23:59:59.000&quot;, &quot;2017-11-04T23:59:58.000&quot;, &quot;&quot;, &quot;No&quot;)&#xA;)&#xA;  .map(row =&amp;gt; (row(0), row(1), row(2), row(3), row(4)))&#xA;&#xA;val spark = SparkSession.builder().master(&quot;local&quot;).getOrCreate()&#xA;&#xA;import spark.implicits._&#xA;&#xA;val df = values.toDF(&quot;KEY&quot;, &quot;ROW_START_DATE&quot;, &quot;ROW_END_DATE&quot;, &quot;CODE&quot;, &quot;Indicator&quot;)&#xA;&#xA;val filterCond = (col(&quot;ROW_START_DATE&quot;) &amp;lt;= &quot;2017-10-31T23:59:59.999&quot; &amp;amp;&amp;amp; col(&quot;ROW_END_DATE&quot;) &amp;gt;= &quot;2017-10-31T23:59:59.999&quot; &amp;amp;&amp;amp; col(&quot;CODE&quot;).isin(&quot;M&quot;, &quot;A&quot;, &quot;R&quot;, &quot;G&quot;))&#xA;&#xA;&#xA;val Filtered = df.filter(filterCond)&#xA;val Excluded = df.except(df.filter(filterCond))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Expected Output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df.show(false)&#xA;Filtered.show(false)&#xA;Excluded.show(false)&#xA;+-----+-----------------------+-----------------------+----+---------+&#xA;|KEY  |ROW_START_DATE         |ROW_END_DATE           |CODE|Indicator|&#xA;+-----+-----------------------+-----------------------+----+---------+&#xA;|One  |2017-07-01T23:59:59.000|2017-11-04T23:59:58.000|A   |Yes      |&#xA;|Two  |2017-07-01T23:59:59.000|2017-11-04T23:59:58.000|X   |No       |&#xA;|Three|2017-07-09T23:59:59.000|2017-12-05T23:59:58.000|M   |Yes      |&#xA;|Four |2017-11-01T23:59:59.000|2017-12-09T23:59:58.000|A   |No       |&#xA;|Five |2017-07-09T23:59:59.000|2017-12-05T23:59:58.000|    |No       |&#xA;|One  |2017-07-01T23:59:59.000|2017-11-04T23:59:58.000|    |No       |&#xA;+-----+-----------------------+-----------------------+----+---------+&#xA;+-----+-----------------------+-----------------------+----+---------+&#xA;|KEY  |ROW_START_DATE         |ROW_END_DATE           |CODE|Indicator|&#xA;+-----+-----------------------+-----------------------+----+---------+&#xA;|One  |2017-07-01T23:59:59.000|2017-11-04T23:59:58.000|A   |Yes      |&#xA;|Three|2017-07-09T23:59:59.000|2017-12-05T23:59:58.000|M   |Yes      |&#xA;+-----+-----------------------+-----------------------+----+---------+&#xA;+----+-----------------------+-----------------------+----+---------+&#xA;|KEY |ROW_START_DATE         |ROW_END_DATE           |CODE|Indicator|&#xA;+----+-----------------------+-----------------------+----+---------+&#xA;|Four|2017-11-01T23:59:59.000|2017-12-09T23:59:58.000|A   |No       |&#xA;|Two |2017-07-01T23:59:59.000|2017-11-04T23:59:58.000|X   |No       |&#xA;|Five|2017-07-09T23:59:59.000|2017-12-05T23:59:58.000|    |No       |&#xA;|One |2017-07-01T23:59:59.000|2017-11-04T23:59:58.000|    |No       |&#xA;+----+-----------------------+-----------------------+----+---------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But Getting something like below when ran over S3 bucket&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Filtered.show(false)&#xA;+-----+-----------------------+-----------------------+----+---------+&#xA;|KEY  |ROW_START_DATE         |ROW_END_DATE           |CODE|Indicator|&#xA;+-----+-----------------------+-----------------------+----+---------+&#xA;|One  |2017-07-01T23:59:59.000|2017-11-04T23:59:58.000|A   |Yes      |&#xA;|Three|2017-07-09T23:59:59.000|2017-12-05T23:59:58.000|M   |Yes      |&#xA;+-----+-----------------------+-----------------------+----+---------+&#xA;&#xA;Excluded.show(false)&#xA;&#xA;+----+-----------------------+-----------------------+----+---------+&#xA;|KEY |ROW_START_DATE         |ROW_END_DATE           |CODE|Indicator|&#xA;+----+-----------------------+-----------------------+----+---------+&#xA;|One |2017-07-01T23:59:59.000|2017-11-04T23:59:58.000|A   |Yes      |---&amp;gt; wrong&#xA;|Four|2017-11-01T23:59:59.000|2017-12-09T23:59:58.000|A   |No       |&#xA;|Two |2017-07-01T23:59:59.000|2017-11-04T23:59:58.000|X   |No       |&#xA;|Five|2017-07-09T23:59:59.000|2017-12-05T23:59:58.000|    |No       |&#xA;|One |2017-07-01T23:59:59.000|2017-11-04T23:59:58.000|    |No       |&#xA;+----+-----------------------+-----------------------+----+---------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there any other way to perform minus of two spark dataframe ?&lt;/p&gt;&#xA;" OwnerUserId="7373869" LastEditorUserId="7373869" LastEditDate="2018-03-09T07:33:09.383" LastActivityDate="2018-03-09T11:30:51.307" Title="Spark Dataframe except method Issue" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="8" />
  <row Id="49188768" PostTypeId="1" CreationDate="2018-03-09T07:36:54.847" Score="1" ViewCount="14" Body="&lt;p&gt;I am new to Machine learning. Just started with new project for a POC. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have my dataset for model as a csv file in the below format.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My goal is to deduct the anomaly for any src/destination when there is a spike in in/out respectively. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;timestamp src destination in  out  type&#xA; 00       p1    p2        2    3    abc&#xA; 00       p2    p3        1    4    abc&#xA; 00       p3    pn        3    5    abc&#xA; 05       p1    pn        4    3    abc&#xA; 05       p2    p1        2    2    abc&#xA; 10       p1    p3       91    6    abc &amp;lt;- src anomaly deduction&#xA; 10       px    py       30   92    abc &amp;lt;- dest anomay deduction&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here src maps to in. destination maps to out value.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to prepare a Kmean model for each src / destinaton with in and out in Spark using pyspark. I have no idea on what to do? How to prepare a model. Can you give me some pointer on this?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks.     &lt;/p&gt;&#xA;" OwnerUserId="1557637" LastEditorUserId="1557637" LastEditDate="2018-03-09T07:42:58.667" LastActivityDate="2018-03-09T07:42:58.667" Title="Machine learning kmeans model in Spark" Tags="&lt;apache-spark&gt;&lt;machine-learning&gt;&lt;pyspark&gt;&lt;k-means&gt;&lt;apache-spark-mllib&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49188822" PostTypeId="1" CreationDate="2018-03-09T07:41:02.653" Score="1" ViewCount="15" Body="&lt;p&gt;kafka is Producing data where as Spark is consuming data. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;JavaStreamingContext ssc = new JavaStreamingContext(sc, new Duration(1000));&#xA;Map&amp;lt;String, String&amp;gt; kafkaParams = new HashMap&amp;lt;&amp;gt;();&#xA;kafkaParams.put(&quot;metadata.broker.list&quot;, &quot;localhost:9092&quot;);&#xA;Set&amp;lt;String&amp;gt; topics = Collections.singleton(&quot;mytopic&quot;);&#xA;JavaPairInputDStream&amp;lt;String, String&amp;gt; directKafkaStream = KafkaUtils.createDirectStream(ssc,&#xA;                String.class, String.class, StringDecoder.class, StringDecoder.class, kafkaParams, topics);&#xA;JSONArray json = new JSONArray();&#xA;&#xA;directKafkaStream.foreachRDD(rdd -&amp;gt; {&#xA;    rdd.foreach(record -&amp;gt; json.put(record._2));&#xA;});&#xA;System.out.println(json.length()); //printing length as zero&#xA;ssc.start();&#xA;ssc.awaitTermination();&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I'm not able to put the values into JSONArray.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    Caused by: java.io.NotSerializableException: org.json.JSONArray&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="8013662" LastActivityDate="2018-03-09T07:41:02.653" Title="How to put spark streaming data from kafka to JSONArray" Tags="&lt;java&gt;&lt;apache-spark&gt;&lt;apache-kafka&gt;&lt;spark-streaming&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="49188977" PostTypeId="1" AcceptedAnswerId="49189024" CreationDate="2018-03-09T07:50:14.030" Score="0" ViewCount="19" Body="&lt;p&gt;&lt;strong&gt;Short version:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;How to preview a column in the pyspark shell? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have an object &lt;code&gt;a&lt;/code&gt; of the type &lt;code&gt;pyspark.sql.column.Column&lt;/code&gt; and when I do &lt;code&gt;a.show()&lt;/code&gt;, I get &lt;code&gt;TypeError: 'Column' object is not callable&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Wondering if there's a pyspark equivalent of pandas &lt;a href=&quot;https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.to_frame.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;to_frame()&lt;/a&gt; functionality so that I can use &lt;code&gt;show()&lt;/code&gt; after converting the column to a pyspark dataframe.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Long version:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have the following data set as &lt;code&gt;df&lt;/code&gt;:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+----------+-------------------+--------+&#xA;|    number|          p_efc_dtm|cus_type|&#xA;+----------+-------------------+--------+&#xA;|0000000000|2010-05-25 00:05:10|       1|&#xA;|0015195163|2013-01-03 19:01:10|       1|&#xA;|0018283269|2006-10-01 00:10:09|       1|&#xA;|0018988813|2012-11-29 17:11:45|       1|&#xA;|0020095510|2012-03-09 09:03:44|       1|&#xA;|0023688381|2008-03-13 00:03:51|       1|&#xA;|0024651256|2009-09-22 00:09:09|       1|&#xA;|0025647711|2015-01-13 18:01:27|       1|&#xA;+----------+--------------------+--------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The following is done on the &lt;code&gt;df&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark.sql.window import Window&#xA;import pyspark.sql.functions as F&#xA;&#xA;ws = Window.partitionBy(df['number']).orderBy(df['p_efc_dtm'].desc())&#xA;&#xA;a = F.max(df['p_efc_dtm']).over(ws)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How to see the contents of &lt;code&gt;a&lt;/code&gt;?&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;I'm on Spark 1.6.0 atm.&lt;/p&gt;&#xA;" OwnerUserId="5864582" LastEditorUserId="5864582" LastEditDate="2018-03-09T08:47:10.157" LastActivityDate="2018-03-09T08:47:10.157" Title="Preview a column in pyspark shell" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49189075" PostTypeId="1" CreationDate="2018-03-09T07:56:52.693" Score="0" ViewCount="42" Body="&lt;p&gt;I am new to using spark and python&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have the following tuple&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#Money in each account&#xA;[(key), {(4.32,32.32,23.3),(3.23,32.2,21.3)}]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What I have to do, is to subtract from each element for example (4.32,32.32,23.3) the mean corresponding to each account, to 4.32 - 3.23, to 32.32 - 32.32 ... &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My idea is to do the following.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;subtract = myRDD.reduceByKey(lambda x,y: ((x[0][0]-y[1][0]), (x[0][1]-y[1][1]), (x[0][2]-y[1][2])))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But I don't know how can I enter the index properly.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;My input file has the following structure.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Index,Person,Account1,Account2,Account3&#xA;0,Serge,5.958191,0.6880646,8.135345&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Then I calculate the mean for each row, so for example to all the Keys with name Serge I have the following data tuple.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{('Serge'),((5.958191,0.6880646,8.135345),(3.23,32.2,21.3))}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now what I want is to subtract to each element its mean...&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So for example in this case, I want a tuple with the following structure.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{('Serge'),((5.958191-3.23,0.6880646-32.2,8.135345-21.3, 10))}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My code;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;data = sc.textFile(&quot;myFile.csv&quot;)&#xA;&#xA;data1 = data.map(lambda x: ((x.split(&quot;,&quot;)[1]),(x.split(&quot;,&quot;)[2],x.split(&quot;,&quot;)[3],x.split(&quot;,&quot;)[4]))&#xA;&#xA;mean = data.mapValues( lambda x: (x,1) ) \&#xA;     .reduceByKey (lambda x , y: ((x[0][0] + y[0][0], x[0][1] + y[0][1], x[0][2] + y[0][2]), x[1] + y[1])).mapValues (lambda a: (a[0][0]/a[1],a[0][1]/a[1],a[0][2]/a[1], a[1]))&#xA;&#xA;data2 = mean.join(data1)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Last a[1] from mapValues is to get my items count.&lt;/strong&gt;&lt;/p&gt;&#xA;" OwnerUserId="9466316" LastEditorUserId="9466316" LastEditDate="2018-03-09T09:33:15.393" LastActivityDate="2018-03-09T17:49:49.490" Title="Subtract tuple element to other tuple - Pyspark" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="9" />
  <row Id="49189097" PostTypeId="1" CreationDate="2018-03-09T07:58:43.807" Score="0" ViewCount="18" Body="&lt;p&gt;This is the sample example code in my book:    &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark import SparkConf, SparkContext&#xA;&#xA;conf = SparkConf().setMaster(&quot;spark://chetan-ThinkPad- &#xA;E470:7077&quot;).setAppName(&quot;FlatMap&quot;)&#xA;sc = SparkContext(conf=conf)&#xA;&#xA;numbersRDD = sc.parallelize([1, 2, 3, 4])&#xA;actionRDD = numbersRDD.flatMap(lambda x: x + x).collect()&#xA;for values in actionRDD:&#xA;    print(values)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am getting this error:&#xA;    TypeError: 'int' object is not iterable&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)&#xA;    at org.apache.spark.api.python.PythonRunner$$anon$1.&amp;lt;init&amp;gt;(PythonRDD.scala:234)&#xA;    at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)&#xA;    at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)&#xA;    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)&#xA;    at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)&#xA;    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)&#xA;    at org.apache.spark.scheduler.Task.run(Task.scala:99)&#xA;    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)&#xA;    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)&#xA;    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)&#xA;    ... 1 more&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="4599571" LastActivityDate="2018-03-09T08:03:51.847" Title="pyspark flatmat error: TypeError: 'int' object is not iterable" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;&lt;python-3.5&gt;&lt;flatmap&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49189127" PostTypeId="1" CreationDate="2018-03-09T08:00:53.313" Score="0" ViewCount="23" Body="&lt;p&gt;My case is the db server having more than 30M records of data now. I want to create a big data system in order to speed up the searching speed.&#xA;I have read some article about combination of spark and hadoop. It is a good combination for that. I'm concerning the following points.&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;In python, can we submit the script without using spark-shell? Since I want to develop some RESTful API for querying the data.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;It looks like if the db has some new or updated data. I have to push whole db data every times by Scoop. Or It can be deal with the other ways?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Can we see this combination as a faster db system? Can I implement the system like this?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/iR2X1.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/iR2X1.jpg&quot; alt=&quot;&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks&lt;/p&gt;&#xA;" OwnerUserId="4592120" LastActivityDate="2018-03-09T08:00:53.313" Title="Can Spark and Hadoop help if SQL db server becoming so big" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;" AnswerCount="0" CommentCount="3" ClosedDate="2018-03-09T16:37:55.143" />
  <row Id="49189247" PostTypeId="1" CreationDate="2018-03-09T08:09:27.917" Score="0" ViewCount="32" Body="&lt;p&gt;I have used maven project in Scala. I have used all dependencies in pom. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Still I am getting &lt;code&gt;ClassNotFoundException&lt;/code&gt; when I run &lt;code&gt;spark-submit&lt;/code&gt; command.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;clean compile assembly:single&lt;/code&gt; is the Maven goal which I used.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Following is the spark submit command I have used.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark-submit --class com.SimpleScalaApp.SimpleApp --master local /location/file.jar&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="7309740" LastEditorUserId="6378311" LastEditDate="2018-03-09T08:11:17.967" LastActivityDate="2018-03-09T16:07:34.073" Title="ClassNotFound Exception in Spark Scala Code" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;spark-submit&gt;" AnswerCount="1" CommentCount="12" />
  <row Id="49189382" PostTypeId="1" AcceptedAnswerId="49190228" CreationDate="2018-03-09T08:19:42.720" Score="0" ViewCount="23" Body="&lt;p&gt;I am having trouble with converting my dataframe to a dataset in Spark (2.2.1) Scala (2.11.8) &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Basically, I am attempting an aggregation that collects the left dataset into a list. I am doing this step all over the place using case classes and tuples. I don't want to rewrite the same routine over and over again so i decided to refactor the step into this method:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; def collectUsingGenerics[L &amp;lt;: Product : Encoder,R &amp;lt;: Product : Encoder](&#xA;                              left: Dataset[L],&#xA;                              right: Dataset[R],&#xA;                              joinCol: Column,&#xA;                              groupCol: Column): Dataset[(L,List[R])] = {&#xA;&#xA;import left.sparkSession.implicits._&#xA;import org.apache.spark.sql.functions._&#xA;&#xA;val result = left&#xA;  .join(right, joinCol)&#xA;  .select(&#xA;    groupCol.as(&quot;groupCol&quot;),&#xA;    struct(left(&quot;*&quot;)).as(&quot;_1&quot;),&#xA;    struct(right(&quot;*&quot;)).as(&quot;_2&quot;))&#xA;  .groupBy($&quot;groupCol&quot;)&#xA;  .agg(&#xA;    first($&quot;_1&quot;).as(&quot;_1&quot;),&#xA;    collect_list($&quot;_2&quot;).as(&quot;_2&quot;)&#xA;  )&#xA;  .drop($&quot;groupCol&quot;)&#xA;&#xA;//This does not Work!!!&#xA;  result.as[(L,List[R])]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The Unit Test:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&quot;collectUsingGenerics&quot; should &quot;collect the right-side Dataset&quot; in {&#xA;   val left = spark.createDataset(Seq(&#xA;     (1, &quot;Left 1&quot;),&#xA;     (2, &quot;Left 2&quot;)&#xA;   ))&#xA;&#xA;   val right = spark.createDataset(Seq(&#xA;     (101, 1, &quot;Right 1&quot;),&#xA;     (102, 1, &quot;Right 2&quot;),&#xA;     (103, 2, &quot;Right 3&quot;)&#xA;   ))&#xA;&#xA;  val collectedDataset = Transformations.collectUsingGenerics[(Int, String), (Int, Int, String)](left, right, left(&quot;_1&quot;) === right(&quot;_2&quot;), left(&quot;_1&quot;))&#xA;      .collect()&#xA;      .sortBy(_._1._1)&#xA;&#xA;  val data1 = collectedDataset(0)&#xA;  data1._1 should be (1, &quot;Left 1&quot;)&#xA;  data1._2 should contain only((101, 1, &quot;Right 1&quot;), (102, 1, &quot;Right 2&quot;))&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The problem is, I can't compile this due to missing encoders:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Unable to find encoder for type stored in a Dataset.  Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._  Support for serializing other types will be added in future releases.&#xA;[error]     result.as[(L,List[R])]&#xA;[error]              ^&#xA;[error] one error found&#xA;[error] (Compile / compileIncremental) Compilation failed&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I was under the impression that importing spark.implicits._ is enough to generate encoders for tuple and case classes, as well as primitive types. Did I miss something?&lt;/p&gt;&#xA;" OwnerUserId="3239047" LastActivityDate="2018-03-09T09:12:28.063" Title="How do I make a Generic Tuple dataset in Spark" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;tuples&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="2" FavoriteCount="1" />
  <row Id="49189962" PostTypeId="1" CreationDate="2018-03-09T08:58:03.533" Score="0" ViewCount="31" Body="&lt;p&gt;I have a dataframe, which one column 'topic1' is a type of structure and another one 'topic' is string, i want to extract the value of 'topic1' according to column 'topic' use the udf. But something goes wrong because the number of dataframe is  mismatch.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def mapExtr_miss(map_var, key, miss_num): &#xA;    try:&#xA;        if key and str(key) in map_var.keys() and len(map_var[str(key)])&amp;gt;0:&#xA;            return map_var[str(key)]&#xA;    except:&#xA;        return [0]*miss_num&#xA;&#xA;def udf_Extr_miss(miss_num):&#xA;return udf(lambda map_var, key: mapExtr_miss(map_var, key, miss_num), ArrayType(IntergType()))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;the results as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;c1 = combine.withColumn('topic2', udf_Extr_miss(198)(col('topic1'), col('topic')))&#xA;c1.count()&#xA;266132                                                                          &#xA;combine.count()&#xA;271997                                                                          &#xA;c2= combine.withColumn('topic2', udf_Extr_miss(41)(col('topic1'), col('topic')))&#xA;c2.count()&#xA;277497 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;thanks for any idea about this problem&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;c1.head(n=2)&#xA;&#xA;[Row(uid=u'u1', newsid=u'451', topic1=Row(98=None, 99=None, 9999=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1212, 1212, 1212, 1212, 1212, 1212, 1, 1, 1, 500, 0]),  topic=None, heat=0.5841230154037476, jumpLink=0, topic2=None),Row(uid=u'u822316', newsid=u'4514', topic1=Row(98=None, 99=None, 9999=[1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1212, 1212, 1212, 1212, 1212, 1212, 1, 1, 1, 500, 0]),  topic='9999', heat=0.5841230154037476, jumpLink=0, topic2=[1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1212, 1212, 1212, 1212, 1212, 1212, 1, 1, 1, 500, 0])]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="8842441" LastEditorUserId="5880706" LastEditDate="2018-03-09T15:46:00.880" LastActivityDate="2018-03-09T16:59:33.920" Title="Pyspark apply udf to add a new col, but the the number of rows is not correspond" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;user-defined-functions&gt;" AnswerCount="1" CommentCount="4" FavoriteCount="1" />
  <row Id="49190099" PostTypeId="1" CreationDate="2018-03-09T09:05:33.310" Score="1" ViewCount="38" Body="&#xA;&#xA;&lt;p&gt;I want to apply &lt;code&gt;splitUtlisation&lt;/code&gt; on each row of &lt;code&gt;utilisationDataFarme&lt;/code&gt; and pass &lt;code&gt;startTime&lt;/code&gt; and &lt;code&gt;endTime&lt;/code&gt; as parameters., as a result &lt;code&gt;splitUtlisation&lt;/code&gt; will return multiple rows of data hence I want to create a new DataFrame with (Id, Day, Hour, Minute)&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;def splitUtlisation(onDateTime, offDateTime):&#xA;    yield onDateTime&#xA;    rule = rrule.rrule(rrule.HOURLY, byminute = 0, bysecond = 0, dtstart=offDateTime)&#xA;    for result in rule.between(onDateTime, offDateTime):&#xA;      yield result&#xA;    yield offDateTime&#xA;&#xA;&#xA;utilisationDataFarme = (&#xA;sc.parallelize([&#xA;    (10001, &quot;2017-02-12 12:01:40&quot; , &quot;2017-02-12 12:56:32&quot;),&#xA;    (10001, &quot;2017-02-13 12:06:32&quot; , &quot;2017-02-15 16:06:32&quot;),&#xA;    (10001, &quot;2017-02-16 21:45:56&quot; , &quot;2017-02-21 21:45:56&quot;),&#xA;    (10001, &quot;2017-02-21 22:32:41&quot; , &quot;2017-02-25 00:52:50&quot;),&#xA;    ]).toDF([&quot;id&quot;,  &quot;startTime&quot; ,  &quot;endTime&quot;])&#xA;    .withColumn(&quot;startTime&quot;, col(&quot;startTime&quot;).cast(&quot;timestamp&quot;))&#xA;    .withColumn(&quot;endTime&quot;, col(&quot;endTime&quot;).cast(&quot;timestamp&quot;))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In core Python I did like this &lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;dayList = ['SUN' , 'MON' , 'TUE' , 'WED' , 'THR' , 'FRI' , 'SAT']&#xA;    for result in hours_aligned(datetime.datetime.now(), datetime.datetime.now() + timedelta(hours=68)):&#xA;      print(dayList[datetime.datetime.weekday(result)],  result.hour, 60 if result.minute == 0 else result.minute)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Result &lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;THR 21 60&#xA;THR 22 60&#xA;THR 23 60&#xA;FRI 0 60&#xA;FRI 1 60&#xA;FRI 2 60&#xA;FRI 3 60&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How to create it in pySpark? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried to create new Schema and apply &lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;schema = StructType([StructField(&quot;Id&quot;, StringType(), False), StructField(&quot;Day&quot;, StringType(), False), StructField(&quot;Hour&quot;, StringType(), False) , StructField(&quot;Minute&quot;, StringType(), False)])&#xA;udf_splitUtlisation = udf(splitUtlisation, schema)&#xA;df = sqlContext.createDataFrame([],&quot;id&quot; , &quot;Day&quot; , &quot;Hour&quot; , &quot;Minute&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Still I could not handle multiple rows as response.&lt;/p&gt;&#xA;" OwnerUserId="903521" LastEditorUserId="5858851" LastEditDate="2018-03-09T15:02:05.167" LastActivityDate="2018-03-09T15:02:05.167" Title="UDF with multiple rows as response pySpark" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" />
  <row Id="49191108" PostTypeId="1" CreationDate="2018-03-09T09:58:43.720" Score="0" ViewCount="19" Body="&lt;pre&gt;&lt;code&gt;val df = sqlContext.createDataFrame(rddData, schema).show(false)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I try to register as table I'm getting below error &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df.registerTempTable(&quot;bankDetails&quot;)&#xA;42:error registerTempTable is not a member of unit&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="7429472" LastEditorUserId="6378311" LastEditDate="2018-03-09T12:36:00.713" LastActivityDate="2018-03-09T12:36:00.713" Title="CREATING TABLE USING DATA FRAME in scala" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;spark-dataframe&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="49191232" PostTypeId="1" CreationDate="2018-03-09T10:05:09.843" Score="1" ViewCount="19" Body="&lt;p&gt;my scenario:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Create a spark streaming context to pull data from kafka for batch time 50 sec&lt;/li&gt;&#xA;&lt;li&gt;create temp tables in spark for the data pulled.&lt;/li&gt;&#xA;&lt;li&gt;run spark sql query on 50s batch temp table data&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;this must repeat till spark streaming job is running.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;but i am unable to figure out a way where spark sql will be run within the streaming context.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;As of now spark sql query is running only once and streaming is happening continuously.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val nv = (new Streamreader(kafkaParams,db))&#xA;&#xA;  kafkaOpTopic.split(&quot;,&quot;).foreach(x =&amp;gt; {&#xA;    nv.streader(ssc, Array(x))&#xA;  })&#xA;  spark.sql(&quot;show tables&quot;).show()&#xA;  ssc.start()&#xA;  ssc.awaitTermination()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;StreamReader is doing kafka pull operation.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What would be the right way to run spark sql at the end of every spark streaming batch window ? how to realize it ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any idea would be greatly helpful.&lt;/p&gt;&#xA;" OwnerUserId="6587738" LastActivityDate="2018-03-09T10:05:09.843" Title="how to run spark sql queries in every spark streaming batch?" Tags="&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;spark-streaming&gt;&lt;spark-structured-streaming&gt;" AnswerCount="0" CommentCount="2" FavoriteCount="2" />
  <row Id="49191500" PostTypeId="1" AcceptedAnswerId="49191901" CreationDate="2018-03-09T10:19:47.543" Score="0" ViewCount="12" Body="&lt;p&gt;I am trying to add a column to DataFrame depending on whether column value is in another column as follow:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df=df.withColumn('new_column',when(df['color']=='blue'|df['color']=='green','A').otherwise('WD'))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;after running the code I obtain the following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Py4JError: An error occurred while calling o59.or. Trace:&#xA;py4j.Py4JException: Method or([class java.lang.String]) does not exist&#xA;    at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)&#xA;    at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)&#xA;    at py4j.Gateway.invoke(Gateway.java:274)&#xA;    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&#xA;    at py4j.commands.CallCommand.execute(CallCommand.java:79)&#xA;    at py4j.GatewayConnection.run(GatewayConnection.java:214)&#xA;    at java.lang.Thread.run(Thread.java:748)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;what shall I do to overcome this issue? &#xA;I am using PySpark 2.3.0&lt;/p&gt;&#xA;" OwnerUserId="9087140" LastActivityDate="2018-03-09T10:40:58.810" Title="Spark __getnewargs__ error ... Method or([class java.lang.String]) does not exist" Tags="&lt;apache-spark&gt;&lt;pyspark-sql&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49191853" PostTypeId="1" CreationDate="2018-03-09T10:37:43.807" Score="0" ViewCount="22" Body="&lt;p&gt;I am new in pyspark and i was trying to make a multinomional linear regression model but got stuck in middle. Please help me.&#xA;these are the step i followed.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    lr_data=loan_data.select('int_rate','loan_amnt','term','grade','sub_grade','emp_length','verification_status','home_ownership','annual_inc','purpose','addr_state','open_acc')&#xA;&#xA;    lr_data.printSchema()&#xA;    root&#xA;    |-- int_rate: string (nullable = true)&#xA;    |-- loan_amnt: integer (nullable = true)&#xA;    |-- term: string (nullable = true)&#xA;    |-- grade: string (nullable = true)&#xA;    |-- sub_grade: string (nullable = true)&#xA;    |-- emp_length: string (nullable = true)&#xA;    |-- verification_status: string (nullable = true)&#xA;    |-- home_ownership: string (nullable = true)&#xA;    |-- annual_inc: double (nullable = true)&#xA;    |-- purpose: string (nullable = true)&#xA;    |-- addr_state: string (nullable = true)&#xA;    |-- open_acc: string (nullable = true)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here &lt;strong&gt;int_rate&lt;/strong&gt; my target variable and rest of them are independent variable. Since target variable should not be string type so I transform this into string indexer and other I used pipeline method and transform with one hot encoder and then vector assembler. but first let's see my dataset.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;   int_rate     loan_amnt   term    grade   sub_grade   emp_length   verification_status    home_ownership  annual_inc  purpose     addr_state  open_acc&#xA;    0   10.65%  5000.0  36 months   B   B2  10+ years   Verified     RENT   24000.0     credit_card     AZ  3&#xA;   1    15.27%  2500.0  60 months   C   C4  &amp;lt; 1 year    Source Verified     RENT    30000.0     car     GA  3&#xA;   2    15.96%  2400.0  36 months   C   C5  10+ years   Not Verified    RENT    12252.0     small_business  IL  2&#xA;&#xA; only three rows&#xA;&#xA;   from pyspark.ml.feature import StringIndexer&#xA;    stringIndexer = StringIndexer(inputCol=&quot;int_rate&quot;, outputCol = &quot;rate_ind&quot;)&#xA;    model=stringIndexer.fit(lr_data)&#xA;    lr_data2= model.transform(lr_data)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;after doing this my int_rate variable is double type not numeric but I continued.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;   ## build pipeline model&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;from pyspark.ml import Pipeline&#xA;pipeline = Pipeline(stages=all_stages)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    ## fit pipeline model&#xA;   pipeline_mode = pipeline.fit(lr_data2)&#xA;   ## transform data&#xA;  lr_data_coded = pipeline_mode.transform(lr_data2)&#xA; from pyspark.ml.feature import VectorAssembler&#xA;&#xA;  # feature columns&#xA;  feature_columns = lr_data_coded.columns[0:10]&#xA;&#xA;  # build VectorAssembler instance&#xA;  vectorassembler = VectorAssembler(inputCols=feature_columns,   outputCol='features')&#xA;&#xA; # transform data&#xA;  lr_data3 = vectorassembler.transform(lr_data_coded)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;now all my transform data in the lr_data3 now I have to make a linear regression model. I first split my data into train and test data.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;  #spilt into two partition &#xA;&#xA;  train_data, test_data = lr_data3.randomSplit([.7,.3], seed = 100)&#xA;  df=lr_data3.select(&quot;rate_ind&quot;,&quot;features&quot;,).show(5, truncate = False)&#xA;  # Import LinearRegression class&#xA;  from pyspark.ml.regression import LinearRegression&#xA;  lr = LinearRegression(labelCol=&quot;label&quot;, featuresCol=&quot;features&quot;)&#xA;  modelA = lr.fit(train_data, {lr.regParam:0.0})&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;now the error I am getting is u'Field &quot;label&quot; does not exist.'. ofcource I do not have any label colcum in my dataset. Please suggest me what can i do to run this model&lt;/p&gt;&#xA;" OwnerUserId="9459842" LastActivityDate="2018-03-09T10:37:43.807" Title="multinomional linear regression model in pyspark error u'Field &quot;label&quot; does not exist.'" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;machine-learning&gt;&lt;pyspark&gt;" AnswerCount="0" CommentCount="7" />
  <row Id="49192083" PostTypeId="1" CreationDate="2018-03-09T10:51:19.667" Score="1" ViewCount="32" Body="&lt;p&gt;I have a large grouped dataset in spark that I need to return the percentiles from 0.01 to 0.99 for.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have been using online resources to determine different methods of doing this, from operations on RDD:&#xA;&lt;a href=&quot;https://stackoverflow.com/questions/28805602/how-to-compute-percentiles-in-apache-spark&quot;&gt;How to compute percentiles in Apache Spark&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;To SQLContext functionality:&#xA;&lt;a href=&quot;https://stackoverflow.com/questions/39633614/calculate-quantile-on-grouped-data-in-spark-dataframe&quot;&gt;Calculate quantile on grouped data in spark Dataframe&lt;/a&gt;&#xA;My question is does anyone have any opinion on what the most efficient approach is?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Also as a bonus, in SQLContext there is functions for both percentile_approx and percentile. There isn't much documentation available online for 'percentile' is this just a non-approximated 'percentile_approx' function?&lt;/p&gt;&#xA;" OwnerUserId="3699720" LastEditorUserId="3699720" LastEditDate="2018-03-09T11:09:06.980" LastActivityDate="2018-03-09T20:09:40.860" Title="Percentiles in spark - most efficient method (RDD vs SqlContext)" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;&lt;pyspark-sql&gt;" AnswerCount="1" CommentCount="0" FavoriteCount="1" ClosedDate="2018-03-10T16:16:04.070" />
  <row Id="49192798" PostTypeId="1" CreationDate="2018-03-09T11:30:29.573" Score="0" ViewCount="41" Body="&lt;p&gt;Hope this is the right place to ask!&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am trying to set up a cluster with spark, cassandra and one more external tool. So, the external tool is executed in parallel across the cluster with the help of spark(pipe command) and this tool has the ability to store straight into cassandra database(see picture below) through a simple sql Insert command. This means that in every node the results are send from the external tool of the node straight to the cassandra of the node. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;My wild guess/doubt/question is that each of these nodes will act as coordinator node and will be responsible for destributing/sending the data to other nodes according to the primary/partition key at the same time. Is that right? If not...what will happen?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/zl6wB.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;external_tool_to_cassandra&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="9467089" LastActivityDate="2018-03-09T12:04:09.633" Title="Can we have many coordinator nodes at one time in Cassandra?" Tags="&lt;apache-spark&gt;&lt;cassandra&gt;&lt;parallel-processing&gt;&lt;nodes&gt;&lt;distributed&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49192828" PostTypeId="1" CreationDate="2018-03-09T11:32:26.373" Score="0" ViewCount="33" Body="&lt;p&gt;i have problem when i am trying to write HiveQL query with aggregation function or join this Warnning appear  :&#xA;WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;---&gt;So i want to run Spark as a engine instead of MapReduce on hive , I'm running hive 2.1.1, hadoop 2.3.0 on windows 10.&#xA;is that the solution or there is another solution to this issue ? &lt;/p&gt;&#xA;" OwnerUserId="9467233" LastEditorUserId="9467233" LastEditDate="2018-03-09T12:16:50.847" LastActivityDate="2018-03-09T12:16:50.847" Title="I'm running hive 2.1.1, hadoop 2.3.0 on windows 10" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;hive&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="49193253" PostTypeId="1" CreationDate="2018-03-09T11:55:45.197" Score="-2" ViewCount="9" Body="&lt;p&gt;which is the best solution to connect pyspark (on azure) to a flask app with python?&#xA;I need that the flask app send a &quot;query&quot; to Pyspark and this execute it and after it will send the result to Flask app.&#xA;Solution?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you:)&lt;/p&gt;&#xA;" OwnerUserId="9457768" LastActivityDate="2018-03-09T11:55:45.197" Title="Flask App and Pyspark" Tags="&lt;sql&gt;&lt;flask&gt;&lt;pyspark&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49193614" PostTypeId="1" CreationDate="2018-03-09T12:14:17.257" Score="-1" ViewCount="14" Body="&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/MtfU9.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;enter image description here&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Hi everyone,&#xA;I have computed the number of rows of dataframe in pyspark twice, but the number is not the same. please info me, if you have any idea.Thanks very much&lt;/p&gt;&#xA;" OwnerUserId="8842441" LastActivityDate="2018-03-09T12:14:17.257" Title="pyspark count different numbers for the same dataframe" Tags="&lt;dataframe&gt;&lt;count&gt;&lt;pyspark&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49193718" PostTypeId="1" CreationDate="2018-03-09T12:20:38.047" Score="0" ViewCount="29" Body="&lt;p&gt;I have a Data with Columns below&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;dataset.show();&#xA;+---------+------+--------------+------+--------------------------+&#xA;|  Col1   | Col2 | Acceleration | Mass | Force(Acceleration*Mass) |&#xA;+---------+------+--------------+------+--------------------------+&#xA;| weight1 | ex1  |           10 |    5 |                       50 |&#xA;| weight1 | ex2  |            8 |    4 |                       32 |&#xA;| weight2 | ex1  |            5 |    3 |                       15 |&#xA;| weight2 | ex2  |            9 |    4 |                       36 |&#xA;+---------+------+--------------+------+--------------------------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I use a aggMap as below.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;aggMap.put(&quot;Acceleration&quot;,&quot;sum&quot;);&#xA;aggMap.put(&quot;Mass&quot;,&quot;sum&quot;);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;for &lt;strong&gt;Force&lt;/strong&gt; I want it to be calculated always as &lt;code&gt;Acceleration*Mass&lt;/code&gt;, How can I pass this in a &lt;code&gt;aggMap&lt;/code&gt;(Here I'm not passing as I am unable to do)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In java I do groupby as &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;dataset=dataset.select(col(&quot;Col1&quot;)).groupBy(col(&quot;Col1&quot;)).agg(aggMap);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and the result I get as &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+---------+-------------------+-----------+&#xA;|  Col1   | sum(Acceleration) | sum(Mass) |&#xA;+---------+-------------------+-----------+&#xA;| weight1 |                18 |         9 |&#xA;| weight2 |                14 |         7 |&#xA;+---------+-------------------+-----------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But these columns need to modified &lt;code&gt;sum(Acceleration) as Acceleration&lt;/code&gt; and &lt;code&gt;sum(Mass) as Mass&lt;/code&gt;&#xA;and I want column of Force to be calculated in the aggregation and should come as &lt;code&gt;Force&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+---------+-------------------+-----------+-------+&#xA;|  Col1   | sum(Acceleration) | sum(Mass) | Force |&#xA;+---------+-------------------+-----------+-------+&#xA;| weight1 |                18 |         9 |   172 |&#xA;| weight2 |                14 |         7 |    98 |&#xA;+---------+-------------------+-----------+-------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How Can I achieve the Same?&#xA;I'm doing Map because I am getting the Column Names (force,mass,Accele..) dynamically, Its not every time I have to compute.So I'll check if I need only Acceleration or Mass or Both or All three.&lt;/p&gt;&#xA;" OwnerUserId="8854940" LastEditorUserId="8854940" LastEditDate="2018-03-09T12:52:59.413" LastActivityDate="2018-03-09T12:52:59.413" Title="Using Map For Aggregation in Spark JAVA" Tags="&lt;java&gt;&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;apache-spark-dataset&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49194149" PostTypeId="1" AcceptedAnswerId="49194569" CreationDate="2018-03-09T12:47:21.413" Score="0" ViewCount="28" Body="&lt;p&gt;How to find and filter unique values from a text file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I tried like below, its not working.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; val spark = SparkSession.builder().master(&quot;local&quot;).appName(&quot;distinct&quot;).getOrCreate()&#xA;var data = spark.sparkContext.textFile(&quot;text/file/opath&quot;)&#xA;val uniqueval = data.map { rec =&amp;gt; (rec.split(&quot;,&quot;)(3).distinct) }&#xA;var fils = data.filter(line =&amp;gt; line.split(&quot;,&quot;)(3).equals(uniqueval)).map(x =&amp;gt; (x)).foreach { println }&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Sample Data:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ID | Name&#xA;1    john&#xA;1    john&#xA;2    david&#xA;3    peter&#xA;4    steve&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Required Output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1    john&#xA;2    david&#xA;3    peter&#xA;4    steve&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="8199134" LastEditorUserId="8199134" LastEditDate="2018-03-09T13:04:55.817" LastActivityDate="2018-03-09T15:53:43.853" Title="Filtering unique values from a text file" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;apache-spark-sql&gt;" AnswerCount="3" CommentCount="0" />
  <row Id="49194761" PostTypeId="1" CreationDate="2018-03-09T13:21:42.777" Score="0" ViewCount="19" Body="&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;Is it good practice to integrate output of Spark to any web application ? if yes, how do we do it?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;Generally which all tools or application will consume the data returned by Spark submit command so that it can be presented in better view?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;Thanks.&lt;/p&gt;&#xA;" OwnerUserId="6286288" LastActivityDate="2018-03-09T13:21:42.777" Title="Apache Spark integration with Web application" Tags="&lt;apache-spark&gt;" AnswerCount="0" CommentCount="0" ClosedDate="2018-03-09T14:47:46.860" />
  <row Id="49194784" PostTypeId="1" CreationDate="2018-03-09T13:22:29.320" Score="0" ViewCount="20" Body="&#xA;&#xA;&lt;p&gt;just dragged in a road block kind situation while applying map function on pyspark dataframe and need your help in coming out of this.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Though problem is even more complicated but let me simplify it with below example using dictionary and for loop, and need solution in pyspark.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here example of python code on dummy data, I want same in pyspark map transformation with when, clause using window or any other way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Problem - I have a pyspark dataframe with column name as keys in below dictionary and want to add/modify section column with similar logic applied in for loop in this example.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;record=[&#xA;{'id':xyz,'SN':xyz,'miles':xyz,'feet':xyz,'MP':xyz,'section':xyz},&#xA;{'id':xyz,'SN':xyz,'miles':xyz,'feet':xyz,'MP':xyz,'section':xyz},&#xA;{'id':xyz,'SN':xyz,'miles':xyz,'feet':xyz,'MP':xyz,'section':xyz}&#xA;]&#xA;&#xA;last_rec='null'&#xA;section=0&#xA;for cur_rec in record:&#xA;    if lastTrack != null:&#xA;        if (last_rec.id != cur_rec.id | last_rec.SN != cur_rec.SN):&#xA;            section+=1&#xA;&#xA;        elif (last_rec.miles == cur_rec.miles &amp;amp; abs(last_rec.feet- cur_rec.feet) &amp;gt; 1):&#xA;            section+=1&#xA;&#xA;        elif (last_rec.MP== 555 &amp;amp; cur_rec.MP != 555):&#xA;            section+=1&#xA;&#xA;        elif (abs(last_rec.miles- cur_rec.miles) &amp;gt; 1):&#xA;            section+=1&#xA;&#xA;&#xA;    cur_rec['section']= section&#xA;    last_rec = cur_rec&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="4613376" LastEditorUserId="5858851" LastEditDate="2018-03-09T15:52:56.487" LastActivityDate="2018-03-09T22:12:14.763" Title="How to replace for loop in python with map transformation in pyspark where we want to compare previous row and current row with multiple conditions" Tags="&lt;pyspark&gt;&lt;rdd&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="49194921" PostTypeId="1" CreationDate="2018-03-09T13:29:41.010" Score="0" ViewCount="18" Body="&lt;p&gt;I want to parse a json string in our spark Project. &#xA;I use IDEA. If i run my scala code in IDEA, everything runs well. &#xA;But When i pack a jar by IDEA+SBT and submit this jar to cluster. &#xA;Error happpened: &lt;strong&gt;Exception in thread &quot;main&quot; Java.lang.NoClassDefFoundError: play/api/libs/json/JSObject&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If i not use extra dependency jar, submit to cluster run well.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;My pack operation procedure:&lt;/strong&gt; &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;File -&amp;gt; Project Structure &#xA;     -&amp;gt; Artifacts &#xA;     -&amp;gt; &quot;+&quot; &#xA;     -&amp;gt; JAR &#xA;     -&amp;gt; From modules with dependencies &#xA;     -&amp;gt; select &quot;extract to the target JAR&quot; and set &quot;Main Class&quot; &#xA;     -&amp;gt; Bulid &#xA;     -&amp;gt; Build Artifacts &#xA;     -&amp;gt; Build&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;screenshot:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/LgdrZ.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;Artifacts&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/SpXX8.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;MANIFEST&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/46465.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;spark-Error&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="8740367" LastActivityDate="2018-03-09T13:29:41.010" Title="Using play Json lib in spark project throws :NoClassDefFoundError" Tags="&lt;apache-spark&gt;&lt;intellij-idea&gt;&lt;sbt&gt;&lt;play-json&gt;" AnswerCount="0" CommentCount="5" />
  <row Id="49195885" PostTypeId="1" CreationDate="2018-03-09T14:26:39.880" Score="0" ViewCount="18" Body="&lt;p&gt;I am using Spark 2.0.2 and Cassandra 3.11.2 I am using this code but it give me connection error.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;./spark-shell --jars ~/spark/spark-cassandra-connector/spark-cassandra-connector/target/full/scala-2.10/spark-cassandra-connector-assembly-2.0.5-121-g1a7fa1f8.jar&#xA;import com.datastax.spark.connector._&#xA;&#xA; val conf = new SparkConf(true).set(&quot;spark.cassandra.connection.host&quot;, &quot;localhost&quot;)&#xA;val test = sc.cassandraTable(&quot;sensorkeyspace&quot;, &quot;sensortable&quot;)&#xA;test.count&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I enter test.count command it give me this error.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;java.io.IOException: Failed to open native connection to Cassandra at {127.0.0.1}:9042&#xA;    at com.datastax.spark.connector.cql.CassandraConnector$.com$datastax$spark$connector$cql$CassandraConnector$$createSession(CassandraConnector.scala:168)&#xA;    at com.datastax.spark.connector.cql.CassandraConnector$$anonfun$8.apply(CassandraConnector.scala:154)&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;" OwnerUserId="6450118" LastActivityDate="2018-03-09T14:33:49.077" Title="Connection to Cassandra from spark Error" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;cassandra&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49195920" PostTypeId="1" AcceptedAnswerId="49196206" CreationDate="2018-03-09T14:29:07.880" Score="0" ViewCount="17" Body="&lt;p&gt;I have a Dataframe with a Column of Array Type&#xA;For example :&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val df = List((&quot;a&quot;, Array(1d,2d,3d)), (&quot;b&quot;, Array(4d,5d,6d))).toDF(&quot;ID&quot;, &quot;DATA&quot;)&#xA;df: org.apache.spark.sql.DataFrame = [ID: string, DATA: array&amp;lt;double&amp;gt;]&#xA;&#xA;scala&amp;gt; df.show&#xA;+---+---------------+&#xA;| ID|           DATA|&#xA;+---+---------------+&#xA;|  a|[1.0, 2.0, 3.0]|&#xA;|  b|[4.0, 5.0, 6.0]|&#xA;+---+---------------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I wish to explode the array and have index like&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+---+------------------+&#xA;| ID|  DATA_INDEX| DATA|&#xA;+---+------------------+&#xA;|  a|1           | 1.0 |&#xA;|  a|2           | 2.0 |&#xA;|  a|3           | 3.0 |&#xA;|  b|1           | 4.0 |&#xA;|  b|2           | 5.0 |&#xA;|  b|3           | 6.0 |&#xA;+---+------------+-----+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I wish be able to do that with scala, and Sparlyr or SparkR&#xA;I'm using spark 1.6&lt;/p&gt;&#xA;" OwnerUserId="959517" LastEditorUserId="959517" LastEditDate="2018-03-09T14:35:47.037" LastActivityDate="2018-03-09T14:44:48.673" Title="Extracting array index in Spark Dataframe" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;dataframe&gt;" AnswerCount="2" CommentCount="1" />
  <row Id="49196055" PostTypeId="1" CreationDate="2018-03-09T14:37:15.303" Score="1" ViewCount="43" Body="&lt;p&gt;I am trying to run a process using the Futures API of scala to run certain actions in parallel. Below is a sample code snippet&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import scala.util._&#xA;import scala.concurrent._&#xA;import scala.concurrent.ExecutionContext.Implicits.global&#xA;&#xA;object ConcurrentContext {&#xA;  def def appMain(args: Array[String]) = {&#xA;// configure spark&#xA;val spark = SparkSession&#xA;    .builder&#xA;    .appName(&quot;parjobs&quot;)&#xA;    .getOrCreate()&#xA;&#xA;val pool = Executors.newFixedThreadPool(5)&#xA;   // create the implicit ExecutionContext based on our thread pool&#xA;implicit val xc = ExecutionContext.fromExecutorService(pool)&#xA;&#xA; /** Wraps a code block in a Future and returns the future */&#xA;def executeAsync[T](f: =&amp;gt; T): Future[T] = {&#xA;Future(f)&#xA;}&#xA;} &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My questions are:-&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;If I set executor-cores value to 4 which controls the number of threads per executor JVM and create a thread pool of 5 inside the application, which one would take precedence?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;If I don't explicitly set the thread pool then the default &lt;code&gt;ExecutionContext&lt;/code&gt; will create a default thread pool based on all the cores present on the machine from where the process is initiated (which would be the driver), in that situation how would the executor-core property impact?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;If the thread pool value takes precedence over executor-core and if I use the default value is there a possibility that there are many threads(equal to CPU cores) per JVM?&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="3226630" LastEditorUserId="1305344" LastEditDate="2018-03-09T17:45:21.427" LastActivityDate="2018-03-09T17:45:21.427" Title="What is the relationship between thread pools (on driver) and executor cores?" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="13" />
  <row Id="49196607" PostTypeId="1" CreationDate="2018-03-09T15:06:01.310" Score="0" ViewCount="11" Body="&lt;p&gt;I'm aware that this same error has been asked about before, but in &lt;a href=&quot;https://stackoverflow.com/questions/40297403/pyspark-error-attributeerror-nonetype-object-has-no-attribute-jvm&quot;&gt;this example&lt;/a&gt; and &lt;a href=&quot;https://stackoverflow.com/questions/47552309/withcolumn-with-udf-yields-attributeerror-nonetype-object-has-no-attribute?rq=1&quot;&gt;this other example&lt;/a&gt;, the errors are caused by using &lt;code&gt;pyspark.sql&lt;/code&gt; functions in a &lt;code&gt;udf&lt;/code&gt;. This is not what I am doing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The offending code is the following function definition (if I remove the default parameter my code runs and passes all tests). &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark.sql import functions as F&#xA;&#xA;def apply_filter(df, group=F.lit(True)):&#xA;    filtered_df = df.filter(group)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am mainly looking for the reason that this code produces the same error that is found in the other examples. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit: &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I cannot share the original code due to work, but if you run the previous code with &lt;code&gt;spark-submit --deploy-mode cluster &amp;lt;filename&amp;gt;&lt;/code&gt; the following error is produced. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;LogType:stdout&#xA;Log Upload Time:Fri Mar 09 16:01:45 +0000 2018&#xA;LogLength:343&#xA;Log Contents:&#xA;Traceback (most recent call last):&#xA;  File &quot;temp.py&quot;, line 3, in &amp;lt;module&amp;gt;&#xA;    def apply_filter(df, group=F.lit(True)):&#xA;  File &quot;/mnt/yarn/usercache/hadoop/appcache/application_1520603520946_0005/container_1520603520946_0005_01_000001/pyspark.zip/pyspark/sql/functions.py&quot;, line 40, in _&#xA;AttributeError: 'NoneType' object has no attribute '_jvm'&#xA;End of LogType:stdout&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Interestingly enough, the error doesn't persist if it is ran locally. &lt;/p&gt;&#xA;" OwnerUserId="4500078" LastEditorUserId="4500078" LastEditDate="2018-03-09T16:06:00.673" LastActivityDate="2018-03-09T21:38:36.627" Title="AttributeError: 'NoneType' object has no attribute '_jvm' when passing sql function as a default parameter" Tags="&lt;python&gt;&lt;python-3.x&gt;&lt;pyspark&gt;&lt;apache-spark-sql&gt;&lt;pyspark-sql&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49196920" PostTypeId="1" AcceptedAnswerId="49197492" CreationDate="2018-03-09T15:21:36.690" Score="0" ViewCount="23" Body="&lt;p&gt;I want to split a schema of a dataframe into a collection. I am trying this, but the schema is printed out as a string. Is there anyway I can split it into a collection per StructType so that I can manipulate it (like take only array columns from the output)? I am trying to flatten a complex multi level struct + array dataframe. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import org.apache.spark.sql.functions.explode&#xA;import org.apache.spark.sql._&#xA;&#xA;val test = sqlContext.read.json(sc.parallelize(Seq(&quot;&quot;&quot;{&quot;a&quot;:1,&quot;b&quot;:[2,3],&quot;d&quot;:[2,3]}&quot;&quot;&quot;)))&#xA;&#xA;test.printSchema&#xA;&#xA;val flattened = test.withColumn(&quot;b&quot;, explode($&quot;d&quot;))&#xA;&#xA;flattened.printSchema&#xA;&#xA;def identifyArrayColumns(dataFrame : DataFrame) = {&#xA;    val output = for ( d &amp;lt;- dataFrame.collect()) yield&#xA;    {&#xA;       d.schema&#xA;    }&#xA;    output.toList&#xA;}&#xA;&#xA;&#xA;identifyArrayColumns(test)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Output currently is &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;identifyArrayColumns: (dataFrame: org.apache.spark.sql.DataFrame)List[org.apache.spark.sql.types.StructType]&#xA;res58: List[org.apache.spark.sql.types.StructType] = List(StructType(StructField(a,LongType,true), StructField(b,ArrayType(LongType,true),true), StructField(d,ArrayType(LongType,true),true)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;It is one full string, so I cannot filter only the array columns. Suppose if I do a foreach(println). I get only one line&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;scala&amp;gt; output.foreach(println)&#xA;StructType(StructField(a,LongType,true), StructField(b,ArrayType(LongType,true),true), StructField(d,ArrayType(LongType,true),true))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What I want is each StructTypes in a single element in a collection&lt;/p&gt;&#xA;" OwnerUserId="5964853" LastEditorUserId="5964853" LastEditDate="2018-03-09T15:36:57.037" LastActivityDate="2018-03-09T16:03:17.813" Title="How can i split a string of dataframe schema into each Structs" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;schema&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="49197007" PostTypeId="1" CreationDate="2018-03-09T15:26:33.020" Score="0" ViewCount="5" Body="&lt;p&gt;I am running into a problem in pyspark that does not make sense to me. I cache the pyspark dataframe by:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sdf.persist(pyspark.StorageLevel.MEMORY_AND_DISK)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;My understanding after reading the help document is that if memory is not enough to hold the entire dataframe, disk space will be used.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;However, I am still getting error like this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#&#xA;# There is insufficient memory for the Java Runtime Environment to continue.&#xA;# Native memory allocation (mmap) failed to map 3400007680 bytes for committing reserved memory.&#xA;# An error report file with more information is saved as:&#xA;# /home/centos/digital_product/prog/hs_err_pid24622.log&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there anything I am missing here? Thanks&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Edit:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am running pyspark in local mode and here is the config:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;conf.set(&quot;spark.driver.memory&quot;,&quot;30g&quot;)\&#xA;    .set(&quot;spark.debug.maxToStringFields&quot;,&quot;10000&quot;)\&#xA;    .set(&quot;spark.driver.maxResultSize&quot;, &quot;30g&quot;)\&#xA;    .setMaster(&quot;local[16]&quot;)\&#xA;    .setAppName(&quot;digital&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="5236286" LastActivityDate="2018-03-09T15:26:33.020" Title="pyspark dataframe persist and insufficient memory error" Tags="&lt;pyspark&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49197224" PostTypeId="1" CreationDate="2018-03-09T15:38:51.597" Score="0" ViewCount="23" Body="&lt;p&gt;Is there any difference in support of Scala/Java in apache spark? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Frequency of updates? &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Potential drop of support?&lt;/p&gt;&#xA;" OwnerUserId="1343364" LastEditorUserId="1343364" LastEditDate="2018-03-09T15:51:47.117" LastActivityDate="2018-03-09T15:51:47.117" Title="Java vs Scala support in Apache Spark" Tags="&lt;apache-spark&gt;" AnswerCount="0" CommentCount="2" ClosedDate="2018-03-09T15:47:25.650" />
  <row Id="49197904" PostTypeId="1" CreationDate="2018-03-09T16:16:51.267" Score="0" ViewCount="10" Body="&lt;p&gt;How can i make &lt;code&gt;g.bfs&lt;/code&gt; output all the paths to vertices that match a condition. Take for example this graph&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;vertices = sqlContext.createDataFrame([&#xA;  (&quot;a&quot;, &quot;Alice&quot;, 34),&#xA;  (&quot;b&quot;, &quot;Bob&quot;, 36),&#xA;  (&quot;c&quot;, &quot;Charlie&quot;, 30),&#xA;  (&quot;d&quot;, &quot;David&quot;, 29),&#xA;  (&quot;e&quot;, &quot;Esther&quot;, 32),&#xA;  (&quot;f&quot;, &quot;Fanny&quot;, 36),&#xA;  (&quot;g&quot;, &quot;Gabby&quot;, 60)], [&quot;id&quot;, &quot;name&quot;, &quot;age&quot;])&#xA;&#xA;edges = sqlContext.createDataFrame([&#xA;  (&quot;a&quot;, &quot;b&quot;, &quot;friend&quot;),&#xA;  (&quot;b&quot;, &quot;c&quot;, &quot;follow&quot;),&#xA;  (&quot;c&quot;, &quot;b&quot;, &quot;follow&quot;),&#xA;  (&quot;f&quot;, &quot;c&quot;, &quot;follow&quot;),&#xA;  (&quot;e&quot;, &quot;f&quot;, &quot;follow&quot;),&#xA;  (&quot;e&quot;, &quot;d&quot;, &quot;friend&quot;),&#xA;  (&quot;d&quot;, &quot;a&quot;, &quot;friend&quot;),&#xA;  (&quot;d&quot;, &quot;e&quot;, &quot;friend&quot;),&#xA;  (&quot;a&quot;, &quot;e&quot;, &quot;friend&quot;)&#xA;], [&quot;src&quot;, &quot;dst&quot;, &quot;relationship&quot;])&#xA;&#xA;toy_g = GraphFrame(vertices, edges)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I try to get all paths from 'd' to nodes with more than 30 years.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-py prettyprint-override&quot;&gt;&lt;code&gt;paths = g_toy.bfs(&quot;name = 'David'&quot;, &quot;age &amp;gt; 30&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, &lt;code&gt;paths.show()&lt;/code&gt; only outputs this:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+------------+------------+-------------+&#xA;|        from|          e0|           to|&#xA;+------------+------------+-------------+&#xA;|[d,David,29]|[d,e,friend]|[e,Esther,32]|&#xA;|[d,David,29]|[d,a,friend]| [a,Alice,34]|&#xA;+------------+------------+-------------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is the shortest path to nodes that first match the condition, but how to get the output also for example to show the path to 'e' and 'f', nodes that are also meet the condition.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I know it its related to &lt;a href=&quot;https://stackoverflow.com/questions/40942281/graphframes-bfs-issue&quot;&gt;Graphframes BFS issue&lt;/a&gt; but I still need to get the path to all the nodes that meet the criteria.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thanks.&lt;/p&gt;&#xA;" OwnerUserId="2704022" LastActivityDate="2018-03-09T16:16:51.267" Title="GraphFrames BFS only outputs first match" Tags="&lt;pyspark&gt;&lt;graphframes&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49198823" PostTypeId="1" CreationDate="2018-03-09T17:11:09.650" Score="0" ViewCount="17" Body="&lt;p&gt;i've been following several tutorial but i couldnt grasp the idea ( couldt apply the principle of map split filter in my example ) let say i have a String&#xA;&quot; customer1,buy:order5&quot; &#xA;i want to count the number of buy words in my different strings and also the number of bought item by customers in my stream &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;            JavaDStream&amp;lt;String&amp;gt; words = lines.flatMap(x -&amp;gt; Arrays.asList(x.split(&quot;,&quot;)).iterator()).filter(x -&amp;gt; Arrays.asList(conteneur).contains(x));&#xA;        // compte le nombre d'ads accepté et le nombre d'add refusées&#xA;        JavaPairDStream&amp;lt;String,Integer&amp;gt; nbr = words.mapToPair(x -&amp;gt; new Tuple2&amp;lt;&amp;gt;(x,1)).reduceByKey((a,b) -&amp;gt; a+b);&#xA;       nbr.map((Function&amp;lt;Tuple2&amp;lt;String, Integer&amp;gt;, String&amp;gt;) tuple -&amp;gt; {&#xA;&#xA;            return   String.valueOf(tuple._2) +&quot;,&quot;+tuple._1 ;}).print();&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;in this code bellow i calculate the number of buy and sell in a stream of strings like this one &quot;customer1,buy&quot;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;i would be glad if some could explain to me &lt;/p&gt;&#xA;" OwnerUserId="9418632" LastEditorUserId="5880706" LastEditDate="2018-03-09T17:12:02.010" LastActivityDate="2018-03-09T17:26:02.060" Title="How to split and filter String with apache SPARK in java" Tags="&lt;java&gt;&lt;apache-spark&gt;&lt;data-science&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49199272" PostTypeId="1" CreationDate="2018-03-09T17:40:23.877" Score="0" ViewCount="17" Body="&lt;p&gt;I have a spark task that runs a series of complicated SQL queries. Consider the following two loaders - if I use the first, I get the results I expect, but if I use the second, I don't get all expected rows in my result dataframe. What gives?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Version 1 (Works great, very slow!):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;static &amp;lt;T&amp;gt; Dataset&amp;lt;T&amp;gt; parseThing(String fName,Class&amp;lt;T&amp;gt; clazz,SparkSession spk,JavaSparkContext jsc, String tbName) throws FileNotFoundException&#xA;{&#xA;    System.out.println(clazz.getName());&#xA;    List&amp;lt;T&amp;gt; q  = ParseCSV(clazz, fName);&#xA;    System.out.println(&quot;Parsed: &quot; + clazz.getName() + &quot; &quot; + q.size());&#xA;    Dataset&amp;lt;T&amp;gt; ret = spk.createDataset(q,Encoders.bean(clazz)).persist(MEMORY_ONLY);&#xA;    ret.createOrReplaceTempView(tbName);&#xA;    spk.sqlContext().cacheTable(tbName);&#xA;    System.out.println(ret.count());&#xA;    return ret;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Version 2 (When you use it on a series of inputs then run a query using them, you don't get all rows in results!):&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;static &amp;lt;T&amp;gt; Dataset&amp;lt;T&amp;gt; parseThing(String fName,Class&amp;lt;T&amp;gt; clazz,SparkSession spk,JavaSparkContext jsc, String tbName) throws FileNotFoundException&#xA;{&#xA;    System.out.println(clazz.getName());&#xA;    List&amp;lt;T&amp;gt; q  = ParseCSV(clazz, fName);&#xA;    System.out.println(&quot;Parsed: &quot; + clazz.getName() + &quot; &quot; + q.size());&#xA;    RDD&amp;lt;T&amp;gt; rdd = jsc.parallelize(q).rdd().persist(MEMORY_ONLY);&#xA;    System.out.println(rdd.count());&#xA;    rdd.setName(clazz.getName());&#xA;    Dataset&amp;lt;T&amp;gt; ret = spk.createDataset(rdd,Encoders.bean(clazz)).persist(MEMORY_ONLY);&#xA;    ret.createOrReplaceTempView(tbName);&#xA;    spk.sqlContext().cacheTable(tbName);&#xA;    System.out.println(ret.count());&#xA;    return ret;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="1763955" LastEditorUserId="1763955" LastEditDate="2018-03-09T18:28:30.987" LastActivityDate="2018-03-09T18:28:30.987" Title="Why do I get different results between two different Spark loaders?" Tags="&lt;apache-spark&gt;&lt;rdd&gt;&lt;apache-spark-dataset&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49199316" PostTypeId="1" CreationDate="2018-03-09T17:42:50.187" Score="0" ViewCount="22" Body="&lt;p&gt;I need suggestions on importing data from Hadoop datalake (Kerberos authenticated) to AWS. All the tables in the Hive table should land in s3 and then needs to be loaded to AWS RDS.&#xA;I have considered the following options:&#xA;1) AWS Glue ?&#xA;2) Spark connecting to hive metastore ?&#xA;3) Connecting to impala from AWS ?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are around 50 tables to be imported. How can i maintain the schema ? IS it better to import the data and then create a seperate schema in RDS ?&lt;/p&gt;&#xA;" OwnerUserId="6786417" LastActivityDate="2018-03-09T20:40:17.487" Title="How to import hive tables from Hadoop datalake to AWS RDS?" Tags="&lt;amazon-web-services&gt;&lt;hadoop&gt;&lt;apache-spark&gt;&lt;impala&gt;" AnswerCount="1" CommentCount="3" />
  <row Id="49199392" PostTypeId="1" AcceptedAnswerId="49199724" CreationDate="2018-03-09T17:47:30.087" Score="1" ViewCount="16" Body="&lt;p&gt;How to convert this kind of data &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&quot;Row-Key-001, K1, 10, A2, 20, K3, 30, B4, 42, K5, 19, C20, 20&quot;&#xA;&quot;Row-Key-002, X1, 20, Y6, 10, Z15, 35, X16, 42&quot;&#xA;&quot;Row-Key-003, L4, 30, M10, 5, N12, 38, O14, 41, P13, 8&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;to a spark RDD using Scala so we can get:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Row-Key-001, K1&#xA;Row-Key-001, A2&#xA;Row-Key-001, K3&#xA;Row-Key-001, B4&#xA;Row-Key-001, K5&#xA;Row-Key-001, C20&#xA;Row-Key-002, X1&#xA;Row-Key-002, Y6&#xA;Row-Key-002, Z15&#xA;Row-Key-002, X16&#xA;Row-Key-003, L4&#xA;Row-Key-003, M10&#xA;Row-Key-003, N12&#xA;Row-Key-003, O14&#xA;Row-Key-003, P13&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I think we can split the input to get an array of lines and again split each line on ',' and then add to a Map like the 1st element of each row as the key and every alternate element as value. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;But need help to implement in Scala.&lt;/p&gt;&#xA;" OwnerUserId="9128581" LastEditorUserId="9297144" LastEditDate="2018-03-09T17:51:25.603" LastActivityDate="2018-03-09T18:11:38.137" Title="Spark Scala Array of String lines to pairRDD" Tags="&lt;scala&gt;&lt;dictionary&gt;&lt;apache-spark&gt;&lt;rdd&gt;&lt;pair&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49199655" PostTypeId="1" CreationDate="2018-03-09T18:04:30.367" Score="0" ViewCount="8" Body="&lt;p&gt;Oozie spark 2 action is failing, but when I run using spark-submit it works.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Error - /./assembly/target/scala-2.11/jars' does not exist; make sure Spark is built.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;StackTrace Below:&#xA;Failing Oozie Launcher, Main class &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[org.apache.oozie.action.hadoop.SparkMain], main() threw exception, Library directory '/mnt/resource/hadoop/yarn/local/usercache/admin/appcache/application_1518822111928_3264/container_e06_1518822111928_3264_01_000002/./assembly/target/scala-2.11/jars' does not exist; make sure Spark is built.&#xA;java.lang.IllegalStateException: Library directory '/mnt/resource/hadoop/yarn/local/usercache/admin/appcache/application_1518822111928_3264/container_e06_1518822111928_3264_01_000002/./assembly/target/scala-2.11/jars' does not exist; make sure Spark is built.&#xA;at org.apache.spark.launcher.CommandBuilderUtils.checkState(CommandBuilderUtils.java:260)&#xA;at org.apache.spark.launcher.CommandBuilderUtils.findJarsDir(CommandBuilderUtils.java:380)&#xA;at org.apache.spark.launcher.YarnCommandBuilderUtils$.findJarsDir(YarnCommandBuilderUtils.scala:38)&#xA;at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:587)&#xA;at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:912)&#xA;at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:172)&#xA;at org.apache.spark.deploy.yarn.Client.run(Client.scala:1248)&#xA;at org.apache.spark.deploy.yarn.Client$.main(Client.scala:1307)&#xA;at org.apache.spark.deploy.yarn.Client.main(Client.scala)&#xA;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#xA;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#xA;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#xA;at java.lang.reflect.Method.invoke(Method.java:498)&#xA;at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:751)&#xA;at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)&#xA;at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)&#xA;at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)&#xA;at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)&#xA;at org.apache.oozie.action.hadoop.SparkMain.runSpark(SparkMain.java:312)&#xA;at org.apache.oozie.action.hadoop.SparkMain.run(SparkMain.java:233)&#xA;at org.apache.oozie.action.hadoop.LauncherMain.run(LauncherMain.java:58)&#xA;at org.apache.oozie.action.hadoop.SparkMain.main(SparkMain.java:62)&#xA;at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#xA;at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#xA;at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#xA;at java.lang.reflect.Method.invoke(Method.java:498)&#xA;at org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:239)&#xA;at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)&#xA;at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)&#xA;at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)&#xA;at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)&#xA;at java.security.AccessController.doPrivileged(Native Method)&#xA;at javax.security.auth.Subject.doAs(Subject.java:422)&#xA;at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)&#xA;at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;more info yarn log : &#xA;Spark Action Main class        : org.apache.spark.deploy.SparkSubmit&lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Oozie Spark action configuration&lt;/h1&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;                --master&#xA;                yarn-cluster&#xA;                --name&#xA;                WordCount&#xA;                --class&#xA;                rdd.WordCount&#xA;                --conf&#xA;                spark.executor.extraClassPath=$PWD/*&#xA;                --conf&#xA;                spark.driver.extraClassPath=$PWD/*&#xA;                --conf&#xA;                spark.yarn.security.tokens.hive.enabled=false&#xA;                --conf&#xA;                spark.yarn.security.tokens.hbase.enabled=false&#xA;                --conf&#xA;                spark.executor.extraJavaOptions=-Dlog4j.configuration=spark-log4j.properties&#xA;                --conf&#xA;                spark.driver.extraJavaOptions=-Dlog4j.configuration=spark-log4j.properties&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;...&lt;/p&gt;&#xA;" OwnerUserId="1377418" LastEditorUserId="1377418" LastEditDate="2018-03-09T18:17:02.693" LastActivityDate="2018-03-09T18:17:02.693" Title="Failing oozie action for spark 2" Tags="&lt;apache-spark&gt;&lt;oozie&gt;&lt;hortonworks-data-platform&gt;&lt;oozie-workflow&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49200233" PostTypeId="1" CreationDate="2018-03-09T18:44:01.347" Score="0" ViewCount="10" Body="&lt;p&gt;I'm trying to integrate with schemaRegistry and SparkStreaming. By the moment I want to use GenericRecords. It seems that my producer works and new schemas are published in _schemas topic. When I try to read with my Consumer, I'm not able to deserialize the data. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;How could I say to Spark that I'm going to deserializer to GenericRecord? Do I need to say the Decoder and it isn't enough with the KafkaAvroDeserializer?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have seen examples using SpecificRecord, but, I would like something more generic and I don't want to be generatings these classes each time the schema changes.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;public class SparkStreamingSchemaRegister {&#xA;&#xA;    public static void main(String[] args) throws InterruptedException {&#xA;        String topic = &quot;avro_example_schemaRegistry&quot;;&#xA;&#xA;        final JavaStreamingContext jssc = new JavaStreamingContext(getSparkConf(),&#xA;                Durations.milliseconds(Constants.STREAM_BATCH_DURATION_MILLIS));&#xA;&#xA;&#xA;        final JavaInputDStream&amp;lt;ConsumerRecord&amp;lt;byte[], GenericRecord&amp;gt;&amp;gt; rawStream = KafkaSource.getKafkaDirectStream(jssc);&#xA;&#xA;        rawStream.foreachRDD(rdd -&amp;gt; {&#xA;            JavaRDD&amp;lt;Client&amp;gt; javaRddClient = rdd.map(&#xA;                    kafkaRecord -&amp;gt; {&#xA;&#xA;                        GenericRecord record = kafkaRecord.value(); --&amp;gt; ERROR&#xA;                        return CrmClient.getCrmClient(kafkaRecord.value());&#xA;                    });&#xA;&#xA;&#xA;           CassandraJavaUtil&#xA;                    .javaFunctions(javaRddClient)&#xA;                    .writerBuilder(&quot;keyspace&quot;, &quot;client&quot;, CassandraJavaUtil.mapToRow(CrmClient.class))&#xA;                    .withColumnSelector(CassandraJavaUtil.someColumns(&quot;id&quot;, &quot;name&quot;, &quot;lastname&quot;))&#xA;                    .saveToCassandra();&#xA;        });&#xA;&#xA;&#xA;        jssc.start();&#xA;        jssc.awaitTermination();&#xA;        jssc.close();&#xA;    }&#xA;&#xA;&#xA;    private static class KafkaSource {&#xA;        public static JavaInputDStream&amp;lt;ConsumerRecord&amp;lt;byte[], GenericRecord&amp;gt;&amp;gt; getKafkaDirectStream(JavaStreamingContext jssc) {&#xA;            JavaInputDStream&amp;lt;ConsumerRecord&amp;lt;byte[], GenericRecord&amp;gt;&amp;gt; stream = KafkaUtils.createDirectStream(jssc,&#xA;                    LocationStrategies.PreferConsistent(),&#xA;                    ConsumerStrategies.&amp;lt;byte[], GenericRecord&amp;gt;Subscribe(getKafkaTopic(), getKafkaConf()));&#xA;            return stream;&#xA;        }&#xA;&#xA;&#xA;        private static Map&amp;lt;String, Object&amp;gt; getKafkaConf() {&#xA;            Map&amp;lt;String, Object&amp;gt; kafkaParams = new HashMap&amp;lt;&amp;gt;();&#xA;            kafkaParams.put(Constants.KAFKA_PROPERTIES.KAFKA_BOOTSTRAP_SERVERS.getValue(), Constants.KAFKA_BOOTSTRAP_SERVERS);&#xA;            kafkaParams.put(Constants.KAFKA_PROPERTIES.KAFKA_KEY_DESERIALIZER.getValue(), ByteArrayDeserializer.class);&#xA;            kafkaParams.put(Constants.KAFKA_PROPERTIES.KAFKA_GROUPID.getValue(), Constants.KAFKA_GROUP_ID);&#xA;            kafkaParams.put(Constants.KAFKA_PROPERTIES.KAFKA_ENABLE_AUTO_COMMIT.getValue(), false);&#xA;            kafkaParams.put(Constants.KAFKA_PROPERTIES.KAFKA_AUTO_OFFSET_RESET.getValue(), &quot;earliest&quot;);&#xA;            kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,KafkaAvroDeserializer.class.getName());&#xA;            kafkaParams.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, &quot;false&quot;);&#xA;            kafkaParams.put(KafkaAvroDeserializerConfig.SCHEMA_REGISTRY_URL_CONFIG,&quot;http://localhost:8081&quot;);&#xA;&#xA;            return kafkaParams;&#xA;        }&#xA;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="1964202" LastActivityDate="2018-03-09T18:44:01.347" Title="SchemaRegistry with SparkStreaming with GenericRecords" Tags="&lt;apache-spark&gt;&lt;apache-kafka&gt;&lt;avro&gt;&lt;confluent-schema-registry&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49200522" PostTypeId="1" AcceptedAnswerId="49200777" CreationDate="2018-03-09T19:06:10.580" Score="0" ViewCount="18" Body="&lt;p&gt;I have a &lt;strong&gt;DataFrame&lt;/strong&gt; of the following format:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;item_id1: Long, item_id2: Long, similarity_score: Double&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;What I'm trying to do is to get top N highest similarity_score records for each item_id1.&#xA;So, for example:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1 2 0.5&#xA;1 3 0.4&#xA;1 4 0.3&#xA;2 1 0.5&#xA;2 3 0.4&#xA;2 4 0.3&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;With top 2 similar items would give:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;1 2 0.5&#xA;1 3 0.4&#xA;2 1 0.5&#xA;2 3 0.4&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I vaguely guess that it can be done by first grouping records by item_id1, then sorting in reverse by score and then limiting the results. But I'm stuck with how to implement it in Spark Scala.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Thank you.&lt;/p&gt;&#xA;" OwnerUserId="7179862" LastEditorUserId="7179862" LastEditDate="2018-03-09T19:11:37.400" LastActivityDate="2018-03-09T19:24:19.063" Title="Spark get top N highest score results for each (item1, item2, score)" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;spark-dataframe&gt;&lt;rdd&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49200863" PostTypeId="1" CreationDate="2018-03-09T19:29:50.443" Score="0" ViewCount="9" Body="&lt;p&gt;I have a number of &lt;strong&gt;Hive&lt;/strong&gt; files in &lt;strong&gt;parquet&lt;/strong&gt; format that contain both &lt;code&gt;string&lt;/code&gt; and &lt;code&gt;double&lt;/code&gt; columns. I can read most of them into a Spark Data Frame with &lt;code&gt;sparklyr&lt;/code&gt; using the syntax below:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark_read_parquet(sc, name = &quot;name&quot;, path = &quot;path&quot;, memory = FALSE)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However, I have one file that I read in where all of the &lt;code&gt;string&lt;/code&gt; values get converted to unrecognizable lists that looks like this when collected into an R Data Frame and printed:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;s_df &amp;lt;- spark_read_parquet(sc, &#xA;                           name = &quot;s_df&quot;, &#xA;                           path = &quot;hdfs://nameservice1/user/hive/warehouse/s_df&quot;, &#xA;                           memory = FALSE)&#xA;df &amp;lt;- collect(s_df)&#xA;head(df)&#xA;&#xA;# A tibble: 11,081 x 13&#xA;   provid   hospital_name servcode  servcode_desc codegroup claimid  amountpaid&#xA;   &amp;lt;list&amp;gt;   &amp;lt;list&amp;gt;        &amp;lt;list&amp;gt;    &amp;lt;list&amp;gt;        &amp;lt;list&amp;gt;    &amp;lt;list&amp;gt;        &amp;lt;dbl&amp;gt;&#xA; 1 &amp;lt;raw [8… &amp;lt;raw [32]&amp;gt;    &amp;lt;raw [5]&amp;gt; &amp;lt;raw [25]&amp;gt;    &amp;lt;raw [29… &amp;lt;raw [1…       7.41&#xA; 2 &amp;lt;raw [8… &amp;lt;raw [32]&amp;gt;    &amp;lt;raw [5]&amp;gt; &amp;lt;raw [15]&amp;gt;    &amp;lt;raw [22… &amp;lt;raw [1…       4.93&#xA; 3 &amp;lt;raw [8… &amp;lt;raw [32]&amp;gt;    &amp;lt;raw [5]&amp;gt; &amp;lt;raw [28]&amp;gt;    &amp;lt;raw [22… &amp;lt;raw [1…       5.36&#xA; 4 &amp;lt;raw [8… &amp;lt;raw [32]&amp;gt;    &amp;lt;raw [5]&amp;gt; &amp;lt;raw [28]&amp;gt;    &amp;lt;raw [30… &amp;lt;raw [1…       5.46&#xA; 5 &amp;lt;raw [8… &amp;lt;raw [32]&amp;gt;    &amp;lt;raw [5]&amp;gt; &amp;lt;raw [16]&amp;gt;    &amp;lt;raw [30… &amp;lt;raw [1…       2.80 &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The &lt;code&gt;hospital_name&lt;/code&gt; for the top 5 rows of &lt;code&gt;df&lt;/code&gt; should read &lt;code&gt;METHODIST HOSPITAL OF SOUTHERN CALIFORNIA&lt;/code&gt;, but are coming out like this instead:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;head(df$hospital_name)&#xA;&#xA;[[1]]&#xA; [1] 48 45 4e 52 59 20 4d 41 59 4f 20 4e 45 57 48 41 4c 4c 20 4d 45 4d 4f 52 49&#xA;[26] 41 4c 20 48 4f 53 50&#xA;&#xA;[[2]]&#xA; [1] 48 45 4e 52 59 20 4d 41 59 4f 20 4e 45 57 48 41 4c 4c 20 4d 45 4d 4f 52 49&#xA;[26] 41 4c 20 48 4f 53 50&#xA;&#xA;[[3]]&#xA; [1] 48 45 4e 52 59 20 4d 41 59 4f 20 4e 45 57 48 41 4c 4c 20 4d 45 4d 4f 52 49&#xA;[26] 41 4c 20 48 4f 53 50&#xA;&#xA;[[4]]&#xA; [1] 48 45 4e 52 59 20 4d 41 59 4f 20 4e 45 57 48 41 4c 4c 20 4d 45 4d 4f 52 49&#xA;[26] 41 4c 20 48 4f 53 50&#xA;&#xA;[[5]]&#xA; [1] 48 45 4e 52 59 20 4d 41 59 4f 20 4e 45 57 48 41 4c 4c 20 4d 45 4d 4f 52 49&#xA;[26] 41 4c 20 48 4f 53 50&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I tried the below solution, but it didn't work:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;head(df %&amp;gt;% mutate(hospital_name = as.character(hospital_name)))&#xA;&#xA;[1] &quot;as.raw(c(0x48, 0x45, 0x4e, 0x52, 0x59, 0x20, 0x4d, 0x41, 0x59, 0x4f, 0x20, 0x4e, 0x45, 0x57, 0x48, 0x41, 0x4c, 0x4c, 0x20, 0x4d, 0x45, 0x4d, 0x4f, 0x52, 0x49, 0x41, 0x4c, 0x20, 0x48, 0x4f, 0x53, 0x50))&quot;&#xA;[2] &quot;as.raw(c(0x48, 0x45, 0x4e, 0x52, 0x59, 0x20, 0x4d, 0x41, 0x59, 0x4f, 0x20, 0x4e, 0x45, 0x57, 0x48, 0x41, 0x4c, 0x4c, 0x20, 0x4d, 0x45, 0x4d, 0x4f, 0x52, 0x49, 0x41, 0x4c, 0x20, 0x48, 0x4f, 0x53, 0x50))&quot;&#xA;[3] &quot;as.raw(c(0x48, 0x45, 0x4e, 0x52, 0x59, 0x20, 0x4d, 0x41, 0x59, 0x4f, 0x20, 0x4e, 0x45, 0x57, 0x48, 0x41, 0x4c, 0x4c, 0x20, 0x4d, 0x45, 0x4d, 0x4f, 0x52, 0x49, 0x41, 0x4c, 0x20, 0x48, 0x4f, 0x53, 0x50))&quot;&#xA;[4] &quot;as.raw(c(0x48, 0x45, 0x4e, 0x52, 0x59, 0x20, 0x4d, 0x41, 0x59, 0x4f, 0x20, 0x4e, 0x45, 0x57, 0x48, 0x41, 0x4c, 0x4c, 0x20, 0x4d, 0x45, 0x4d, 0x4f, 0x52, 0x49, 0x41, 0x4c, 0x20, 0x48, 0x4f, 0x53, 0x50))&quot;&#xA;[5] &quot;as.raw(c(0x48, 0x45, 0x4e, 0x52, 0x59, 0x20, 0x4d, 0x41, 0x59, 0x4f, 0x20, 0x4e, 0x45, 0x57, 0x48, 0x41, 0x4c, 0x4c, 0x20, 0x4d, 0x45, 0x4d, 0x4f, 0x52, 0x49, 0x41, 0x4c, 0x20, 0x48, 0x4f, 0x53, 0x50))&quot;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I appreciate any help in being able to resolve the issue OR with any suggestions to make my request more clear. Thanks.&lt;/p&gt;&#xA;" OwnerUserId="4716625" LastEditorUserId="4716625" LastEditDate="2018-03-09T19:36:40.647" LastActivityDate="2018-03-09T19:36:40.647" Title="sparklyr spark_read_parquet Reading String Fields as Lists" Tags="&lt;r&gt;&lt;hive&gt;&lt;spark-dataframe&gt;&lt;parquet&gt;&lt;sparklyr&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49201146" PostTypeId="1" CreationDate="2018-03-09T19:48:59.087" Score="0" ViewCount="7" Body="&lt;p&gt;Spark application is deployed in Standalone cluster mode with supervise enabled.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;During high availability testing , when a rack with driver instance is powered off (ungracefully) , spark master didn't know about the killed driver and application and master continuously launch executors for the application for about 15 minutes .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;masterlogs (below is logged for 15 minutes)&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;2018-03-09 18:09:02 INFO  org.apache.spark.internal.Logging$class:54 - Launching executor app-20180309175053-0002/5202 on worker worker-20180309171520-10.247.247.191-51426&#xA;2018-03-09 18:09:02 INFO  org.apache.spark.internal.Logging$class:54 - Removing executor app-20180309175053-0002/5153 because it is EXITED&#xA;2018-03-09 18:09:02 INFO  org.apache.spark.internal.Logging$class:54 - Launching executor app-20180309175053-0002/5203 on worker worker-20180309171632-10.247.247.156-57784&#xA;2018-03-09 18:09:02 INFO  org.apache.spark.internal.Logging$class:54 - Removing executor app-20180309175053-0002/5155 because it is EXITED&#xA;2018-03-09 18:09:02 INFO  org.apache.spark.internal.Logging$class:54 - Launching executor app-20180309175053-0002/5204 on worker worker-20180309123802-10.247.247.121-45652&#xA;2018-03-09 18:09:02 INFO  org.apache.spark.internal.Logging$class:54 - Removing executor app-20180309175053-0002/5157 because it is EXITED&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;after 15th minute&lt;/strong&gt; &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;code&gt;2018-03-09 18:09:16 WARN  org.apache.spark.internal.Logging$class:66 - Got status update for unknown executor app-20180309175053-0002/5282&#xA;2018-03-09 18:09:16 WARN  org.apache.spark.internal.Logging$class:66 - Got status update for unknown executor app-20180309175053-0002/5295&#xA;2018-03-09 18:09:16 WARN  org.apache.spark.internal.Logging$class:66 - Got status update for unknown executor app-20180309175053-0002/5296&#xA;2018-03-09 18:09:16 WARN  org.apache.spark.internal.Logging$class:66 - Got status update for unknown executor app-20180309175053-0002/5289&#xA;2018-03-09 18:09:16 WARN  org.apache.spark.internal.Logging$class:66 - Got status update for unknown executor app-20180309175053-0002/5277&#xA;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;executor logs&lt;/strong&gt;&#xA;&lt;code&gt;&#xA;2018-03-09 18:50:17 INFO  org.apache.spark.internal.Logging$class:54 - Asked to kill executor app-20180309180931-0004/50&#xA;2018-03-09 18:50:17 INFO  org.apache.spark.internal.Logging$class:54 - Runner thread for executor app-20180309180931-0004/50 interrupted&#xA;2018-03-09 18:50:17 INFO  org.apache.spark.internal.Logging$class:54 - Killing process!&#xA;2018-03-09 18:50:17 INFO  org.apache.spark.internal.Logging$class:54 - Executor app-20180309180931-0004/50 finished with state KILLED exitStatus 143&#xA;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I checked in spark &lt;a href=&quot;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/master/Master.scala&quot; rel=&quot;nofollow noreferrer&quot;&gt;master&lt;/a&gt; code , could not find much in there .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help is appreciated , thanks .&lt;/p&gt;&#xA;" OwnerUserId="6101514" LastActivityDate="2018-03-09T19:48:59.087" Title="Spark master continuously launch executors for non-existing driver" Tags="&lt;apache-spark&gt;&lt;apache-spark-standalone&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49201278" PostTypeId="1" CreationDate="2018-03-09T19:58:40.110" Score="0" ViewCount="34" Body="&lt;p&gt;I am streaming data read from a folder stored on HDFS. I have the following small piece of code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;// Convert text into a DataSet of LogEntry rows. Select the two columns we care about&#xA;  val df = rawData.flatMap(parseLog).select(&quot;ip&quot;, &quot;status&quot;)&#xA;  df .isStreaming&#xA;&#xA;&#xA;  val kmeans = new KMeans().setK(2).setSeed(1L)&#xA;  val model = kmeans.fit(df)&#xA;&#xA;  // Evaluate clustering by computing Within Set Sum of Squared Errors.&#xA;  val WSSSE = model.computeCost(df)&#xA;  println(s&quot;Within Set Sum of Squared Errors = $WSSSE&quot;)&#xA;&#xA;  // Shows the K-means result&#xA;  println(&quot;Cluster Centers: &quot;)&#xA;  model.clusterCenters.foreach(println)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I run the above, I get the following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;java.lang.IllegalArgumentException: Field &quot;features&quot; does not exist.&#xA;  at org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:266)&#xA;  at org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:266)&#xA;  at scala.collection.MapLike$class.getOrElse(MapLike.scala:128)&#xA;  at scala.collection.AbstractMap.getOrElse(Map.scala:59)&#xA;  at org.apache.spark.sql.types.StructType.apply(StructType.scala:265)&#xA;  at org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:40)&#xA;  at org.apache.spark.ml.clustering.KMeansParams$class.validateAndTransformSchema(KMeans.scala:93)&#xA;  at org.apache.spark.ml.clustering.KMeans.validateAndTransformSchema(KMeans.scala:254)&#xA;  at org.apache.spark.ml.clustering.KMeans.transformSchema(KMeans.scala:340)&#xA;  at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)&#xA;  at org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:305)&#xA;  at StructuredStreaming$.main(&amp;lt;console&amp;gt;:189)&#xA;  ... 90 elided&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I am completely stumped on this&lt;/p&gt;&#xA;&#xA;&lt;p&gt;. Any assistance would be appreciated.&lt;/p&gt;&#xA;" OwnerUserId="2297683" LastActivityDate="2018-03-10T23:10:57.450" Title="Spark structured streaming 2.2 and k-means" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;k-means&gt;&lt;apache-spark-mllib&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49201341" PostTypeId="1" CreationDate="2018-03-09T20:02:57.263" Score="0" ViewCount="10" Body="&lt;h1&gt;Goal&lt;/h1&gt;&#xA;&#xA;&lt;p&gt;I'm trying to get DEAP to parallelize across a spark cluster. I have seen this referenced by other users, as it allows for tight integration with existing server architecture easily via yarn. I have followed several tutorials online cited in the references. I have working code for deap, and then code that I have attempted to transform to use spark. The same error 'Can't get attribute Individual on module deap.creator' is the one to usually occur. &lt;/p&gt;&#xA;&#xA;&lt;h1&gt;Working Code&lt;/h1&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import numpy as np&#xA;import random&#xA;&#xA;from deap import base&#xA;from deap import creator&#xA;from deap import tools&#xA;from deap import algorithms&#xA;&#xA;creator.create(&quot;FitnessMax&quot;, base.Fitness, weights=(1.0,))&#xA;creator.create(&quot;Individual&quot;, list, fitness=creator.FitnessMax)&#xA;&#xA;def evalOneMax(individual):&#xA;    return sum(individual),&#xA;&#xA;toolbox = base.Toolbox()&#xA;toolbox.register(&quot;attr_bool&quot;, random.randint, 0, 1)&#xA;toolbox.register(&quot;individual&quot;, tools.initRepeat, creator.Individual,&#xA;    toolbox.attr_bool, 100)&#xA;toolbox.register(&quot;population&quot;, tools.initRepeat, list, toolbox.individual)&#xA;toolbox.register(&quot;evaluate&quot;, evalOneMax)&#xA;toolbox.register(&quot;mate&quot;, tools.cxTwoPoint)&#xA;toolbox.register(&quot;mutate&quot;, tools.mutFlipBit, indpb=0.05)&#xA;toolbox.register(&quot;select&quot;, tools.selTournament, tournsize=3)&#xA;&#xA;# Define parallelism outside main&#xA;if __name__==&quot;__main__&quot;:&#xA;&#xA;    pop = toolbox.population(n=300)&#xA;    hof = tools.HallOfFame(1)&#xA;    stats = tools.Statistics(lambda ind: ind.fitness.values)&#xA;    stats.register(&quot;avg&quot;, np.mean)&#xA;    stats.register(&quot;std&quot;, np.std)&#xA;    stats.register(&quot;min&quot;, np.min)&#xA;    stats.register(&quot;max&quot;, np.max)&#xA;&#xA;    pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=40,&#xA;                                   stats=stats, halloffame=hof, verbose=True)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ran:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;python3 test-deap.py&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Output&lt;/h2&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;gen nevals  avg     std     min max&#xA;0   300     50.5533 4.78893 38  63&#xA;1   184     54.2433 3.85627 44  65&#xA;2   191     57.2667 3.38756 47  65&#xA;3   182     60.0333 3.15154 51  69&#xA;4   179     62.35   2.95762 53  71&#xA;5   176     64.3267 2.77968 55  73&#xA;6   183     66.2467 2.80223 58  75&#xA;7   182     68.1733 2.66019 59  79&#xA;8   186     69.8367 2.83255 62  77&#xA;9   179     71.7233 2.70557 59  78&#xA;10  187     73.3667 2.63165 63  80&#xA;11  169     74.7933 2.43255 65  80&#xA;12  182     76.0967 2.42363 65  82&#xA;13  204     77.3    2.36995 67  83&#xA;14  203     78.6267 2.31818 70  84&#xA;15  182     79.8933 2.29535 72  84&#xA;16  183     81.02   2.29483 72  86&#xA;17  185     81.87   2.41242 73  87&#xA;18  190     83.0633 2.13057 74  87&#xA;19  182     84.06   2.16096 75  89&#xA;20  194     84.8167 2.41724 77  91&#xA;21  174     85.9633 2.22755 79  91&#xA;22  180     86.8033 2.23263 79  92&#xA;23  177     87.7533 2.3831  78  93&#xA;24  174     88.61   2.34334 79  93&#xA;25  171     89.6167 2.36144 78  95&#xA;26  195     90.57   2.4695  81  95&#xA;27  169     91.5233 2.23072 82  96&#xA;28  173     92.3733 2.16347 83  97&#xA;29  203     93.1    2.13151 85  97&#xA;30  179     93.6067 2.41356 84  98&#xA;31  169     94.3067 2.23293 86  99&#xA;32  184     94.8933 2.49706 85  99&#xA;33  175     95.8733 2.14413 88  99&#xA;34  168     96.2167 2.30428 88  99&#xA;35  173     96.88   2.22537 87  100&#xA;36  171     97.33   2.29951 87  100&#xA;37  184     97.89   2.05375 91  100&#xA;38  175     98.0333 2.52565 88  100&#xA;39  176     98.6667 2.07579 90  100&#xA;40  175     98.6867 2.32562 91  100&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h1&gt;Not Working Code&lt;/h1&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark import SparkContext&#xA;&#xA;import numpy as np&#xA;import random&#xA;&#xA;from deap import base&#xA;from deap import creator&#xA;from deap import tools&#xA;from deap import algorithms&#xA;&#xA;creator.create(&quot;FitnessMax&quot;, base.Fitness, weights=(1.0,))&#xA;creator.create(&quot;Individual&quot;, list, fitness=creator.FitnessMax)&#xA;&#xA;def evalOneMax(individual):&#xA;    return sum(individual),&#xA;&#xA;toolbox = base.Toolbox()&#xA;toolbox.register(&quot;attr_bool&quot;, random.randint, 0, 1)&#xA;toolbox.register(&quot;individual&quot;, tools.initRepeat, creator.Individual,&#xA;    toolbox.attr_bool, 100)&#xA;toolbox.register(&quot;population&quot;, tools.initRepeat, list, toolbox.individual)&#xA;toolbox.register(&quot;evaluate&quot;, evalOneMax)&#xA;toolbox.register(&quot;mate&quot;, tools.cxTwoPoint)&#xA;toolbox.register(&quot;mutate&quot;, tools.mutFlipBit, indpb=0.05)&#xA;toolbox.register(&quot;select&quot;, tools.selTournament, tournsize=3)&#xA;&#xA;# Define parallelism outside main&#xA;if __name__==&quot;__main__&quot;:&#xA;    sc = SparkContext(appName=&quot;DEAP&quot;)&#xA;&#xA;    def sparkMap(algorithm, population):&#xA;        return sc.parallelize(population).map(algorithm).collect()&#xA;&#xA;    toolbox.register(&quot;map&quot;, sparkMap)&#xA;&#xA;    pop = toolbox.population(n=300)&#xA;    hof = tools.HallOfFame(1)&#xA;    stats = tools.Statistics(lambda ind: ind.fitness.values)&#xA;    stats.register(&quot;avg&quot;, np.mean)&#xA;    stats.register(&quot;std&quot;, np.std)&#xA;    stats.register(&quot;min&quot;, np.min)&#xA;    stats.register(&quot;max&quot;, np.max)&#xA;&#xA;    pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=40,&#xA;                                   stats=stats, halloffame=hof, verbose=True)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Ran:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;spark-submit --master local test-deap.py&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h2&gt;Output&lt;/h2&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://pastebin.com/WPDnKHuE&quot; rel=&quot;nofollow noreferrer&quot;&gt;Full Text (pastebin)&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Traceback (most recent call last):&#xA;  File &quot;/Users/ryapeach/Documents/Workspace/relay-death/test-deap.py&quot;, line 45, in &amp;lt;module&amp;gt;&#xA;    stats=stats, halloffame=hof, verbose=True)&#xA;  File &quot;/usr/local/lib/python3.6/site-packages/deap-1.2.2-py3.6-macosx-10.13-x86_64.egg/deap/algorithms.py&quot;, line 150, in eaSimple&#xA;    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)&#xA;  File &quot;/Users/ryapeach/Documents/Workspace/relay-death/test-deap.py&quot;, line 32, in sparkMap&#xA;    return sc.parallelize(population).map(algorithm).collect()&#xA;AttributeError: Can't get attribute 'Individual' on &amp;lt;module 'deap.creator' from '/usr/local/lib/python3.6/site-packages/deap-1.2.2-py3.6-macosx-10.13-x86_64.egg/deap/creator.py'&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;h1&gt;References:&lt;/h1&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://deap.readthedocs.io/en/master/examples/ga_onemax.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;DEAP 1.2.2 documentation » Examples » One Max Problem&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://deap.readthedocs.io/en/master/examples/ga_onemax_short.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;DEAP 1.2.2 documentation » Examples » One Max Problem: Short Version&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/45607035/using-deap-genetic-algorithm-library-with-spark&quot;&gt;Using DEAP (genetic algorithm library) with spark&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;http://deap.readthedocs.io/en/master/tutorials/basic/part4.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;DEAP 1.2.2 documentation » Using Multiple Processors&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&quot;https://github.com/DEAP/deap/issues/57&quot; rel=&quot;nofollow noreferrer&quot;&gt;Can't run DEAP program with SCOOP #57&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;" OwnerUserId="799723" LastActivityDate="2018-03-09T20:02:57.263" Title="Spark + Deap Examples Not Working" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;genetic-algorithm&gt;&lt;deap&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49201436" PostTypeId="1" CreationDate="2018-03-09T20:10:40.480" Score="1" ViewCount="17" Body="&#xA;&#xA;&lt;p&gt;I'm am having issues with the schema for Hive tables being out of sync between Spark and Hive on a Mapr cluster with Spark 2.1.0 and Hive 2.1.1.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I need to try to resolve this problem specifically for managed tables, but the issue can be reproduced with unmanaged/external tables.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Overview of Steps&lt;/h3&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Use &lt;code&gt;saveAsTable&lt;/code&gt; to save a dataframe to a given table.&lt;/li&gt;&#xA;&lt;li&gt;Use &lt;code&gt;mode(&quot;overwrite&quot;).parquet(&quot;path/to/table&quot;)&lt;/code&gt; to overwrite the data for the previously saved table. I am actually modifying the data through a process external to Spark and Hive, but this reproduces the same issue.&lt;/li&gt;&#xA;&lt;li&gt;Use &lt;code&gt;spark.catalog.refreshTable(...)&lt;/code&gt; to refresh metadata&lt;/li&gt;&#xA;&lt;li&gt;Query the table with &lt;code&gt;spark.table(...).show()&lt;/code&gt;. Any columns that were the same between the original dataframe and the overwriting one will show the new data correctly, but any columns that were only in the new table will not be displayed.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;h3&gt;Example&lt;/h3&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;db_name = &quot;test_39d3ec9&quot;&#xA;table_name = &quot;overwrite_existing&quot;&#xA;table_location = &quot;&amp;lt;spark.sql.warehouse.dir&amp;gt;/{}.db/{}&quot;.format(db_name, table_name)&#xA;&#xA;qualified_table = &quot;{}.{}&quot;.format(db_name, table_name)&#xA;spark.sql(&quot;CREATE DATABASE IF NOT EXISTS {}&quot;.format(db_name))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Save as a managed table&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;existing_df = spark.createDataFrame([(1, 2)])&#xA;existing_df.write.mode(&quot;overwrite&quot;).saveAsTable(table_name)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Note that saving as an unmanaged table with the following will produce the same issue:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;existing_df.write.mode(&quot;overwrite&quot;) \&#xA;    .option(&quot;path&quot;, table_location) \&#xA;    .saveAsTable(qualified_table)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;View the contents of the table&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;spark.table(table_name).show()&#xA;+---+---+&#xA;| _1| _2|&#xA;+---+---+&#xA;|  1|  2|&#xA;+---+---+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Overwrite the parquet files directly&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;new_df = spark.createDataFrame([(3, 4, 5, 6)], [&quot;_4&quot;, &quot;_3&quot;, &quot;_2&quot;, &quot;_1&quot;])&#xA;new_df.write.mode(&quot;overwrite&quot;).parquet(table_location)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;View the contents with the parquet reader, the contents show correctly&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;spark.read.parquet(table_location).show()&#xA;+---+---+---+---+&#xA;| _4| _3| _2| _1|&#xA;+---+---+---+---+&#xA;|  3|  4|  5|  6|&#xA;+---+---+---+---+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Refresh spark's metadata for the table and read in again as a table. The data will be updated for the columns that were the same, but the additional columns do not display.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;spark.catalog.refreshTable(qualified_table)&#xA;spark.table(qualified_table).show()&#xA;+---+---+&#xA;| _1| _2|&#xA;+---+---+&#xA;|  6|  5|&#xA;+---+---+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have also tried updating the schema in hive before calling &lt;code&gt;spark.catalog.refreshTable&lt;/code&gt; with the below command in the hive shell:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;ALTER TABLE test_39d3ec9.overwrite_existing REPLACE COLUMNS (`_1` bigint, `_2` bigint, `_3` bigint, `_4` bigint);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;After running the ALTER command I then run describe and it shows correctly in hive&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;DESCRIBE test_39d3ec9.overwrite_existing&#xA;OK&#xA;_1                      bigint&#xA;_2                      bigint&#xA;_3                      bigint&#xA;_4                      bigint&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Before running the alter command it only shows the original columns as expected&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;DESCRIBE test_39d3ec9.overwrite_existing&#xA;OK&#xA;_1                      bigint&#xA;_2                      bigint&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I then ran &lt;code&gt;spark.catalog.refreshTable&lt;/code&gt; but it didn't effect spark's view of the data.&lt;/p&gt;&#xA;&#xA;&lt;h3&gt;Additional Notes&lt;/h3&gt;&#xA;&#xA;&lt;p&gt;From the spark side, I did most of my testing with PySpark, but also tested in a spark-shell (scala) and a sparksql shell. While in the spark shell I also tried using a &lt;code&gt;HiveContext&lt;/code&gt; but didn't work.&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-python prettyprint-override&quot;&gt;&lt;code&gt;import org.apache.spark.sql.hive.HiveContext&#xA;import spark.sqlContext.implicits._&#xA;val hiveObj = new HiveContext(sc)&#xA;hiveObj.refreshTable(&quot;test_39d3ec9.overwrite_existing&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;After performing the ALTER command in the hive shell, I verified in Hue that the schema also changed there.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I also tried running the ALTER command with &lt;code&gt;spark.sql(&quot;ALTER ...&quot;)&lt;/code&gt; but the version of Spark we are on (2.1.0) does not allow it, and looks like it won't be available until Spark 2.2.0 based on this issue: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-19261&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://issues.apache.org/jira/browse/SPARK-19261&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have also read through the spark docs again, specifically this section: &lt;a href=&quot;https://spark.apache.org/docs/2.1.0/sql-programming-guide.html#hive-metastore-parquet-table-conversion&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://spark.apache.org/docs/2.1.0/sql-programming-guide.html#hive-metastore-parquet-table-conversion&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Based on those docs, &lt;code&gt;spark.catalog.refreshTable&lt;/code&gt; should work. The configuration for &lt;code&gt;spark.sql.hive.convertMetastoreParquet&lt;/code&gt; is typically &lt;code&gt;false&lt;/code&gt;, but I switched it to &lt;code&gt;true&lt;/code&gt; for testing and it didn't seem to effect anything.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any help would be appreciated, thank you!&lt;/p&gt;&#xA;" OwnerUserId="3723796" LastEditorUserId="5858851" LastEditDate="2018-03-09T20:17:13.763" LastActivityDate="2018-03-09T20:17:13.763" Title="Spark and Hive table schema out of sync after external overwrite" Tags="&lt;apache-spark&gt;&lt;hive&gt;&lt;pyspark&gt;&lt;mapr&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49201829" PostTypeId="1" CreationDate="2018-03-09T20:41:51.700" Score="0" ViewCount="11" Body="&lt;p&gt;I am running a Logistic Regression with the following code given at &lt;a href=&quot;https://spark.apache.org/docs/2.2.0/ml-pipeline.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://spark.apache.org/docs/2.2.0/ml-pipeline.html&lt;/a&gt; (Example: Pipeline)&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Original Code from Link...  &lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;div class=&quot;snippet&quot; data-lang=&quot;js&quot; data-hide=&quot;false&quot; data-console=&quot;true&quot; data-babel=&quot;false&quot;&gt;&#xD;&#xA;&lt;div class=&quot;snippet-code&quot;&gt;&#xD;&#xA;&lt;pre class=&quot;snippet-code-css lang-css prettyprint-override&quot;&gt;&lt;code&gt;import org.apache.spark.ml.{Pipeline, PipelineModel}&#xD;&#xA;import org.apache.spark.ml.classification.LogisticRegression&#xD;&#xA;import org.apache.spark.ml.feature.{HashingTF, Tokenizer}&#xD;&#xA;import org.apache.spark.ml.linalg.Vector&#xD;&#xA;import org.apache.spark.sql.Row&#xD;&#xA;&#xD;&#xA;val training = spark.createDataFrame(Seq(&#xD;&#xA;    (0L, &quot;a b c d e spark&quot;, 1.0),&#xD;&#xA;    (1L, &quot;b d&quot;, 0.0),&#xD;&#xA;    (2L, &quot;spark f g h&quot;, 1.0),&#xD;&#xA;    (3L, &quot;hadoop mapreduce&quot;, 0.0)&#xD;&#xA;  )).toDF(&quot;id&quot;, &quot;text&quot;, &quot;label&quot;)&#xD;&#xA;  val test = spark.createDataFrame(Seq(&#xD;&#xA;    (4L, &quot;spark i j k&quot;),&#xD;&#xA;    (5L, &quot;l m n&quot;),&#xD;&#xA;    (6L, &quot;spark hadoop spark&quot;),&#xD;&#xA;    (7L, &quot;apache hadoop&quot;)&#xD;&#xA;  )).toDF(&quot;id&quot;, &quot;text&quot;)&#xD;&#xA;&#xD;&#xA;  // Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.&#xD;&#xA;  val tokenizer = new Tokenizer().setInputCol(&quot;text&quot;).setOutputCol(&quot;words&quot;)&#xD;&#xA;  val hashingTF = new HashingTF().setNumFeatures(1000).setInputCol(tokenizer.getOutputCol).setOutputCol(&quot;features&quot;)&#xD;&#xA;  val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.001)&#xD;&#xA;  val pipeline = new Pipeline().setStages(Array(tokenizer, hashingTF,lr))&#xD;&#xA;  val model = pipeline.fit(training)&#xD;&#xA;  model.transform(test).show()&lt;/code&gt;&lt;/pre&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;There are two things happening - Can someone help explain&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;p&gt;If I reduce the setNumFeatures to 10. i.e. setNumFeatures(10), then the algorithm predicts id 5 in the test to be 1. I was thinking that this might be because of hashing collision.&lt;/p&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;p&gt;When i change my code to word2vec instead of hashingTF&lt;/p&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;div class=&quot;snippet&quot; data-lang=&quot;js&quot; data-hide=&quot;false&quot; data-console=&quot;false&quot; data-babel=&quot;false&quot;&gt;&#xD;&#xA;&lt;div class=&quot;snippet-code&quot;&gt;&#xD;&#xA;&lt;pre class=&quot;snippet-code-js lang-js prettyprint-override&quot;&gt;&lt;code&gt;val tokenizer = new Tokenizer(&#xD;&#xA;).setInputCol(&quot;text&quot;&#xD;&#xA;).setOutputCol(&quot;words&quot;)&#xD;&#xA;&#xD;&#xA;val word2Vec = new Word2Vec(&#xD;&#xA;).setInputCol(tokenizer.getOutputCol&#xD;&#xA;).setOutputCol(&quot;features&quot;).setVectorSize(1000).setMinCount(0)&#xD;&#xA;&#xD;&#xA;val lr = new LogisticRegression(&#xD;&#xA;).setMaxIter(10).setRegParam(0.001)&#xD;&#xA;&#xD;&#xA;val pipeline = new Pipeline(&#xD;&#xA;).setStages(Array(tokenizer, word2Vec,lr))&#xD;&#xA;&#xD;&#xA;val model = pipeline.fit(training)&#xD;&#xA;&#xD;&#xA;model.transform(test).show()&lt;/code&gt;&lt;/pre&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This also gives me id 5 prediction as 1, even at VectorSize 1000. I also noticed that the column &quot;features&quot; is all zeros for id = 5. When i change the test data to the following it predicts correctly&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;div class=&quot;snippet&quot; data-lang=&quot;js&quot; data-hide=&quot;false&quot; data-console=&quot;true&quot; data-babel=&quot;false&quot;&gt;&#xD;&#xA;&lt;div class=&quot;snippet-code&quot;&gt;&#xD;&#xA;&lt;pre class=&quot;snippet-code-js lang-js prettyprint-override&quot;&gt;&lt;code&gt;val test = spark.createDataFrame(Seq(&#xD;&#xA;    (4L, &quot;spark i j k&quot;),&#xD;&#xA;    (5L, &quot;l d&quot;),&#xD;&#xA;    (6L, &quot;spark hadoop spark&quot;),&#xD;&#xA;    (7L, &quot;apache hadoop&quot;)&#xD;&#xA;  )).toDF(&quot;id&quot;, &quot;text&quot;)&lt;/code&gt;&lt;/pre&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;/div&gt;&#xD;&#xA;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Questions&#xA;1. What is the best way to run a logisticRegression in an situation where my test data might not contain words in the train data.&#xA;2. In a case like this would hashingTF be better than word2vec&#xA;3. What is the logic to set - setNumFeatures and setVectorSize&lt;/p&gt;&#xA;" OwnerUserId="9469339" LastActivityDate="2018-03-09T20:41:51.700" Title="Spark 2.x - Running Logistic with word2vec or HashingTF" Tags="&lt;apache-spark&gt;&lt;logistic-regression&gt;&lt;word2vec&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49202485" PostTypeId="1" CreationDate="2018-03-09T21:35:57.617" Score="0" ViewCount="12" Body="&lt;p&gt;I'm hitting a GC overhead limit exceeded error in Spark using &lt;code&gt;spark_apply&lt;/code&gt;. Here are my specs:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;sparklyr v0.6.2&#xA;Spark    v2.1.0&#xA;4 workers with 8 cores and 29G of memory&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The closure &lt;code&gt;get_dates&lt;/code&gt; pulls data from Cassandra one row at a time. There are about 200k rows total. The process run for about an hour and a half and then given me this memory error. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've experimented with &lt;code&gt;spark.driver.memory&lt;/code&gt; which is supposed to increase the heap size, but it's not working. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any ideas? Usage below &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; config &amp;lt;- spark_config()&#xA;&amp;gt; config$spark.executor.cores = 1 # this ensures a max of 32 separate executors&#xA;&amp;gt; config$spark.cores.max = 26 # this ensures that cassandra gets some resources too, not all to spark&#xA;&amp;gt; config$spark.driver.memory = &quot;4G&quot;&#xA;&amp;gt; config$spark.driver.memoryOverhead = &quot;10g&quot; &#xA;&amp;gt; config$spark.executor.memory = &quot;4G&quot;&#xA;&amp;gt; config$spark.executor.memoryOverhead = &quot;1g&quot;&#xA;&amp;gt; sc &amp;lt;- spark_connect(master = &quot;spark://ip-10-0-0-57.us-west-2.compute.internal:7077&quot;,&#xA;+                     config = config)&#xA;&amp;gt; accounts &amp;lt;- sdf_copy_to(sc, insight %&amp;gt;%&#xA;+                           # slice(1:100) %&amp;gt;% &#xA;+                           {.}, &quot;accounts&quot;, overwrite=TRUE)&#xA;&amp;gt; accounts &amp;lt;- accounts %&amp;gt;% sdf_repartition(78)&#xA;&amp;gt; dag &amp;lt;- spark_apply(accounts, get_dates, group_by = c(&quot;row&quot;), &#xA;+                    columns = list(row = &quot;integer&quot;,&#xA;+                                   last_update_by = &quot;character&quot;,&#xA;+                                   last_end_time = &quot;character&quot;,&#xA;+                                   read_val = &quot;numeric&quot;,&#xA;+                                   batch_id = &quot;numeric&quot;,&#xA;+                                   fail_reason = &quot;character&quot;,&#xA;+                                   end_time = &quot;character&quot;,&#xA;+                                   meas_type = &quot;character&quot;,&#xA;+                                   svcpt_id = &quot;numeric&quot;,&#xA;+                                   org_id = &quot;character&quot;,&#xA;+                                   last_update_date = &quot;character&quot;,&#xA;+                                   validation_status = &quot;character&quot;&#xA;+                                   ))&#xA;&amp;gt; peak_usage &amp;lt;- dag %&amp;gt;% collect  &#xA;Error: java.lang.OutOfMemoryError: GC overhead limit exceeded&#xA;    at org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:260)&#xA;    at org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:254)&#xA;    at scala.collection.Iterator$class.foreach(Iterator.scala:743)&#xA;    at org.apache.spark.sql.execution.SparkPlan$$anon$1.foreach(SparkPlan.scala:254)&#xA;    at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeCollect$1.apply(SparkPlan.scala:276)&#xA;    at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeCollect$1.apply(SparkPlan.scala:275)&#xA;    at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)&#xA;    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)&#xA;    at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:275)&#xA;    at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)&#xA;    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)&#xA;    at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)&#xA;    at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)&#xA;    at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2375)&#xA;    at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2375)&#xA;    at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2778)&#xA;    at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2375)&#xA;    at org.apache.spark.sql.Dataset.collect(Dataset.scala:2351)&#xA;    at sparklyr.Utils$.collect(utils.scala:196)&#xA;    at sparklyr.Utils.collect(utils.scala)&#xA;    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&#xA;    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&#xA;    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&#xA;    at java.lang.reflect.Method.invoke(Method.java:498)&#xA;    at sparklyr.Invoke$.invoke(invoke.scala:102)&#xA;    at sparklyr.StreamHandler$.handleMethodCall(stream.scala:97)&#xA;    at sparklyr.StreamHandler$.read(stream.scala:62)&#xA;    at sparklyr.BackendHandler.channelRead0(handler.scala:52)&#xA;    at sparklyr.BackendHandler.channelRead0(handler.scala:14)&#xA;    at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)&#xA;    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:367)&#xA;    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:353)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="4373061" LastActivityDate="2018-03-09T21:35:57.617" Title="sparklyr failing with java.lang.OutOfMemoryError: GC overhead limit exceeded" Tags="&lt;java&gt;&lt;apache-spark&gt;&lt;sparklyr&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49203014" PostTypeId="1" CreationDate="2018-03-09T22:16:49.503" Score="0" ViewCount="17" Body="&lt;p&gt;In &lt;code&gt;MySQL&lt;/code&gt;, we can query the table &lt;code&gt;information_schema.tables&lt;/code&gt; and obtain useful information such as &lt;code&gt;data_length&lt;/code&gt; or &lt;code&gt;table_rows&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;select&#xA;  data_length&#xA;  , table_rows&#xA;from&#xA;  information_schema.tables&#xA;where  &#xA;  table_schema='some_db'&#xA;  and table_name='some_table';&#xA;&#xA;+-------------+------------+&#xA;| data_length | table_rows |&#xA;+-------------+------------+&#xA;|        8368 |        198 |&#xA;+-------------+------------+&#xA;1 row in set (0.01 sec)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Is there an equivalent mechanism for SparkSQL/Hive?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am okay to use &lt;code&gt;SparkSQL&lt;/code&gt; or program API like &lt;code&gt;HiveMetaStoreClient&lt;/code&gt; (java API &lt;code&gt;org.apache.hadoop.hive.metastore.HiveMetaStoreClient&lt;/code&gt;).  For the latter I read the API doc (&lt;a href=&quot;https://hive.apache.org/javadocs/r2.1.1/api/index.html?org/apache/hadoop/hive/metastore/HiveMetaStoreClient.html&quot; rel=&quot;nofollow noreferrer&quot;&gt;here&lt;/a&gt;) and could not find any method related to table row numbers and sizes.&lt;/p&gt;&#xA;" OwnerUserId="1168041" LastEditorUserId="2748004" LastEditDate="2018-03-10T05:13:22.200" LastActivityDate="2018-03-10T05:13:22.200" Title="SparkSQL/Hive: equivalent of MySQL's `information_schema.table.{data_length, table_rows}`?" Tags="&lt;apache-spark&gt;&lt;hive&gt;&lt;apache-spark-sql&gt;&lt;hiveql&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49203088" PostTypeId="1" CreationDate="2018-03-09T22:23:58.013" Score="0" ViewCount="12" Body="&lt;p&gt;I'm trying to run a Spark streaming app from my local to connect to an S3 bucket and am running into a &lt;code&gt;SocketTimeoutException&lt;/code&gt;. This is the code to read from the bucket:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val sc: SparkContext = createSparkContext(scName)&#xA;val hadoopConf=sc.hadoopConfiguration&#xA;hadoopConf.set(&quot;fs.s3a.impl&quot;, &quot;org.apache.hadoop.fs.s3a.S3AFileSystem&quot;)&#xA;val ssc = new StreamingContext(sc, Seconds(time))&#xA;val lines = ssc.textFileStream(&quot;s3a://foldername/subfolder/&quot;)&#xA;lines.print()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is the error I get:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;com.amazonaws.http.AmazonHttpClient executeHelper - Unable to execute HTTP request: connect timed out&#xA;java.net.SocketTimeoutException: connect timed out&#xA;at java.net.PlainSocketImpl.socketConnect(Native Method)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I thought it might be due to the proxy so I ran my spark-submit with the proxy options like so:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    spark-submit --conf &quot;spark.driver.extraJavaOptions=&#xA;-Dhttps.proxyHost=proxyserver.com -Dhttps.proxyPort=9000&quot; &#xA;--class application.jar s3module 5 5 SampleApp&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;That still gave me the same error. Perhaps I'm not setting the proxy properly? Is there a way to set it in the code in SparkContext's conf?&lt;/p&gt;&#xA;" OwnerUserId="3004041" LastActivityDate="2018-03-10T10:01:24.820" Title="Spark streaming connecting to S3 gives socket timeout" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;amazon-s3&gt;&lt;proxy&gt;&lt;spark-streaming&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49203390" PostTypeId="1" CreationDate="2018-03-09T22:55:27.350" Score="0" ViewCount="20" Body="&lt;p&gt;I am using Spark sql dataframes to perform a groupby operation and then compute the mean and median of data for each group. The original amount of data is about 1 terabyte.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val df_result = df.filter($&quot;DayOfWeek&quot; &amp;lt;= 5).groupBy(&quot;id&quot;).agg(&#xA;        count(&quot;Error&quot;).as(&quot;Count&quot;), &#xA;        avg(&quot;Error&quot;).as(&quot;MeanError&quot;), &#xA;        callUDF(&quot;percentile_approx&quot;, col(&quot;Error&quot;), lit(0.05)).as(&quot;5thError&quot;), &#xA;        callUDF(&quot;percentile_approx&quot;, col(&quot;Error&quot;), lit(0.5)).as(&quot;MedianError&quot;), &#xA;        callUDF(&quot;percentile_approx&quot;, col(&quot;Error&quot;), lit(0.95)).as(&quot;95thError&quot;)).&#xA;    filter($&quot;Count&quot; &amp;gt; 1000)&#xA;&#xA;&#xA;df_result.orderBy(asc(&quot;MeanError&quot;)).limit(5000)&#xA;    .write.format(&quot;csv&quot;).option(&quot;header&quot;, &quot;true&quot;).save(&quot;/user/foo.bar/result.csv&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;When I run that query, my job gets stuck and does not complete. How do I go about debugging the problem? Is there a key imbalance that causes the &lt;code&gt;groupby()&lt;/code&gt; to get stuck?&lt;/p&gt;&#xA;" OwnerUserId="4561314" LastActivityDate="2018-03-09T22:55:27.350" Title="Spark dataframe groupby mean and median does not complete" Tags="&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;spark-dataframe&gt;" AnswerCount="0" CommentCount="3" />
  <row Id="49203446" PostTypeId="1" CreationDate="2018-03-09T23:01:15.830" Score="-2" ViewCount="15" Body="&lt;p&gt;I'm new to Scala and Spark but I'm working on &quot;POCing&quot; Structured Streaming and one of the available API class is DataStreamReader accessed by a sample code below (&lt;strong&gt;.readStream&lt;/strong&gt; returns DataStreamReader) &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val spark: SparkSession = ...&#xA;// Read text from socket&#xA;val socketDF = spark&#xA;  .readStream&#xA;  .format(&quot;socket&quot;)&#xA;  .option(&quot;host&quot;, &quot;localhost&quot;)&#xA;  .option(&quot;port&quot;, 9999)&#xA;  .load()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I've tried looking at the &lt;a href=&quot;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader&quot; rel=&quot;nofollow noreferrer&quot;&gt;DataStreamReader&lt;/a&gt; API but it doesn't tell you any of the available parameters, am I missing something? Or let alone all the value member for &lt;strong&gt;.option&lt;/strong&gt; where do I find these information?&lt;/p&gt;&#xA;" OwnerUserId="7293348" LastActivityDate="2018-03-10T04:51:52.473" Title="Options for Scala DataStreamReader Class 'format' Value Member" Tags="&lt;scala&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49203638" PostTypeId="1" CreationDate="2018-03-09T23:22:56.477" Score="0" ViewCount="11" Body="&lt;p&gt;At one point using EMR 5.2.1 (which I have stuck with for over a year and a  half now) I was able to write to a postgres db from within Spark using:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    try:&#xA;        df = self._spark.createDataFrame([Row(id=str(self.uuid),&#xA;                                              datetime=datetime.now(),&#xA;                                              metadata=json.dumps(self._metadata))])&#xA;&#xA;        df.write.jdbc(url, table, properties={&quot;driver&quot;: &quot;org.postgresql.Driver&quot;})&#xA;        return self&#xA;    except AttributeError as e:&#xA;        logging.error(e)&#xA;    except ReferenceError as e:&#xA;        logging.error(e)&#xA;    except ValueError as e:&#xA;        logging.error(e)&#xA;    except Py4JJavaError as e:&#xA;        logging.error(e)&#xA;    except IllegalArgumentException as e:&#xA;        logging.error(e)&#xA;    return None&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This is not working on EMR 5.12.0 and I can't figure out what the problem is. I have looked at this summary of JDBC/PySpark but don't see any obvious answer there:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/30983982/how-to-use-jdbc-source-to-write-and-read-data-in-pyspark&quot;&gt;How to use JDBC source to write and read data in (Py)Spark?&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Currently using &lt;code&gt;postgresql-9.4.1210.jre7.jar&lt;/code&gt;. Any suggestions?&lt;/p&gt;&#xA;" OwnerUserId="1245418" LastActivityDate="2018-03-09T23:22:56.477" Title="Spark 2.2.x issues with jdbc writing to database in Amazon EMR 5.12.0" Tags="&lt;postgresql&gt;&lt;apache-spark&gt;&lt;jdbc&gt;&lt;pyspark&gt;&lt;amazon-emr&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49204024" PostTypeId="1" CreationDate="2018-03-10T00:15:33.327" Score="0" ViewCount="32" Body="&lt;p&gt;Is there something like an &lt;strong&gt;eval&lt;/strong&gt; function equivalent in PySpark.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am trying to convert Python code into PySpark&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am Querying a Dataframe and one of the Column has the Data as shown below but in &lt;strong&gt;String Format&lt;/strong&gt;.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[{u'date': u'2015-02-08', u'by': u'abc@gg.com', u'value': u'NA'}, {u'date': u'2016-02-08', u'by': u'dfg@yaa.com', u'value': u'applicable'}, {u'date': u'2017-02-08', u'by': u'wrwe@hot.com', u'value': u'ufc'}]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Assume that 'x' is the column which holds this value in the Dataframe.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Now i want to pass in that String column 'x' and get the List so that i can pass it to mapPartition function.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to avoid iterating to each row on my Driver that's the reason i am thinking this way.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;In Python using eval() function if used: I get below output:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;x = &quot;[{u'date': u'2015-02-08', u'by': u'abc@gg.com', u'value': u'NA'}, {u'date': u'2016-02-08', u'by': u'dfg@yaa.com', u'value': u'applicable'}, {u'date': u'2017-02-08', u'by': u'wrwe@hot.com', u'value': u'ufc'}]&quot;&#xA;&#xA;list = eval(x)&#xA;&#xA;for i in list:  print i&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Output: (This is what i want in PySpark as well)&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{u'date': u'2015-02-08', u'by': u'abc@gg.com', u'value': u'NA'}&#xA;{u'date': u'2016-02-08', u'by': u'dfg@yaa.com', u'value': u'applicable'}&#xA;{u'date': u'2017-02-08', u'by': u'wrwe@hot.com', u'value': u'ufc'}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How to do this in PySpark ??&lt;/p&gt;&#xA;" OwnerUserId="4265823" LastEditorUserId="2308683" LastEditDate="2018-03-10T03:42:26.883" LastActivityDate="2018-03-10T03:42:26.883" Title="How to get a List from a String in PySpark" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;&lt;apache-spark-sql&gt;" AnswerCount="1" CommentCount="4" />
  <row Id="49204346" PostTypeId="1" CreationDate="2018-03-10T01:11:05.630" Score="-2" ViewCount="14" Body="&lt;p&gt;I want to save the output/rows read from cassandra table to a file in either csv or json format.&#xA;using, Spark 1.6.3&#xA;scala&gt;val results.sqlContext.sql(&quot;select * from myks.mytable&quot;)&#xA;scala&gt;val.write.option(&quot;header&quot;,&quot;true&quot;).save(&quot;/tmp/xx.csv&quot;) -- writes to cfs:// filesystem; i am not able to find an option to write to the OS as csv or json format file.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Appreciate any help!&lt;/p&gt;&#xA;" OwnerUserId="5265410" LastActivityDate="2018-03-10T01:16:58.343" Title="how to save output from cassandra table using spark" Tags="&lt;scala&gt;&lt;apache-spark-sql&gt;&lt;cassandra-3.0&gt;" AnswerCount="1" CommentCount="2" />
  <row Id="49205023" PostTypeId="1" CreationDate="2018-03-10T03:27:18.400" Score="0" ViewCount="20" Body="&lt;p&gt;We have our huge legacy files sitting in our hadoop cluster in compressed sequence file Format. The sequence files were created using hive ETL. Lets say I had table in hive created using the following DDL:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-sql prettyprint-override&quot;&gt;&lt;code&gt;CREATE TABLE sequence_table(&#xA;col1 string,&#xA;col2 int)&#xA;stored as sequence file;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the script used to load above sequence table:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-sql prettyprint-override&quot;&gt;&lt;code&gt;set hive.exec.compress.output=true;&#xA;set io.seqfile.compression.type=BLOCK;&#xA;set mapred.output.compression.codec = org.apache.hadoop.io.compress.SnappyCodec;&#xA;insert into table sequence_table&#xA;select * from text_table;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now we have exported the data inside sequence file location to S3 for archival. Now am trying to process those files using spark in AWS EMR.  How can i read the sequence file in spark. I have taken a look at the sample file which has header like below and got to know that sequence file is with &lt;code&gt;&amp;lt;K,V&amp;gt;&lt;/code&gt; as &lt;code&gt;&amp;lt;BytesWritable,Text&amp;gt;&lt;/code&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;SEQ&quot;org.apache.hadoop.io.BytesWritableorg.apache.hadoop.io.Text)&#xA;org.apache.hadoop.io.compress.SnappyCodec&#xA;Ì(KÇØ»Ô:˜t£¾äIrlÿÿÿÿÌ(KÇØ»Ô:˜t£¾äIrlŽ£E  £   =£ &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I have tried this way:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-scala prettyprint-override&quot;&gt;&lt;code&gt;val file = sc.sequenceFile(&#xA;  &quot;s3://viewershipforneo4j/viewership/event_date=2017-09-17/*&quot;,&#xA;  classOf[BytesWritable],&#xA;  classOf[Text])&#xA;file.take(10)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But it generates this error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;18/03/09 16:48:07 ERROR TaskSetManager: Task 0.0 in stage 0.0 (TID 0) had a not serializable result: org.apache.hadoop.io.BytesWritable&#xA;Serialization stack:&#xA;        - object not serializable (class: org.apache.hadoop.io.BytesWritable, value: )&#xA;        - field (class: scala.Tuple2, name: _1, type: class java.lang.Object)&#xA;        - object (class scala.Tuple2, (,8255909859607837R188956557001505628000150562818115056280001505628181TimerRecord9558SRM-1528454-0PiYo Workout!FRFM1810000002017-09-17 01:29:29))&#xA;        - element of array (index: 0)&#xA;        - array (class [Lscala.Tuple2;, size 1); not retrying&#xA;18/03/09 16:48:07 WARN ExecutorAllocationManager: No stages are running, but numRunningTasks != 0&#xA;org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 0.0 (TID 0) had a not serializable result: org.apache.hadoop.io.BytesWritable&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I've then tried the following, but still no luck:&lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-scala prettyprint-override&quot;&gt;&lt;code&gt;scala&amp;gt; val data = sc.newAPIHadoopFile(&quot;s3://viewershipforneo4j/viewership/event_date=2017-09-17/*&quot;,classOf[SequenceFileInputFormat],classOf[BytesWritable],classOf[Text],conf)&#xA;&amp;lt;console&amp;gt;:31: error: class SequenceFileInputFormat takes type parameters&#xA;       val data = sc.newAPIHadoopFile(&quot;s3://viewershipforneo4j/viewership/event_date=2017-09-17/*&quot;,classOf[SequenceFileInputFormat],classOf[BytesWritable],classOf[Text],conf)&#xA;                                                                                                           ^&#xA;&#xA;scala&amp;gt; val data = sc.newAPIHadoopFile(&quot;s3://viewershipforneo4j/viewership/event_date=2017-09-17/*&quot;,classOf[SequenceFileInputFormat&amp;lt;BytesWritable,Text&amp;gt;],classOf[BytesWritable],classOf[Text],conf)&#xA;&amp;lt;console&amp;gt;:1: error: identifier expected but ']' found.&#xA;val data = sc.newAPIHadoopFile(&quot;s3://viewershipforneo4j/viewership/event_date=2017-09-17/*&quot;,classOf[SequenceFileInputFormat&amp;lt;BytesWritable,Text&amp;gt;],classOf[BytesWritable],classOf[Text],conf)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="4272727" LastEditorUserId="9297144" LastEditDate="2018-03-10T09:18:27.760" LastActivityDate="2018-03-10T09:18:27.760" Title="How to read snappy compressed sequence File in spark" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;sequencefile&gt;&lt;spark-hive&gt;" AnswerCount="0" CommentCount="5" />
  <row Id="49205169" PostTypeId="1" AcceptedAnswerId="49205257" CreationDate="2018-03-10T03:56:13.457" Score="0" ViewCount="29" Body="&lt;p&gt;I have two data files as below:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;course.txt &#xA;id,course &#xA;1,Hadoop&#xA;2,Spark&#xA;3,HBase&#xA;5,Impala&#xA;&#xA;Fee.txt &#xA;id,amount &#xA;2,3900&#xA;3,4200&#xA;4,2900&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I need to list all course info with their fee:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sqlContext.sql(&quot;select c.id, c.course, f.amount from course c left outer join fee f on f.id = c.id&quot;).show&#xA;+---+------+------+&#xA;| id|course|amount|&#xA;+---+------+------+&#xA;|  1|Hadoop|  null|&#xA;|  2| Spark|3900.0|&#xA;|  3| HBase|4200.0|&#xA;|  5|Impala|  null|&#xA;+---+------+------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;if the course is not indicated in the Fee table, then instead of showing null, I want to show 'N/A'.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've tried the following and not getting it yet:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;command 1:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sqlContext.sql(&quot;select c.id, c.course, ifnull(f.amount, 'N/A') from course c left outer join fee f on f.id = c.id&quot;).show&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Error: org.apache.spark.sql.AnalysisException: undefined function ifnull; line 1 pos 40&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;command 2:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;sqlContext.sql(&quot;select c.id, c.course, isnull(f.amount, 'N/A') from course c left outer join fee f on f.id = c.id&quot;).show&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Error:&#xA;org.apache.spark.sql.AnalysisException: No handler for Hive udf class org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNull because: The operator 'IS NULL' only accepts 1 argument..; line 1 pos 40&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;What is the right way to handle this in sqlContext within Scala? Thank you very much.&lt;/p&gt;&#xA;" OwnerUserId="9074574" LastEditorUserId="5880706" LastEditDate="2018-03-10T05:56:49.530" LastActivityDate="2018-03-10T05:56:49.530" Title="How does Scala handle isnull or ifnull in query with sqlContext" Tags="&lt;sql&gt;&lt;scala&gt;&lt;apache-spark&gt;&lt;isnull&gt;" AnswerCount="3" CommentCount="0" />
  <row Id="49205384" PostTypeId="1" CreationDate="2018-03-10T04:39:18.687" Score="0" ViewCount="23" Body="&lt;p&gt;I am trying to write a kafka consumer in java using Apache spark. The code is not executing due to some Log4jController error. Don't know what I am missing.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;pom.xml file is as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-core_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;2.3.0&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;org.slf4j&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;slf4j-api&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;1.7.25&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;  &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&#xA;  &amp;lt;artifactId&amp;gt;spark-streaming_2.11&amp;lt;/artifactId&amp;gt;&#xA;  &amp;lt;version&amp;gt;2.3.0&amp;lt;/version&amp;gt;&#xA;  &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;spark-streaming-kafka-0-8_2.11&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;2.3.0&amp;lt;/version&amp;gt;&#xA;    &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;  &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt;&#xA;  &amp;lt;artifactId&amp;gt;kafka_2.11&amp;lt;/artifactId&amp;gt;&#xA;  &amp;lt;version&amp;gt;1.0.0&amp;lt;/version&amp;gt;&#xA;    &amp;lt;exclusions&amp;gt;&#xA;        &amp;lt;exclusion&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;org.apache.zookeeper&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;zookeeper&amp;lt;/artifactId&amp;gt;&#xA;        &amp;lt;/exclusion&amp;gt;&#xA;        &amp;lt;exclusion&amp;gt;&#xA;            &amp;lt;groupId&amp;gt;log4j&amp;lt;/groupId&amp;gt;&#xA;            &amp;lt;artifactId&amp;gt;log4j&amp;lt;/artifactId&amp;gt;&#xA;        &amp;lt;/exclusion&amp;gt;&#xA;    &amp;lt;/exclusions&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Got following error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;5645 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler  - ResultStage 11 (start at RuleEngine.java:431) failed in 0.094 s due to Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 8, localhost, executor driver): java.lang.NoClassDefFoundError: Could not initialize class kafka.utils.Log4jController$&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="5877149" LastActivityDate="2018-03-10T10:53:04.380" Title="Kafka with Spark throw Could not initialize class kafka.utils.Log4jController Error" Tags="&lt;java&gt;&lt;apache-spark&gt;&lt;apache-kafka&gt;&lt;spark-streaming&gt;&lt;apache-kafka-streams&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49206283" PostTypeId="1" AcceptedAnswerId="49215164" CreationDate="2018-03-10T06:57:32.990" Score="0" ViewCount="20" Body="&lt;p&gt;I'm doing something dumb with my first Spark/Cassandra program using Java and am hoping someone can help &#xA;me figure out why I am getting this errror:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;: com.datastax.driver.core.exceptions.SyntaxError: line 1:8 no viable alternative at input 'FROM' (SELECT  [FROM]...)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The setup is&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Create keyspace test with replicaton={'class':strategy name, &#xA;                'replication_factor': No of replications on different nodes}&#xA;CREATE KEYSPACE test WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };&#xA;use test;&#xA;CREATE TABLE KeyValue ( key varchar, value bigint, PRIMARY KEY (key));&#xA;INSERT INTO KeyValue (key, value) VALUES ('afoo', 100);&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;And the code (below) is brain dead simple... I am using a 'select' clause, so I'm not sure why the driver is not&#xA;picking up the columns I specify.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import com.datastax.spark.connector.cql.CassandraConnector;&#xA;import org.apache.spark.SparkConf;&#xA;import org.apache.spark.api.java.JavaRDD;&#xA;import org.apache.spark.api.java.JavaSparkContext;&#xA;&#xA;import java.io.Serializable;&#xA;&#xA;import static com.datastax.spark.connector.japi.CassandraJavaUtil.javaFunctions;&#xA;import static com.datastax.spark.connector.japi.CassandraJavaUtil.mapRowTo;&#xA;&#xA;public class JavaDemo {&#xA;  public static void main(String[] args) throws Exception {&#xA;    String sparkMaster = &quot;local[2]&quot;;&#xA;    String cassandraHost = &quot;localhost&quot;;&#xA;    SparkConf conf = new SparkConf(true)&#xA;            .set(&quot;spark.cassandra.connection.host&quot;, cassandraHost);&#xA;&#xA;    JavaSparkContext sc = new JavaSparkContext(sparkMaster, &quot;basicquerycassandra&quot;, conf);&#xA;    CassandraConnector connector = CassandraConnector.apply(conf);&#xA;    JavaRDD&amp;lt;KeyValue&amp;gt; rdd = javaFunctions(sc)&#xA;            .cassandraTable(&quot;test&quot;, &quot;keyvalue&quot;, mapRowTo(KeyValue.class))&#xA;            .withConnector(connector).select(&quot;key&quot;, &quot;value&quot;)&#xA;            .where(&quot;key = 'afoo'&quot;);&#xA;&#xA;    rdd.foreach(row -&amp;gt; System.out.println(&quot;got item&quot; + row));&#xA;  }&#xA;&#xA;  public static class KeyValue implements Serializable {&#xA;    private String key;&#xA;    private Integer value;&#xA;    public KeyValue() {&#xA;    }&#xA;    public static KeyValue newInstance(String k, Integer v) {&#xA;      KeyValue kv = new KeyValue();&#xA;      kv.setKey(k);&#xA;      kv.setValue(v);&#xA;      return kv;&#xA;    }&#xA;    public String getKey() {&#xA;      return key;&#xA;    }&#xA;    public Integer getValue() {&#xA;      return value;&#xA;    }&#xA;    void setKey(String k) {&#xA;      this.key = k;&#xA;    }&#xA;    void setValue(Integer v) {&#xA;      this.value = v;&#xA;    }&#xA;&#xA;    @Override&#xA;    public String toString() {&#xA;      return &quot;KeyValue{&quot; +&#xA;              &quot;key='&quot; + key + '\'' +&#xA;              &quot;, value=&quot; + value +&#xA;              '}';&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;UPDATE: I can avoid the syntax error if i update the code as below... it is not yet exactly what i want.  I will fiddle with it tomorrow and post an answer if no one beats me to it.  I'm close ;^)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    JavaSparkContext sc = new JavaSparkContext(sparkMaster, &quot;basicquerycassandra&quot;, conf);&#xA;    CassandraConnector connector = CassandraConnector.apply(conf);&#xA;    CassandraTableScanJavaRDD&amp;lt;CassandraRow&amp;gt; rdd = javaFunctions(sc)&#xA;            .cassandraTable(&quot;test&quot;, &quot;keyvalue&quot;)&#xA;            .select(&quot;key&quot;, &quot;value&quot;)&#xA;            .where(&quot;key = 'afoo'&quot;);&#xA;&#xA;    rdd.foreach(row -&amp;gt; System.out.println(&quot;got item&quot; + row));&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="1224363" LastEditorUserId="1224363" LastEditDate="2018-03-10T07:31:07.737" LastActivityDate="2018-03-10T23:47:59.940" Title="java spark cassandra mini-program gives datastax driver exceptions: SyntaxError: no viable alternative at input 'FROM'" Tags="&lt;java&gt;&lt;apache-spark&gt;&lt;cassandra&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49206420" PostTypeId="1" AcceptedAnswerId="49208066" CreationDate="2018-03-10T07:18:17.917" Score="0" ViewCount="30" Body="&lt;p&gt;Im encountering error when Im trying to return data frame as list from my user defined function &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;myDataFrame = (&#xA;    sc.parallelize([&#xA;        (10001, &quot;2017-02-12 12:01:40&quot; , &quot;2017-02-12 12:56:32&quot;),&#xA;        (10001, &quot;2017-02-13 12:06:32&quot; , &quot;2017-02-15 16:06:32&quot;),&#xA;        (10001, &quot;2017-02-16 21:45:56&quot; , &quot;2017-02-21 21:45:56&quot;),&#xA;        (10001, &quot;2017-02-21 22:32:41&quot; , &quot;2017-02-25 00:52:50&quot;),&#xA;        ]).toDF([&quot;id&quot;,  &quot;startTime&quot; ,  &quot;endTime&quot;]).withColumn(&quot;startTime&quot;, col(&quot;startTime&quot;).cast(&quot;timestamp&quot;)).withColumn(&quot;endTime&quot;, col(&quot;endTime&quot;).cast(&quot;timestamp&quot;)))&#xA;&#xA;&#xA;return_type = ArrayType(MapType(StringType(), StringType()))&#xA;@udf(returnType=return_type)&#xA;def myUdf(start, end):&#xA;  start = pd.to_datetime(start,infer_datetime_format=True)&#xA;  end = pd.to_datetime(end,infer_datetime_format=True)&#xA;  rng = pd.date_range(start.floor('h'), end.floor('h'), freq='h')&#xA;  left = pd.Series(rng, index=rng).clip_lower(start)&#xA;  right = pd.Series(rng + 1, index=rng).clip_upper(end)&#xA;  timeSeries = right - left&#xA;  resultDataFrame = []&#xA;  for key, result in timeSeries.items():&#xA;    resultDataFrame.append((datetime.weekday(key.date()) , key.time().hour , int(result.total_seconds()//60)))&#xA;  resultDataFrame = pd.DataFrame(resultDataFrame, columns=('day', 'hour', 'minute'))&#xA;  response = resultDataFrame.to_dict(&quot;index&quot;).values()&#xA;  return (list(response))&#xA;&#xA;&#xA;extracted = myUdf(&quot;startTime&quot;, &quot;endTime&quot;)&#xA;exploded = explode(extracted).alias(&quot;exploded&quot;)&#xA;expanded = [col(&quot;exploded&quot;).getItem(k).alias(k) for k in [&quot;day&quot;, &quot;hour&quot;, &quot;minute&quot;]]&#xA;result = myDataFrame.select(&quot;id&quot;, exploded).select(&quot;id&quot;,*expanded)&#xA;result.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to print result like this &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;+---------+----+----+------+                                                         &#xA;|utilityId|day |hour|minute|&#xA;+---------+----+----+------+&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But, I get error like &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ERROR Executor: Exception in task 0.0 in stage 1005.0 (TID 18845)&#xA;net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype)&#xA;    at net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)&#xA;    at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:707)&#xA;    at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:175)&#xA;    at net.razorvine.pickle.Unpickler.load(Unpickler.java:99)&#xA;    at net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)&#xA;    at org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$apply$6.apply(BatchEvalPythonExec.scala:156)&#xA;    at org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$apply$6.apply(BatchEvalPythonExec.scala:155)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="903521" LastActivityDate="2018-03-10T15:54:04.790" Title="Error while Return data frame as list PySpark" Tags="&lt;pandas&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49206628" PostTypeId="1" CreationDate="2018-03-10T07:45:47.400" Score="0" ViewCount="12" Body="&lt;p&gt;I am trying to do wordcount program using Kafka with Spark but I got the below error.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any suggestions please?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Could not find or load main class com.mycom.sparkWscala.streaming.kafkaWC&lt;/p&gt;&#xA;" OwnerUserId="9469158" LastEditorUserId="7897191" LastEditDate="2018-03-10T10:54:11.150" LastActivityDate="2018-03-10T10:54:11.150" Title="Could not find or load main class com.mycom.sparkWscala.streaming.kafkaWC" Tags="&lt;apache-spark&gt;&lt;apache-kafka&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49207165" PostTypeId="1" AcceptedAnswerId="49207375" CreationDate="2018-03-10T08:49:36.543" Score="0" ViewCount="20" Body="&lt;p&gt;&lt;strong&gt;Backgroud:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a dataframe which has three columns : &lt;code&gt;id, x, y&lt;/code&gt;. x,y are Double. &lt;/p&gt;&#xA;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Firstly, I  &lt;code&gt;struct (col(&quot;x&quot;),col(&quot;y&quot;))&lt;/code&gt; to get the coordinate column.&lt;/li&gt;&#xA;&lt;li&gt;Then &lt;code&gt;groupBy(col(&quot;id&quot;))&lt;/code&gt; and &lt;code&gt;agg(collect_list(col(&quot;coordinate&quot;)))&lt;/code&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&#xA;&lt;p&gt;So now the df only have two columns: &lt;code&gt;id ,coordinate&lt;/code&gt;.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I think the datatype of coordinate is &lt;code&gt;collection.mutable.WrappedArray[(Double,Double)]&lt;/code&gt;. &#xA;  So i passed it to udf. However, the datatype is wrong. I got a Error when run the code. I don't know why. what is the real datatype of struct(col1,col2) ? Or is there another way to get the correct answer easily?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This is code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def getMedianPoint = udf((array1: collection.mutable.WrappedArray[(Double,Double)]) =&amp;gt; {  &#xA;    var l = (array1.length/2)&#xA;    var c = array1(l)&#xA;    val x = c._1.asInstanceOf[Double]&#xA;    val y = c._2.asInstanceOf[Double]&#xA;    (x,y)&#xA;})&#xA;&#xA;df.withColumn(&quot;coordinate&quot;,struct(col(&quot;x&quot;),col(&quot;y&quot;)))&#xA;  .groupBy(col(&quot;id&quot;))&#xA;  .agg(collect_list(&quot;coordinate&quot;).as(&quot;coordinate&quot;)&#xA;  .withColumn(&quot;median&quot;,getMedianPoint(col(&quot;coordinate&quot;)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Thank you very much!&lt;/p&gt;&#xA;" OwnerUserId="8740367" LastActivityDate="2018-03-10T09:53:41.667" Title="In spark dataframe udf, what is the type of function parameters which like struct(col1,col2)?" Tags="&lt;apache-spark&gt;&lt;spark-dataframe&gt;&lt;apache-spark-dataset&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49207577" PostTypeId="1" CreationDate="2018-03-10T09:38:50.617" Score="1" ViewCount="19" Body="&lt;p&gt;Im converting DataFrame into List using following code&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;resultDataFrame = []&#xA;resultDataFrame = pd.DataFrame(resultDataFrame, columns=('day', 'hour', 'minute'))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;and appending values ...&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;resultDataFrame.to_dict('l')&#xA;{'day': [6], 'hour': [12], 'minute': [54]}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But, I want to get it like &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;{'day': 6, 'hour': 12, 'minute': 54}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;How to change the format?&lt;/p&gt;&#xA;" OwnerUserId="903521" LastActivityDate="2018-03-10T09:51:10.983" Title="DataFrame to List format" Tags="&lt;pandas&gt;&lt;pyspark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49207798" PostTypeId="1" CreationDate="2018-03-10T10:05:26.117" Score="0" ViewCount="13" Body="&lt;p&gt;I am new in mlib. i wanted to run spark mlib example. I have created new play framework project. and added following dependency : &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;name := &quot;&quot;&quot;EnsembleAI&quot;&quot;&quot;&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;version := &quot;1.0-SNAPSHOT&quot;&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;lazy val root = (project in file(&quot;.&quot;)).enablePlugins(PlayScala)&#xA;  scalaVersion := &quot;2.11.7&quot;&#xA;  libraryDependencies ++= Seq(   jdbc,   cache,   ws,&lt;br&gt;&#xA;  &quot;org.scalatestplus.play&quot; %% &quot;scalatestplus-play&quot; % &quot;1.5.1&quot; % Test,&lt;br&gt;&#xA;  &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % &quot;2.3.0&quot;,   &quot;org.apache.spark&quot; %%&#xA;  &quot;spark-sql&quot; % &quot;2.3.0&quot;,   &quot;org.apache.spark&quot; %% &quot;spark-mllib&quot; % &quot;2.3.0&quot;&#xA;  )&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;when i am running following example : &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;package lib&#xA;&#xA;import org.apache.spark.{SparkConf, SparkContext}&#xA;// $example on$&#xA;import org.apache.spark.mllib.tree.RandomForest&#xA;import org.apache.spark.mllib.tree.model.RandomForestModel&#xA;import org.apache.spark.mllib.util.MLUtils&#xA;// $example off$&#xA;&#xA;object RandomForestClassificationExample {&#xA;  def main(args: Array[String]): Unit = {&#xA;    val conf = new SparkConf().setMaster(&quot;local[1]&quot;).setAppName(&quot;RandomForestClassificationExample&quot;)&#xA;    val sc = new SparkContext(conf)&#xA;    // $example on$&#xA;    // Load and parse the data file.&#xA;    val data = MLUtils.loadLibSVMFile(sc, &quot;sample_libsvm_data.txt&quot;)&#xA;    // Split the data into training and test sets (30% held out for testing)&#xA;    val splits = data.randomSplit(Array(0.7, 0.3))&#xA;    val (trainingData, testData) = (splits(0), splits(1))&#xA;&#xA;    // Train a RandomForest model.&#xA;    // Empty categoricalFeaturesInfo indicates all features are continuous.&#xA;    val numClasses = 2&#xA;    val categoricalFeaturesInfo = Map[Int, Int]()&#xA;    val numTrees = 3 // Use more in practice.&#xA;    val featureSubsetStrategy = &quot;auto&quot; // Let the algorithm choose.&#xA;    val impurity = &quot;gini&quot;&#xA;    val maxDepth = 4&#xA;    val maxBins = 32&#xA;&#xA;    val model = RandomForest.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,&#xA;      numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)&#xA;&#xA;    // Evaluate model on test instances and compute test error&#xA;    val labelAndPreds = testData.map { point =&amp;gt;&#xA;      val prediction = model.predict(point.features)&#xA;      (point.label, prediction)&#xA;    }&#xA;    val testErr = labelAndPreds.filter(r =&amp;gt; r._1 != r._2).count.toDouble / testData.count()&#xA;    println(s&quot;Test Error = $testErr&quot;)&#xA;    println(s&quot;Learned classification forest model:\n ${model.toDebugString}&quot;)&#xA;&#xA;    // Save and load model&#xA;    model.save(sc, &quot;target/tmp/myRandomForestClassificationModel&quot;)&#xA;    val sameModel = RandomForestModel.load(sc, &quot;target/tmp/myRandomForestClassificationModel&quot;)&#xA;    // $example off$&#xA;&#xA;    sc.stop()&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;getting following error : &lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;18/03/10 15:01:55 INFO BlockManager: Initialized BlockManager:&#xA;  BlockManagerId(driver, 192.168.42.97, 34415, None) Exception in thread&#xA;  &quot;main&quot; java.lang.ExceptionInInitializerError  at&#xA;  org.apache.spark.SparkContext.withScope(SparkContext.scala:692)   at&#xA;  org.apache.spark.SparkContext.textFile(SparkContext.scala:821)    at&#xA;  org.apache.spark.mllib.util.MLUtils$.parseLibSVMFile(MLUtils.scala:101)&#xA;    at&#xA;  org.apache.spark.mllib.util.MLUtils$.loadLibSVMFile(MLUtils.scala:76)&#xA;    at&#xA;  org.apache.spark.mllib.util.MLUtils$.loadLibSVMFile(MLUtils.scala:159)&#xA;    at&#xA;  org.apache.spark.mllib.util.MLUtils$.loadLibSVMFile(MLUtils.scala:167)&#xA;    at&#xA;  lib.RandomForestClassificationExample$.main(RandomForestClassificationExample.scala:16)&#xA;    at&#xA;  lib.RandomForestClassificationExample.main(RandomForestClassificationExample.scala)&#xA;  Caused by: com.fasterxml.jackson.databind.JsonMappingException:&#xA;  Incompatible Jackson version: 2.7.8   at&#xA;  com.fasterxml.jackson.module.scala.JacksonModule$class.setupModule(JacksonModule.scala:64)&#xA;    at&#xA;  com.fasterxml.jackson.module.scala.DefaultScalaModule.setupModule(DefaultScalaModule.scala:19)&#xA;    at&#xA;  com.fasterxml.jackson.databind.ObjectMapper.registerModule(ObjectMapper.java:730)&#xA;    at&#xA;  org.apache.spark.rdd.RDDOperationScope$.(RDDOperationScope.scala:82)&#xA;    at&#xA;  org.apache.spark.rdd.RDDOperationScope$.(RDDOperationScope.scala)&#xA;    ... 8 more&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;and here is the &lt;a href=&quot;https://github.com/apache/spark/blob/master/data/mllib/sample_libsvm_data.txt&quot; rel=&quot;nofollow noreferrer&quot;&gt;sample_libsvm_data.txt&lt;/a&gt;.&lt;/p&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;h2&gt;Note : above example taken from spark example only.&lt;/h2&gt;&#xA;&#xA;&lt;hr&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Please guide me where i am wrong .&lt;/strong&gt;&lt;/p&gt;&#xA;" OwnerUserId="6559020" LastActivityDate="2018-03-10T10:05:26.117" Title="Exception in thread &quot;main&quot; java.lang.ExceptionInInitializerError while executiing spark RandomForestClassificationExample" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;machine-learning&gt;&lt;playframework&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49207992" PostTypeId="1" CreationDate="2018-03-10T10:29:26.870" Score="0" ViewCount="25" Body="&lt;p&gt;Im creating list response from following function. However, the First one looks OK but second one doesnt. When I did debug both of them looks same.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;start = pd.to_datetime(start,infer_datetime_format=True)&#xA;end = pd.to_datetime(end,infer_datetime_format=True)&#xA;date_range = pd.date_range(start, end, freq=&quot;h&quot;)&#xA;df = pd.DataFrame({&quot;day&quot;: date_range.strftime(&quot;%a&quot;),&quot;hour&quot;: date_range.hour,&quot;minute&quot;: date_range.minute})&#xA;values = df.to_dict(&quot;index&quot;).values()&#xA;return list(values)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;In this section Im building DataFrame myself (this one not accepted by invoker - given below)&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;resultDataFrame = []&#xA;for key, result in timeSeries.items():&#xA; resultDataFrame.append((datetime.weekday(key.date()) , key.time().hour , int(result.total_seconds()//60)))&#xA;resultDataFrame = pd.DataFrame(resultDataFrame, columns=('day', 'hour', 'minute'))&#xA;return resultDataFrame.to_dict('r')&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Here is the section which invokes above functions&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;extracted = myUdf(&quot;startTime&quot;, &quot;endTime&quot;)&#xA;exploded = explode(extracted).alias(&quot;exploded&quot;)&#xA;expanded = [col(&quot;exploded&quot;).getItem(k).alias(k) for k in [&quot;day&quot;, &quot;hour&quot;, &quot;minute&quot;]]&#xA;result = utilisationDataFarme.select(&quot;id&quot;, exploded).select(&quot;id&quot;,*expanded)&#xA;result.show()&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I get the following error message&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;expected zero arguments for construction of ClassDict (for numpy.dtype)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Anything wrong in my second section?&lt;/p&gt;&#xA;" OwnerUserId="903521" LastActivityDate="2018-03-10T10:29:26.870" Title="Dataframe to list conversion format issue" Tags="&lt;pandas&gt;&lt;pyspark&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49208511" PostTypeId="1" CreationDate="2018-03-10T11:26:08.247" Score="0" ViewCount="6" Body="&lt;p&gt;I am using Spark Streaming with Kafka using the direct approach.Basically when there is multiple tasks failure after certain retries at the batch level the entire batch gets failed and the data gets lost.I want to know if there is any mechanism to track those failed batches with lost offset ranges so that I can reprocess those lost data again. &lt;/p&gt;&#xA;" OwnerUserId="5462259" LastActivityDate="2018-03-10T11:26:08.247" Title="How to keep track of failed batches in spark streaming" Tags="&lt;spark-streaming&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49209794" PostTypeId="1" CreationDate="2018-03-10T13:48:37.527" Score="0" ViewCount="7" Body="&lt;p&gt;&lt;strong&gt;Problem Description:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I have a dataset which is about  &lt;em&gt;35 millons rows&lt;/em&gt;  and  &lt;em&gt;10 columns&lt;/em&gt; . &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to calculate the distance between two rows, which the distancefunction like &lt;code&gt;distance(row1,row2)&lt;/code&gt;, and then store the value in a huge matrix.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;The operations totally needed are nearly &lt;em&gt;6*10^15&lt;/em&gt;, which i think is very huge.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;What I've tried :&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;upload datafile to HDFS&lt;/li&gt;&#xA;&lt;li&gt;read data as dataframe&lt;/li&gt;&#xA;&lt;li&gt;&lt;code&gt;df.collect()&lt;/code&gt; and get a &lt;code&gt;array1 :array[Row]&lt;/code&gt;&lt;/li&gt;&#xA;&lt;li&gt;traverse &lt;code&gt;array1&lt;/code&gt; pair-wisely and calculate distance&lt;/li&gt;&#xA;&lt;li&gt;store the &lt;code&gt;distance(rowi,rowj)&lt;/code&gt; in &lt;em&gt;matrix(i,j)&lt;/em&gt;&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;Scala code :&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val array1 = df.collect()&#xA;val l = array1.length &#xA;for(i &amp;lt;-0 until array.length){&#xA;    for(j &amp;lt;-i+1 until array.length){&#xA;             val vd = Vectors.dense(i,j,distance(array(i),array(j)))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to save each value in Vector like above, and add it to RDD/Dataframe. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;But the only way i've searched is by using &lt;code&gt;union&lt;/code&gt;.I think it's not good enough.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;So there are three questions need to be solved:&lt;/strong&gt;&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&lt;code&gt;collect&lt;/code&gt; is an action function, &lt;code&gt;df.collect()&lt;/code&gt; will throw Exception&#xA;&lt;code&gt;java.lang.OutOf.MemoryError : Java heap space&lt;/code&gt;. Can this be avoided?&lt;/li&gt;&#xA;&lt;li&gt;As soon as i get a &lt;code&gt;distance(rowi,rowj)&lt;/code&gt;, i want to store it, how?&lt;/li&gt;&#xA;&lt;li&gt;Can I store the final matrix in HDFS and read it as a matrix in python?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&#xA;&lt;p&gt;ps: If above all can't be solved, which new idea can i use?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Any answer will help me a lot ,thank you!&lt;/p&gt;&#xA;" OwnerUserId="8740367" LastActivityDate="2018-03-10T13:48:37.527" Title="how to traverse a massive dataframe pair-wisely and store the value in a n*n matrix?" Tags="&lt;pyspark&gt;&lt;hdfs&gt;&lt;spark-dataframe&gt;&lt;apache-spark-mllib&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49209840" PostTypeId="1" CreationDate="2018-03-10T13:53:21.193" Score="0" ViewCount="9" Body="&lt;p&gt;I'm running the example code from the &lt;a href=&quot;https://spark.apache.org/docs/2.1.0/ml-classification-regression.html#binomial-logistic-regression&quot; rel=&quot;nofollow noreferrer&quot;&gt;spark docs&lt;/a&gt; for logistic regression using pyspark and the attendant training summary code and get:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;areaUnderROC: 1.0&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;which I wouldn't expect. Perhaps it overfit and simply memorized the data, but I've done train and test, even randomized labels, and tweaked all the hyper parameters and they all led to the same thing:AUC=1.0. I tried the sample code for the SVC models, which uses the same dataset, and I get the same thing. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I'd normally post the code, but I literally ran the example code only changing the path to the data file.  I've searched and searched and can find no example of anyone having run this example and examined the results. What's odd is that this dataset, sample_libsvm_data.txt, is used throughout the docs yet I can find neither analysis of it nor even an explanation of what the data actually is. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;As a result I've switched to using the RDD-based API of MLLIB because I can't make sense of the results of the sample code. I hope someone can tell me how I'm doing something wrong. &lt;/p&gt;&#xA;" OwnerUserId="2488162" LastActivityDate="2018-03-10T13:53:21.193" Title="When I run the spark sample for logistic regression, I get a perfect model. Did I screw up" Tags="&lt;apache-spark&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49209905" PostTypeId="1" CreationDate="2018-03-10T13:59:34.210" Score="0" ViewCount="6" Body="&lt;p&gt;I am running a spark application in Hadoop Cluster &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Spark version 1.5 , Hadoop cluster CDH5.5 &lt;/p&gt;&#xA;&#xA;&lt;p&gt;Yarn is resource allocator for this spark application .&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am getting the below error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;Application application_1520522414016_3444 failed 2 times due to AM Container for appattempt_1520522414016_3444_000003 exited with exitCode: -104&#xA;Diagnostics: Container  [pid=30107,containerID=container_e96_1520522414016_3444_03_000001] is running beyond physical memory limits. Current usage: 3.5 GB of 3.5 GB physical memory used; 4.4 GB of 7.3 GB virtual memory used. Killing container.&#xA;Dump of the process-tree for container_e96_1520522414016_3444_03_000001 :&#xA;|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE&#xA;|- 30111 30107 30107 30107 (java) 135209 35121 4578676736 918471 /usr/java/jdk1.7.0_67-cloudera/bin/java -server -Xmx3072m -&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now Lets look at my configuration &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;MASTER_URL=yarn-cluster&#xA;NUM_EXECUTORS=8&#xA;EXECUTOR_MEMORY=3G&#xA;EXECUTOR_CORES=2&#xA;DRIVER_MEMORY=3G&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I also tried multiple combinations for the above configuration .Every time I got the same error&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; INFO yarn.YarnAllocator: Will request 8 executor containers, each with 2 cores and 3456 MB memory including 384 MB overhead &#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;As Per logs Yarn has given 8 executor containers , and the container ids are given below &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; 18/03/10 10:33:03 INFO yarn.YarnAllocator: Launching container container_e96_1520522414016_3444_03_000002 for on host 1&#xA; 18/03/10 10:33:03 INFO yarn.YarnAllocator: Launching container container_e96_1520522414016_3444_03_000003 for on host 2&#xA; 18/03/10 10:33:03 INFO yarn.YarnAllocator: Launching container container_e96_1520522414016_3444_03_000004 for on host 3 &#xA; 18/03/10 10:33:03 INFO yarn.YarnAllocator: Launching container container_e96_1520522414016_3444_03_000005 for on host 4&#xA; 18/03/10 10:33:03 INFO yarn.YarnAllocator: Launching container container_e96_1520522414016_3444_03_000006 for on host 5&#xA; 18/03/10 10:33:03 INFO yarn.YarnAllocator: Launching container container_e96_1520522414016_3444_03_000007 for on host 6 &#xA; 18/03/10 10:33:03 INFO yarn.YarnAllocator: Launching container container_e96_1520522414016_3444_03_000008 for on host 7 &#xA; 18/03/10 10:33:03 INFO yarn.YarnAllocator: Launching container container_e96_1520522414016_3444_03_000009 for on host 8&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The error says container_e96_1520522414016_3444_03_000001 is running beyond physical memory limits. Current usage: 3.5 GB of 3.5 GB physical memory used;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;So container_e96_1520522414016_3444_03_000001 is not one among the above 8 executor containers .. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;So Is container_e96_1520522414016_3444_03_000001 is driver container?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Am I getting this getting this error because if the fact that driver container is not having enough physical memory?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;If I increase DRIVER_MEMORY=3G to 5G then Does it work?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I went through similar questions and most of people suggested to increase the below two properties &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;mapreduce.map.memory.mb&#xA;mapreduce.reduce.memory.mb&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The error says 3.5 GB of 3.5 GB physical memory used&lt;/p&gt;&#xA;&#xA;&lt;p&gt;but in my cluster mapred.site.xml, I see the value for mapreduce.map.memory.mb is 4096MB, so it is 4 GB, then why the error say 3.5 GB&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt; &amp;lt;property&amp;gt;&#xA;   &amp;lt;name&amp;gt;mapreduce.map.memory.mb&amp;lt;/name&amp;gt;&#xA;   &amp;lt;value&amp;gt;4096&amp;lt;/value&amp;gt;&#xA; &amp;lt;/property&amp;gt;&#xA; &amp;lt;property&amp;gt;&#xA;   &amp;lt;name&amp;gt;mapreduce.map.cpu.vcores&amp;lt;/name&amp;gt;&#xA;   &amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt;&#xA; &amp;lt;/property&amp;gt;&#xA; &amp;lt;property&amp;gt;&#xA;   &amp;lt;name&amp;gt;mapreduce.reduce.memory.mb&amp;lt;/name&amp;gt;&#xA;   &amp;lt;value&amp;gt;4096&amp;lt;/value&amp;gt;&#xA; &amp;lt;/property&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;If the solution is to change the above two properties in mapred.site.xml, Then Do I need to edit this mapred.site.xml or Can i Overwrite these two properties via spark-submit command?&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Please help me on this problem.&lt;/p&gt;&#xA;" OwnerUserId="3240790" LastEditorUserId="3240790" LastEditDate="2018-03-11T00:02:04.290" LastActivityDate="2018-03-11T00:02:04.290" Title="Spark application throws container physical memory error" Tags="&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;yarn&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49210725" PostTypeId="1" CreationDate="2018-03-10T15:24:03.813" Score="0" ViewCount="17" Body="2B2" OwnerUserId="1020472" LastActivityDate="2018-03-10T15:24:03.813" Title="B?B" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;serialization&gt;&lt;scala-breeze&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49210983" PostTypeId="1" CreationDate="2018-03-10T15:48:45.250" Score="0" ViewCount="12" Body="1A1" OwnerUserId="2329714" LastActivityDate="2018-03-10T15:48:45.250" Title="A?A" Tags="&lt;csv&gt;&lt;apache-spark&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="915697951" PostTypeId="2" ParentId="49210983" CreationDate="2017-08-15T17:11:22.300" Score="0" Body="Resposta A &lt;code&gt;df.collect()&lt;/code&gt;" OwnerUserId="5880706" LastEditorUserId="5880706" LastEditDate="2017-08-16T09:57:09.860" LastActivityDate="2017-08-16T09:57:09.860" CommentCount="9" />
  <row Id="915697952" PostTypeId="2" ParentId="999" CreationDate="2017-08-15T17:11:22.300" Score="0" Body="Resposta B &lt;code&gt;df.collect()&lt;/code&gt;" OwnerUserId="5880706" LastEditorUserId="5880706" LastEditDate="2017-08-16T09:57:09.860" LastActivityDate="2017-08-16T09:57:09.860" CommentCount="9" />
  <row Id="915697954" PostTypeId="2" ParentId="49210725" CreationDate="2017-08-15T17:11:22.300" Score="0" Body="Resposta C &lt;code&gt;df.collect()&lt;/code&gt;" OwnerUserId="5880706" LastEditorUserId="5880706" LastEditDate="2017-08-16T09:57:09.860" LastActivityDate="2017-08-16T09:57:09.860" CommentCount="9" />
  <row Id="49211119" PostTypeId="1" CreationDate="2018-03-10T16:01:23.410" Score="0" ViewCount="15" Body="&lt;p&gt;I can easily write custom UDF which can look as follows:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from pyspark.sql.types import IntegerType&#xA;from pyspark.sql.functions import udf&#xA;&#xA;delta = udf(lambda currentValue: currentValue - 3, IntegerType())&#xA;#Let's assume our data frame is ordered&#xA;df = sqlContext.createDataFrame([{'value': 0}, {'value': 3}, {'value': 5}])&#xA;df.withColumn(&quot;deltas&quot;, delta(df.value))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;However I would need bit more complex function to be captured:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;    delta = udf(lambda currentValue, previousValue: currentValue - previousValue, IntegerType())&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now I would like to somehow call my function, I'd need something like:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df.withColumn(&quot;deltas&quot;, delta(df.value, df.value[-1]))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I.e. I would need to pass the same column from multiple rows into the single udf. Is this even possible anyhow easily? In this particular example it would mean to pass the value from one row before whereas having the dataframe ordered previously. However I would need some generic solution to this sort of &quot;time series&quot; dependence in order to compute columns which would e.g. compound the information of prior 20 rows. Simple example could be moving average over 5 days.&lt;/p&gt;&#xA;" OwnerUserId="1158652" LastActivityDate="2018-03-10T16:01:23.410" Title="Pyspark - UDF receiving values from other rows in spark" Tags="&lt;apache-spark&gt;&lt;pyspark&gt;&lt;user-defined-functions&gt;" AnswerCount="0" CommentCount="1" />
  <row Id="49211580" PostTypeId="1" CreationDate="2018-03-10T16:51:32.303" Score="0" ViewCount="3" Body="&lt;p&gt;I am looking to modify this code to read all json files in /data/*.json one by one, also I would like to trap any error handling during reading of the file and at data frame level when running the process. load data from data frame to hive table.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;This coding is in pyspark 2.2 &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;#!/bin/python2&#xA;from pyspark import SparkContext&#xA;from pyspark.sql import SparkSession&#xA;from pyspark.sql.functions import *&#xA;from pyspark.sql.types import *&#xA;import json&#xA;&#xA;# define context&#xA;sc = SparkContext()&#xA;spark = SparkSession(sc)&#xA;&#xA;# load sources&#xA;jFile = &quot;hdfs://namenode/data/testdatafile.json&quot;&#xA;datafile = spark.read.json(jFile)&#xA;&#xA;sdf = datafile.select(&quot;CompanyID&quot;,&quot;CompanyName&quot;,&quot;ShipmentID&quot;,&quot;StartTime&quot;,&quot;StartTimeZone&quot;, \&#xA;                        &quot;EndTime&quot;, &quot;EndTimeZone&quot;,  \&#xA;                        (explode(col(&quot;ShippingData&quot;)).alias(&quot;Shipping_ROW&quot;)) \&#xA;                        )&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="5983694" LastActivityDate="2018-03-10T16:51:32.303" Title="Error handing in spark 2.2" Tags="&lt;pyspark&gt;&lt;pyspark-sql&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49211760" PostTypeId="1" CreationDate="2018-03-10T17:10:22.517" Score="0" ViewCount="9" Body="&lt;p&gt;What will be the equivalent pyspark modules for the below packages:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;from sklearn.datasets import load_files&#xA;from sklearn.datasets import dump_svmlight_file, load_svmlight_file&#xA;from scipy.sparse import csr_matrix, coo_matrix&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="4289911" LastActivityDate="2018-03-10T17:10:22.517" Title="what will be the equivalent pyspark modules for sklearn.dataset" Tags="&lt;python&gt;&lt;machine-learning&gt;&lt;pyspark&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49211882" PostTypeId="1" CreationDate="2018-03-10T17:21:16.783" Score="0" ViewCount="11" Body="&lt;p&gt;I am new to Apache Spark and its Java API. I have a set a of matrices and I want to concatenate them row wise. I need to done this using JavaPairRDD and broadcast joins. &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;private void concatenate(MatrixBlock b1,MatrixBlock b2){&#xA;//concatenate 2 matrices&#xA;}&#xA;&#xA;public void process(MatrixBlock mb...){   &#xA;    //concatenate all the matrix blocks &#xA;    JavaPairRDD&amp;lt;MatrixIndexes,MatrixBlock&amp;gt; in = ;&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Can someone explain me how to do this? &lt;/p&gt;&#xA;" OwnerUserId="3888646" LastActivityDate="2018-03-10T17:21:16.783" Title="Merging matrices using JavaPairRDD" Tags="&lt;java&gt;&lt;apache-spark&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49212164" PostTypeId="1" CreationDate="2018-03-10T17:48:14.567" Score="0" ViewCount="8" Body="&lt;p&gt;I working on spark streaming job in which incoming stream join with existing hive table. I have created a singleton hiveContext. When hiveContext fetch the table data from hive, spark give warning and after few day warning converts into error.&lt;/p&gt;&#xA;&#xA;&lt;blockquote&gt;&#xA;  &lt;p&gt;18/03/10 15:55:28 INFO parquet.ParquetRelation$$anonfun$buildInternalScan$1$$anon$1: Input split: ParquetInputSplit{part: hdfs://nameservice1/user/hive/warehouse/iot.db/iotdevice/part-r-00000-931d1d81-af03-41a4-b659-81a883131289.gz.parquet start: 0 end: 5695 length: 5695 hosts: []}&lt;/p&gt;&#xA;  &#xA;  &lt;p&gt;18/03/10 15:55:28 WARN security.UserGroupInformation: PriviledgedActionException as:svc-ra-iotloaddev (auth:SIMPLE) cause:org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)&#xA;  18/03/10 15:55:28 WARN kms.LoadBalancingKMSClientProvider: KMS provider at [&lt;a href=&quot;https://iotserver9009.kd.iotserver.com:16000/kms/v1/]&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://iotserver9009.kd.iotserver.com:16000/kms/v1/]&lt;/a&gt; threw an IOException [org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]!!&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&#xA;&lt;p&gt;It will stop the job after some day.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is code for creating hivecontext&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;@transient private var instance: HiveContext = _&#xA; def getHiveContext(sparkContext: SparkContext, propertiesBroadcast: Broadcast[Properties]): HiveContext = {&#xA;        synchronized {&#xA;&#xA;  val configuration = new Configuration&#xA;  configuration.addResource(&quot;/etc/hadoop/conf/hdfs-site.xml&quot;)&#xA;  UserGroupInformation.setConfiguration(configuration)&#xA;  UserGroupInformation.getCurrentUser.setAuthenticationMethod(AuthenticationMethod.KERBEROS)&#xA;&#xA;  val secure = propertiesBroadcast.value.getProperty(&quot;kerberosSecurity&quot;).toBoolean&#xA;  if (instance == null) {&#xA;&#xA;    UserGroupInformation.loginUserFromKeytabAndReturnUGI(&#xA;      propertiesBroadcast.value.getProperty(&quot;hadoop.kerberos.principal&quot;), sparkContext.getConf.get(&quot;spark.yarn.keytab&quot;))&#xA;      .doAs(new PrivilegedExceptionAction[HiveContext]() {&#xA;        @Override&#xA;        def run(): HiveContext = {&#xA;          System.setProperty(&quot;hive.metastore.uris&quot;, propertiesBroadcast.value.getProperty(&quot;hive.metastore.uris&quot;));&#xA;          if (secure) {&#xA;            System.setProperty(&quot;hive.metastore.sasl.enabled&quot;, &quot;true&quot;)&#xA;            System.setProperty(&quot;hive.metastore.kerberos.keytab.file&quot;, sparkContext.getConf.get(&quot;spark.yarn.keytab&quot;))&#xA;            System.setProperty(&quot;hive.security.authorization.enabled&quot;, &quot;false&quot;)&#xA;            System.setProperty(&quot;hive.metastore.kerberos.principal&quot;, propertiesBroadcast.value.getProperty(&quot;hive.metastore.kerberos.principal&quot;))&#xA;            System.setProperty(&quot;hive.metastore.execute.setugi&quot;, &quot;true&quot;)&#xA;          }&#xA;&#xA;          instance = new HiveContext(sparkContext)&#xA;          instance.setConf(&quot;spark.sql.parquet.writeLegacyFormat&quot;, &quot;true&quot;)&#xA;&#xA;          instance.sparkContext.hadoopConfiguration.set(&quot;parquet.enable.summary-metadata&quot;, &quot;false&quot;)&#xA;          instance.setConf(&quot;hive.exec.dynamic.partition&quot;, &quot;true&quot;)&#xA;          instance.setConf(&quot;hive.exec.dynamic.partition.mode&quot;, &quot;nonstrict&quot;)&#xA;          instance&#xA;        }&#xA;      })&#xA;&#xA;  }&#xA;&#xA;  UserGroupInformation.loginUserFromKeytabAndReturnUGI(&#xA;    propertiesBroadcast.value.getProperty(&quot;hadoop.kerberos.principal&quot;), sparkContext.getConf.get(&quot;spark.yarn.keytab&quot;))&#xA;    .doAs(new PrivilegedExceptionAction[HiveContext]() {&#xA;      @Override&#xA;      def run(): HiveContext = {&#xA;        instance&#xA;      }&#xA;    })&#xA;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;}&lt;/p&gt;&#xA;" OwnerUserId="3715328" LastActivityDate="2018-03-10T17:48:14.567" Title="org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException:" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;hive&gt;&lt;kerberos&gt;" AnswerCount="0" CommentCount="2" />
  <row Id="49212433" PostTypeId="1" CreationDate="2018-03-10T18:15:34.030" Score="0" ViewCount="28" Body="&lt;p&gt;I have a Spark SQL query in a file test.sql - &lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-sql prettyprint-override&quot;&gt;&lt;code&gt;CREATE GLOBAL TEMPORARY VIEW VIEW_1 AS select a,b from abc&#xA;&#xA;CREATE GLOBAL TEMPORARY VIEW VIEW_2 AS select a,b from VIEW_1&#xA;&#xA;select * from VIEW_2&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now, I start my spark-shell and try to execute it like this - &lt;/p&gt;&#xA;&#xA;&lt;pre class=&quot;lang-scala prettyprint-override&quot;&gt;&lt;code&gt;val sql = scala.io.Source.fromFile(&quot;test.sql&quot;).mkString&#xA;spark.sql(sql).show&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;This fails with the following error - &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;org.apache.spark.sql.catalyst.parser.ParseException:&#xA;mismatched input '&amp;lt;' expecting {&amp;lt;EOF&amp;gt;, 'GROUP', 'ORDER', 'HAVING', 'LIMIT', 'OR', 'AND', 'WINDOW', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'SORT', 'CLUSTER', 'DISTRIBUTE'}(line 1, pos 128)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I tried to execute these queries 1 by 1 in different spark.sql statements and it runs fine. The problem is, I have 6-7 queries which creates temporary views and finally i need output from my last view. Is there a way through which i can run these SQL's in a single spark.sql statement. I have worked on Postgres SQL (Redshift) and that is able to execute such kind of queries. In spark sql, i will have to maintain a lot of files in this case. &lt;/p&gt;&#xA;" OwnerUserId="9472797" LastEditorUserId="9297144" LastEditDate="2018-03-10T18:19:21.803" LastActivityDate="2018-03-10T21:08:04.113" Title="Executing multiple SQL queries on Spark" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;apache-spark-sql&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49212877" PostTypeId="1" CreationDate="2018-03-10T18:58:56.577" Score="0" ViewCount="14" Body="&lt;p&gt;For my project, I need to harvest data from Twitter.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I am currently facing two design choices:&lt;/p&gt;&#xA;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;What is the best software architecture? I read that spark has Twitter support but I am not familiar with Scala. On the other hand, Apache Spark seems a good option, but then I'm not sure on how to save data to a common sink&lt;/li&gt;&#xA;&lt;li&gt;I have some budget constraints. I surely need one server to do the sink and the processing. However, for the data harvesting, I don't know if several VM/containers offer a better performance / cost ratio than a bunch of Raspberry PI running Kafka producers.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;" OwnerUserId="2170724" LastActivityDate="2018-03-10T19:48:26.927" Title="Twitter data harvesting" Tags="&lt;apache-spark&gt;&lt;twitter&gt;&lt;raspberry-pi&gt;&lt;apache-kafka&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49213543" PostTypeId="1" CreationDate="2018-03-10T20:09:58.790" Score="-1" ViewCount="16" Body="&lt;p&gt;I need help for the below scenario&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;I will get the data from Kafka in the following json format to spark streaming&#xA;{&quot;id&quot; : 1 , &quot;data&quot; : &quot;AFGH00101219&quot;}&#xA;{&quot;id&quot; : 2 , &quot;data&quot; : &quot;AFGH00101215&quot;}&#xA;{&quot;id&quot; : 2 , &quot;data&quot; : &quot;AFGH00101216&quot;}&#xA;{&quot;id&quot; : 3 , &quot;data&quot; : &quot;AFGH00101218&quot;}&#xA;&#xA;val messages= KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topics)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Now I want to process each json record from the messages and each record in turn returns set of records. Please give me some ideas to do the below task.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val output = messages.map(row =&amp;gt;&#xA;{&#xA;//here I will get each json record. My doubt is how to extract id and data &#xA;//filed values from row and store it into variables.&#xA;//Here I need to decode the data filed value which is in hexa decimal format &#xA;//to decimal format.&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Thanks in advance. Please let me know if the question is not clear.&lt;/p&gt;&#xA;" OwnerUserId="9428329" LastActivityDate="2018-03-10T20:09:58.790" Title="Process each json record in json RDD using Spark with Scala" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;apache-kafka&gt;&lt;spark-streaming&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49214489" PostTypeId="1" CreationDate="2018-03-10T22:05:56.820" Score="0" ViewCount="8" Body="&lt;p&gt;The documentation is not clear. All blog post examples about Hive LLAP (Long Live and Process) uses the Tez execution engine, but can Spark/MR hive engines also use LLAP?&lt;/p&gt;&#xA;" OwnerUserId="8874837" LastActivityDate="2018-03-11T01:47:29.397" Title="hive llap - which execution engine supported? spark,mr, tez" Tags="&lt;hadoop&gt;&lt;apache-spark&gt;&lt;hive&gt;&lt;mapreduce&gt;&lt;tez&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49214491" PostTypeId="1" CreationDate="2018-03-10T22:06:24.200" Score="0" ViewCount="20" Body="&lt;p&gt;I have a Byte Array in Scala: &lt;code&gt;val nums = Array[Byte](1,2,3,4,5,6,7,8,9)&lt;/code&gt; or you can take any other Byte array. &lt;/p&gt;&#xA;&#xA;&lt;p&gt;I want to save it as a sequence file in HDFS. Below is the code, I am writing in scala console.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import org.apache.hadoop.io.compress.GzipCodec&#xA;nums.map( x =&amp;gt; (NullWritable.get(), new ByteWritable))).saveAsSequenceFile(&quot;/yourPath&quot;, classOf[GzipCodec])&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;But, it's giving following error:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;error: values saveAsSequenceFile is not a member of Array[ (org.apache.hadoop.io.NullWritable), (org.apache.hadoop.io.ByteWritable)]&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;You require to import these classes as well (in scala console).&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;import org.apache.hadoop.io.NullWritable&#xA;import org.apache.hadoop.io.ByteWritable&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="5548245" LastActivityDate="2018-03-11T00:22:55.903" Title="Not able to write SequenceFile in Scala for Array[NullWritable, ByteWritable]" Tags="&lt;scala&gt;&lt;hadoop&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="1" />
  <row Id="49215303" PostTypeId="1" CreationDate="2018-03-11T00:05:17.127" Score="0" ViewCount="13" Body="&lt;p&gt;I have 3 RDDs.&lt;br&gt;&#xA;1st one is of form ((a,b),c).&lt;br&gt;&#xA;2nd one is of form (b,d).&lt;br&gt;&#xA;3rd one is of form (a,e).&lt;br&gt;&#xA;How can I perform join in scala over these RDDs such that my final output is of the form ((a,b),c,d,e)?&lt;/p&gt;&#xA;" OwnerUserId="5106214" LastActivityDate="2018-03-11T03:30:45.650" Title="Perform join in spark only on one co-ordinate of pair key?" Tags="&lt;apache-spark&gt;&lt;bigdata&gt;&lt;rdd&gt;" AnswerCount="2" CommentCount="0" />
  <row Id="49215321" PostTypeId="1" CreationDate="2018-03-11T00:08:03.943" Score="0" ViewCount="8" Body="&lt;p&gt;I'm facing memory issues running structured stream with aggregation and partitioning in Spark 2.2.0: &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;session&#xA;    .readStream()&#xA;    .schema(inputSchema)&#xA;    .option(OPTION_KEY_DELIMITER, OPTION_VALUE_DELIMITER_TAB)&#xA;    .option(OPTION_KEY_QUOTE, OPTION_VALUE_QUOTATION_OFF)&#xA;    .csv(&quot;s3://test-bucket/input&quot;)&#xA;    .as(Encoders.bean(TestRecord.class))&#xA;    .flatMap(mf, Encoders.bean(TestRecord.class))&#xA;    .dropDuplicates(&quot;testId&quot;, &quot;testName&quot;)&#xA;    .withColumn(&quot;year&quot;, functions.date_format(dataset.col(&quot;testTimestamp&quot;).cast(DataTypes.DateType), &quot;YYYY&quot;))&#xA;    .writeStream()&#xA;    .option(&quot;path&quot;, &quot;s3://test-bucket/output&quot;)&#xA;    .option(&quot;checkpointLocation&quot;, &quot;s3://test-bucket/checkpoint&quot;)&#xA;    .trigger(Trigger.ProcessingTime(60, TimeUnit.SECONDS))&#xA;    .partitionBy(&quot;year&quot;)&#xA;    .format(&quot;parquet&quot;)&#xA;    .outputMode(OutputMode.Append())&#xA;    .queryName(&quot;test-stream&quot;)&#xA;    .start();&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;During testing I noticed that amount of used memory increases each time when new data comes and finally executors exit with code 137:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_1520214726510_0001_01_000003 on host: ip-10-0-1-153.us-west-2.compute.internal. Exit status: 137. Diagnostics: Container killed on request. Exit code is 137&#xA;Container exited with a non-zero exit code 137&#xA;Killed by external signal&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I've created a heap dump and found that most of the memory used by &lt;code&gt;org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider&lt;/code&gt; that is referenced from &lt;a href=&quot;https://github.com/apache/spark/blob/branch-2.2/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/StateStore.scala#L196&quot; rel=&quot;nofollow noreferrer&quot;&gt;StateStore&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;On the first glance it looks normal since that is how Spark keeps aggregation keys in memory. However I did my testing by renaming files in source folder, so that they could be picked up by spark. Since input records are the same all further rows should be rejected as duplicates and memory consumption shouldn't increase but it is.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/mhdMu.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/mhdMu.jpg&quot; alt=&quot;executor memory usage&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Moreover, GC time took more than 30% of total processing time&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/pDrqD.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/pDrqD.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is a heap dump taken from the executor running with smaller amount of memory than on screens above since when I was creating a dump from that one the java process just terminated in the middle of the process.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;&lt;a href=&quot;https://i.stack.imgur.com/reqf7.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/reqf7.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;" OwnerUserId="1791510" LastActivityDate="2018-03-11T00:08:03.943" Title="Memory issue with spark structured streaming" Tags="&lt;apache-spark&gt;&lt;apache-spark-sql&gt;&lt;structured-streaming&gt;&lt;spark-structured-streaming&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49216164" PostTypeId="1" CreationDate="2018-03-11T02:51:34.717" Score="0" ViewCount="18" Body="&lt;p&gt;I am using the GraphX API for spark to build a graph and process it with Pregel API. The error does not happen if I return an argument tuple from vprog function, but if I return a new tuple using the same tuple, I get null point error. &#xA;Here is the relevant code:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;val verticesRDD = cleanDtaDF.select(&quot;ChildHash&quot;, &quot;DN&quot;).rdd.map(row =&amp;gt; (row(0).toString.toLong, (row(1).toString.toDouble,row(0).toString.toLong)))&#xA;&#xA;val edgesRDD = (rawDtaDF.select(&quot;ChildHash&quot;, &quot;ParentHash&quot;, &quot;dealer_code&quot;, &quot;dealer_customer_number&quot;, &quot;parent_dealer_cust_number&quot;).rdd&#xA;        .map(row =&amp;gt; Edge(row.get(0).toString.toLong, row.get(1).toString.toLong, (row(3) + &quot; is a child of &quot; + row(4), &quot; when dealer is &quot; + row.get(2)))))&#xA;&#xA;val myGraph = Graph(verticesRDD, edgesRDD)&#xA;&#xA;def vprog(vertexId: VertexId, vertexDTA:(Double, Long), msg: Double): (Double, Long) = {&#xA;        (vertexDTA._1, vertexDTA._2)&#xA;}&#xA;val result = myGraph.pregel(0.0, 1, activeDirection = EdgeDirection.Out)(vprog,t =&amp;gt; Iterator((t.dstId, t.srcAttr._2)),(x, y) =&amp;gt; x + y)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The error does not happen if I make a simple change to vprog(...)--not access the tuples' members:&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;def vprog(vertexId: VertexId, vertexDTA:(Double, Long), msg: Double): (Double, Long) = {&#xA;        vertexDTA&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;The error is &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;[Stage 101:&amp;gt;              (0 + 0) / 200][Stage 102:&amp;gt;              (0 + 4) / 200]18/03/10 20:43:16 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 102.0 (TID 5959, ue1lslaved25.na.aws.cat.com, executor 146): java.lang.NullPointerException&#xA;        at $line69.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.vprog(&amp;lt;console&amp;gt;:60)&#xA;        at $line70.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.apply(&amp;lt;console&amp;gt;:75)&#xA;        at $line70.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.apply(&amp;lt;console&amp;gt;:75)&#xA;        at org.apache.spark.graphx.Pregel$$anonfun$1.apply(Pregel.scala:125)&#xA;        at org.apache.spark.graphx.Pregel$$anonfun$1.apply(Pregel.scala:125)&#xA;        at org.apache.spark.graphx.impl.VertexPartitionBaseOps.map(VertexPartitionBaseOps.scala:61)&#xA;        at org.apache.spark.graphx.impl.GraphImpl$$anonfun$5.apply(GraphImpl.scala:129)&#xA;        at org.apache.spark.graphx.impl.GraphImpl$$anonfun$5.apply(GraphImpl.scala:129)&#xA;        at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)&#xA;        at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)&#xA;        at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:988)&#xA;        at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:979)&#xA;        at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:919)&#xA;        at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:979)&#xA;        at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:697)&#xA;        at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)&#xA;        at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)&#xA;        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)&#xA;        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)&#xA;        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)&#xA;        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)&#xA;        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)&#xA;        at org.apache.spark.scheduler.Task.run(Task.scala:99)&#xA;        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)&#xA;        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&#xA;        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&#xA;        at java.lang.Thread.run(Thread.java:745)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;" OwnerUserId="7788725" LastEditorUserId="7788725" LastEditDate="2018-03-11T02:56:44.670" LastActivityDate="2018-03-11T02:56:44.670" Title="Spark java.lang.NullPointerException when using tuples" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;spark-graphx&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="49216254" PostTypeId="1" AcceptedAnswerId="49216287" CreationDate="2018-03-11T03:10:54.110" Score="2" ViewCount="9" Body="&lt;p&gt;I'm trying to run a very simple hello world Scala program in IntelliJ IDEA on Mac without using the Scala console configuration. I have followed these &lt;a href=&quot;https://hortonworks.com/tutorial/setting-up-a-spark-development-environment-with-scala/&quot; rel=&quot;nofollow noreferrer&quot;&gt;steps&lt;/a&gt; to largely get started, but I didn't set up the debugger outlined there. There isn't a default run configuration enabled, but I can right-click on my source file and select &quot;Scala Console,&quot; as we can see here:&#xA;&lt;a href=&quot;https://i.stack.imgur.com/GQ4eu.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/GQ4eu.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Is there a way to select or edit my configurations to make it so I don't have to use the console? Below are the available configurations.&#xA;&lt;a href=&quot;https://i.stack.imgur.com/CqcyA.png&quot; rel=&quot;nofollow noreferrer&quot;&gt;&lt;img src=&quot;https://i.stack.imgur.com/CqcyA.png&quot; alt=&quot;enter image description here&quot;&gt;&lt;/a&gt;&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I simply want there to be a way to run my Scala code and see the generated output in the provided console, which Scala Console isn't doing. Thanks for your time.&lt;/p&gt;&#xA;" OwnerUserId="3992091" LastActivityDate="2018-03-11T03:17:25.857" Title="Running simple Scala in Intellij" Tags="&lt;scala&gt;&lt;apache-spark&gt;&lt;intellij-idea&gt;&lt;plugins&gt;&lt;configuration&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="49216681" PostTypeId="1" CreationDate="2018-03-11T04:37:50.490" Score="0" ViewCount="5" Body="&lt;p&gt;Pyspark creates folder instead of file. For the below command, it creates an empty folder with name proto.parquet in the directory.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;df.write.parquet(&quot;output/proto.parquet&quot;)&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Tried with csv and other formats, but still the same.&lt;/p&gt;&#xA;" OwnerUserId="848510" LastActivityDate="2018-03-11T04:37:50.490" Title="pyspark creates output file as folder" Tags="&lt;python&gt;&lt;apache-spark&gt;&lt;pyspark&gt;" AnswerCount="0" CommentCount="0" />
  <row Id="45697163" PostTypeId="1" AcceptedAnswerId="45697951" CreationDate="2017-08-15T16:25:28.123" Score="-1" ViewCount="66" Body="&lt;p&gt;I recently started with spark. I am practicing on spark shell.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;I've a dataset &quot;movies.dat&quot; and is in the following format:&lt;/p&gt;&#xA;&#xA;&lt;p&gt;MovieID,Title,Genres&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Sample Record :-  &lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;2,Jumanji (1995),Adventure|Children|Fantasy&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to generate the list of “Horror” movies released in between 1985 to 1995.&lt;/p&gt;&#xA;&#xA;&lt;p&gt;Here is my approach.&lt;/p&gt;&#xA;&#xA;&lt;pre&gt;&lt;code&gt;scala&amp;gt; val movies_data = sc.textFile(&quot;file:///home/cloudera/cs/movies.dat&quot;)&#xA;&#xA;scala&amp;gt; val tags=movies_data.map(line=&amp;gt;line.split(&quot;,&quot;))&#xA;&#xA;scala&amp;gt; tags.take(5)&#xA;res3: Array[Array[String]] = Array(Array(1, Toy Story (1995), Adventure|Animation|Children|Comedy|Fantasy), Array(2, Jumanji (1995), Adventure|Children|Fantasy), Array(3, Grumpier Old Men (1995), Comedy|Romance), Array(4, Waiting to Exhale (1995), Comedy|Drama|Romance), Array(5, Father of the Bride Part II (1995), Comedy))&#xA;&#xA;scala&amp;gt; val horrorMovies = tags.filter(genre=&amp;gt;genre.contains(&quot;Horror&quot;))&#xA;&#xA;scala&amp;gt; horrorMovies.take(5)&#xA;res4: Array[Array[String]] = Array(Array(177, Lord of Illusions (1995), Horror), Array(220, Castle Freak (1995), Horror), Array(841, Eyes Without a Face (Les Yeux sans visage) (1959), Horror), Array(1105, Children of the Corn IV: The Gathering (1996), Horror), Array(1322, Amityville 1992: It's About Time (1992), Horror))&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;I want to retrieve the data using Spark Shell only. I am able to retrieve all the movies of the &quot;Horror&quot; genres. &#xA;Now, is there any way to filter out of those movies and get only the ones that have the release year in between 1985 and 1995?&lt;/p&gt;&#xA;" OwnerUserId="8090165" LastEditorUserId="5880706" LastEditDate="2017-08-15T16:29:47.750" LastActivityDate="2017-08-16T09:57:09.860" Title="Spark shell - How can I retrieve rows from my dataset based on a time period or in between 2 given dates or two years" Tags="&lt;scala&gt;&lt;shell&gt;&lt;apache-spark&gt;" AnswerCount="1" CommentCount="0" />
  <row Id="915697953" PostTypeId="2" ParentId="49210983" CreationDate="2017-08-15T17:11:22.300" Score="0" Body="Resposta D &lt;pre&gt;df.collect()&lt;/pre&gt;" OwnerUserId="5880706" LastEditorUserId="5880706" LastEditDate="2017-08-16T09:57:09.860" LastActivityDate="2017-08-16T09:57:09.860" CommentCount="9" />
</posts>